{
  "version": "https://jsonfeed.org/version/1.1",
  "title": "Blog",
  "language": "en",
  "home_page_url": "http://localhost:8080/",
  "feed_url": "http://localhost:8080/feed.json",
  "description": "",
  "authors": [
    {
      "name": "The Scalable Way"
    }
  ],
  "items": [{
      "id": "http://localhost:8080/blog/sap-data-ingestion-with-python-a-technical-breakdown-of-using-the-sap-rfc-protocol/",
      "url": "http://localhost:8080/blog/sap-data-ingestion-with-python-a-technical-breakdown-of-using-the-sap-rfc-protocol/",
      "title": "SAP Data Ingestion with Python: A Technical Breakdown of Using the SAP RFC Protocol",
      "content_html": "<nav id=\"toc\" class=\"table-of-contents prose\"><ol><li class=\"flow\"><a href=\"#can-you-give-us-a-general-overview-of-how-sap-rfc-works\">Can you give us a general overview of how SAP RFC works?</a></li><li class=\"flow\"><a href=\"#what-are-some-of-the-challenges-of-using-sap-rfc-to-ingest-data-with-python\">What are some of the challenges of using SAP RFC to ingest data with Python?</a></li><li class=\"flow\"><a href=\"#can-you-explain-how-you-interface-a-c-library-with-python\">Can you explain how you interface a C++ library with Python?</a></li><li class=\"flow\"><a href=\"#how-can-ingestion-speed-be-optimized\">How can ingestion speed be optimized?</a></li><li class=\"flow\"><a href=\"#why-do-you-consider-pyrfc-slow-when-it-comes-to-data-ingestion\">Why do you consider pyRFC ‚Äúslow‚Äù when it comes to data ingestion?</a></li><li class=\"flow\"><a href=\"#can-incremental-ingestions-be-done-using-sap-rfc-if-yes-how\">Can incremental ingestions be done using SAP RFC? If yes, how?</a></li><li class=\"flow\"><a href=\"#can-you-give-us-a-couple-of-code-examples-in-python-on-ingesting-the-data\">Can you give us a couple of code examples in Python on ingesting the data?</a></li><li class=\"flow\"><a href=\"#conclusion\">Conclusion</a></li></ol></nav><p><span id=\"toc-skipped\" class=\"visually-hidden\"></span></p><div class=\"flow prose\"><p></p><p>If you‚Äôve ever tried to get data out of SAP, you know it‚Äôs often easier said than done. Manual exports take up a lot of time, standard tools don‚Äôt always fit real needs, and moving data into modern analytics or machine learning pipelines can feel like forcing two worlds together.</p><p>For years, many teams relied on pyRFC, the official Python library for SAP Remote Function Calls. But with pyRFC being decommissioned, there‚Äôs now a real gap for anyone who wants to keep integrating SAP data into Python workflows.</p><p>That‚Äôs why we‚Äôre working on a new Python library to replace pyRFC, focused on making SAP data ingestion easier, more reliable, and better suited for modern data workflows. A connector like this can directly call SAP functions, pull large tables in manageable chunks, and plug that data into Python pipelines. For data engineers, analysts, and developers, this isn‚Äôt about high-end tech for its own sake; it‚Äôs about saving time, reducing errors, and finally being able to use SAP data in the tools we already work with every day.</p><p>To dig into how this actually works, I‚Äôve talked with Dominik ‚Äí a senior software engineer with more than ten years of experience, a computer science lecturer, and the tech lead of this project. In this conversation, he shares his knowledge, best practices, and lessons learned while building an SAP connector in Python using the RFC protocol.</p><p><strong>______</strong></p><h4 id=\"can-you-give-us-a-general-overview-of-how-sap-rfc-works\"><a href=\"#can-you-give-us-a-general-overview-of-how-sap-rfc-works\" class=\"heading-anchor\"><strong>Can you give us a general overview of how SAP RFC works?</strong></a></h4><p>SAP Remote Function Call is a communication mechanism that allows one system to execute functions in another system as if they were local calls. It is primarily used to integrate different SAP modules, connect SAP with external applications, and support distributed processing. RFC works by exposing specific function modules in SAP that are marked as ‚Äúremote-enabled,‚Äù meaning they can be invoked across system boundaries. When an RFC is triggered, the caller system packages the request, sends it over the network, and waits for a response (in synchronous mode) or continues processing without waiting (in asynchronous mode). This approach provides a standardized and reliable way for SAP systems and external programs to exchange data and trigger business logic, making RFC a cornerstone of SAP interoperability.</p><p><is-land on:idle></is-land></p><dialog class=\"flow modal45\"><button autofocus class=\"button\">Close</button><picture><source type=\"image/webp\" srcset=\"/img/CCj0qkhJii-960.webp 960w, /img/CCj0qkhJii-1600.webp 1600w\" sizes=\"auto\"><img loading=\"lazy\" decoding=\"async\" src=\"/img/CCj0qkhJii-960.jpeg\" alt=\"sap rfc communication\" width=\"1600\" height=\"656\" srcset=\"/img/CCj0qkhJii-960.jpeg 960w, /img/CCj0qkhJii-1600.jpeg 1600w\" sizes=\"auto\"></picture></dialog><button data-index=\"45\"><picture><source type=\"image/webp\" srcset=\"/img/CCj0qkhJii-960.webp 960w, /img/CCj0qkhJii-1600.webp 1600w\" sizes=\"auto\"><img loading=\"lazy\" decoding=\"async\" src=\"/img/CCj0qkhJii-960.jpeg\" alt=\"sap rfc communication\" width=\"1600\" height=\"656\" srcset=\"/img/CCj0qkhJii-960.jpeg 960w, /img/CCj0qkhJii-1600.jpeg 1600w\" sizes=\"auto\"></picture></button><p></p><h4 id=\"what-are-some-of-the-challenges-of-using-sap-rfc-to-ingest-data-with-python\"><a href=\"#what-are-some-of-the-challenges-of-using-sap-rfc-to-ingest-data-with-python\" class=\"heading-anchor\"><strong>What are some of the challenges of using SAP RFC to ingest data with Python?</strong></a></h4><p>One of the main challenges of using SAP RFC for ingesting data with Python is handling the very large data volumes that SAP tables can produce. RFC itself has technical limitations, such as row size restrictions such as:</p><ul class=\"list\"><li><strong>Row size limitations</strong><ul class=\"list\"><li>The 512-character per-row restriction in RFC_READ_TABLE means wide tables with many columns need to be split across multiple queries.</li><li>Reconstructing the full row in Python requires careful mapping of column segments.</li></ul></li><li><strong>Volume and performance</strong><ul class=\"list\"><li>Extracting millions of rows can be slow and may degrade SAP system performance if not throttled.</li><li>Network latency and RFC protocol overhead can become bottlenecks for very large datasets.</li><li>Lack of streaming support means all results are buffered in memory, risking <strong>high RAM usage</strong> in Python.</li></ul></li><li><strong>Pagination and batching</strong><ul class=\"list\"><li>Since RFC has no built-in pagination, developers need to implement logic to fetch data in smaller chunks.</li><li>Requires careful handling of row offsets and consistency to avoid duplicates or missing records.</li></ul></li><li><strong>Data type handling</strong><ul class=\"list\"><li>SAP tables contain many proprietary data types (e.g., RAW, DEC, DATS, TIMS) that need explicit conversion to Python types.</li><li>Inconsistent formatting (e.g., leading zeros, fixed-length fields) can require custom parsing logic.</li></ul></li><li><strong>Error handling and robustness</strong><ul class=\"list\"><li>Large queries can lead to timeouts or aborted RFC sessions.</li><li>Error messages from SAP may be cryptic and require domain knowledge to interpret.</li><li>Retry logic and fault tolerance are not built in and must be handled in the Python layer.</li></ul></li><li><strong>Security and access restrictions</strong><ul class=\"list\"><li>Not all tables are directly accessible due to SAP authorization profiles.</li><li>RFC users often have limited permissions, which may block some required data.</li></ul></li><li><strong>Alternative interfaces</strong><ul class=\"list\"><li>RFC_READ_TABLE is convenient but not officially intended for large-scale data extraction.</li><li>In some cases, more efficient solutions (e.g., SAP OData services, CDS views, or custom ABAP reports) may be required instead of RFC.</li></ul></li><li><strong>Connection/session handling</strong><ul class=\"list\"><li>With Python‚Äôs pyrfc, each call often opens a new RFC session, and connections can drop unexpectedly if not managed carefully. This leads to overhead in session initialization and can cause instability in long-running jobs.</li><li>A custom C++ connector can maintain a <strong>persistent session</strong> without frequent disconnects, providing more stability and efficiency for large-scale or continuous data ingestion.</li></ul></li></ul><p>This means developers must implement batching, efficient memory handling, and sometimes parallelization to make ingestion practical. Without these techniques, performance can degrade quickly, and data may be truncated or lost during transfer, making large-scale SAP-to-Python integration non-trivial.</p><h4 id=\"can-you-explain-how-you-interface-a-c-library-with-python\"><a href=\"#can-you-explain-how-you-interface-a-c-library-with-python\" class=\"heading-anchor\"><strong>Can you explain how you interface a C++ library with Python?</strong></a></h4><p>Interfacing a C++ library with Python involves creating a thin wrapper layer that makes the C++ functions and classes look like native Python objects. To achieve that, we use **pybind11, **which handles this translation by generating a Python extension module that directly calls into the compiled C++ code. Once built, the module can be imported into Python just like any other package, allowing Python code to invoke high-performance C++ logic seamlessly. This approach avoids costly inter-process communication and provides a clean way to combine Python‚Äôs flexibility with the speed and efficiency of C++.</p><h4 id=\"how-can-ingestion-speed-be-optimized\"><a href=\"#how-can-ingestion-speed-be-optimized\" class=\"heading-anchor\"><strong>How can ingestion speed be optimized?</strong></a></h4><p>Ingestion speed from SAP via RFC can be optimized by designing smart calls that minimize the number of round-trip calls to the SAP system. Instead of pulling entire tables blindly, it‚Äôs often more efficient to filter data at the source, select only the required columns, and batch large queries into manageable but sizable chunks. This reduces overhead and avoids overwhelming the Python client with too many small calls. Another common strategy is to push down as much logic as possible into SAP‚Äîusing where clauses, ranges, or custom remote-enabled function modules‚Äîso that Python only receives the data that is truly needed. By shrinking the number of SAP calls in this way, the integration pipeline becomes more efficient, faster, and less resource-intensive on both ends.</p><h4 id=\"why-do-you-consider-pyrfc-slow-when-it-comes-to-data-ingestion\"><a href=\"#why-do-you-consider-pyrfc-slow-when-it-comes-to-data-ingestion\" class=\"heading-anchor\"><strong>Why do you consider pyRFC ‚Äúslow‚Äù when it comes to data ingestion?</strong></a></h4><p>pyRFC is not inherently ‚Äúslow‚Äù for small to medium RFC calls, but it becomes inefficient when used for large-scale data ingestion from SAP tables. Several factors contribute to this:</p><ul class=\"list\"><li><strong>Session management overhead</strong><ul class=\"list\"><li>pyRFC often opens and tears down RFC sessions per call, rather than maintaining a long-lived persistent session. This adds noticeable latency when thousands of calls are required for wide or paginated tables.</li></ul></li><li><strong>Row size and query splitting</strong><ul class=\"list\"><li>Because of the 512-character row size limitation in RFC_READ_TABLE, wide tables must be split into multiple queries. In pyRFC, reconstructing results requires extra calls and significant Python-side processing, slowing ingestion.</li></ul></li><li><strong>Serialization and conversion costs</strong><ul class=\"list\"><li>SAP data types (e.g., packed decimals, dates, times, raw fields) must be converted into Python objects. This conversion layer, implemented in Python, adds overhead compared to a native C++ implementation.</li></ul></li><li><strong>Global Interpreter Lock (GIL)</strong><ul class=\"list\"><li>Python‚Äôs GIL prevents true multithreaded parallel RFC calls within the same process. This limits scalability for high-throughput extraction workloads unless you resort to multiprocessing (which adds its own overhead). We are trying to jump over this problem using C++ library.</li></ul></li><li><strong>Error recovery and retries</strong><ul class=\"list\"><li>pyRFC connections may drop unexpectedly under load, requiring reconnections and retries. This increases latency compared to a C++ connector that maintains stable sessions.</li></ul></li></ul><p>In contrast, a native C++ RFC client avoids much of this overhead by:</p><ul class=\"list\"><li>Keeping a persistent connection/session.</li><li>Handling large data more efficiently with lower-level memory management.</li><li>Offering faster type conversions without Python‚Äôs object allocation overhead.</li></ul><h4 id=\"can-incremental-ingestions-be-done-using-sap-rfc-if-yes-how\"><a href=\"#can-incremental-ingestions-be-done-using-sap-rfc-if-yes-how\" class=\"heading-anchor\"><strong>Can incremental ingestions be done using SAP RFC? If yes, how?</strong></a></h4><p>Incremental ingestions with SAP RFC are not straightforward, because RFC itself is just a transport mechanism and doesn‚Äôt provide built-in change tracking. In most cases, you cannot simply ask RFC for ‚Äúonly new or updated records‚Äù unless the underlying SAP function module or table has fields that can support this, such as timestamps or change indicators. If those exist, you can design your RFC queries in Python to fetch only rows newer than the last ingestion run, effectively simulating incremental loading. Otherwise, true incremental ingestion is better handled through SAP‚Äôs dedicated frameworks like ODP (Operational Data Provisioning) or CDS views, which are specifically designed for delta handling. So while basic RFC on its own doesn‚Äôt guarantee incremental ingestions, with careful design and the right SAP data sources, it can sometimes be approximated.</p><h4 id=\"can-you-give-us-a-couple-of-code-examples-in-python-on-ingesting-the-data\"><a href=\"#can-you-give-us-a-couple-of-code-examples-in-python-on-ingesting-the-data\" class=\"heading-anchor\"><strong>Can you give us a couple of code examples in Python on ingesting the data?</strong></a></h4><p><strong>Connection:</strong></p><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">def</span> <span class=\"token function\">con</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">)</span> <span class=\"token operator\">-</span><span class=\"token operator\">&gt;</span> sap_rfc_connector<span class=\"token punctuation\">.</span>SapRfcConnector<span class=\"token punctuation\">:</span>\n    <span class=\"token triple-quoted-string string\">\"\"\"The C+++ connection to SAP.\"\"\"</span>\n    <span class=\"token keyword\">if</span> self<span class=\"token punctuation\">.</span>_con <span class=\"token keyword\">is</span> <span class=\"token keyword\">not</span> <span class=\"token boolean\">None</span><span class=\"token punctuation\">:</span>\n        <span class=\"token keyword\">return</span> self<span class=\"token punctuation\">.</span>_con\n    con <span class=\"token operator\">=</span> sap_rfc_connector<span class=\"token punctuation\">.</span>SapRfcConnector<span class=\"token punctuation\">(</span><span class=\"token operator\">**</span>self<span class=\"token punctuation\">.</span>credentials<span class=\"token punctuation\">)</span>\n    self<span class=\"token punctuation\">.</span>_con <span class=\"token operator\">=</span> con\n    <span class=\"token keyword\">return</span> con</code></pre><p><strong>Call:</strong></p><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token keyword\">def</span> <span class=\"token function\">call</span><span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">,</span> func<span class=\"token punctuation\">:</span> <span class=\"token builtin\">str</span><span class=\"token punctuation\">,</span> <span class=\"token operator\">*</span>args<span class=\"token punctuation\">,</span> <span class=\"token operator\">**</span>kwargs<span class=\"token punctuation\">)</span> <span class=\"token operator\">-</span><span class=\"token operator\">&gt;</span> <span class=\"token builtin\">dict</span><span class=\"token punctuation\">[</span><span class=\"token builtin\">str</span><span class=\"token punctuation\">,</span> Any<span class=\"token punctuation\">]</span><span class=\"token punctuation\">:</span>\n    <span class=\"token triple-quoted-string string\">\"\"\"Call a SAP RFC function.\"\"\"</span>\n    func_caller <span class=\"token operator\">=</span> sap_rfc_connector<span class=\"token punctuation\">.</span>SapFunctionCaller<span class=\"token punctuation\">(</span>self<span class=\"token punctuation\">.</span>con<span class=\"token punctuation\">)</span>   \n    result <span class=\"token operator\">=</span> func_caller<span class=\"token punctuation\">.</span>smart_call<span class=\"token punctuation\">(</span>func<span class=\"token punctuation\">,</span> <span class=\"token operator\">*</span>args<span class=\"token punctuation\">,</span> <span class=\"token operator\">**</span>kwargs<span class=\"token punctuation\">)</span>\n\n    <span class=\"token keyword\">return</span> result</code></pre><p><strong>One of the approaches to avoid pandas for the data ingestion (POC):</strong></p><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token comment\"># Check and skip if there is no data returned.</span>\n<span class=\"token keyword\">try</span><span class=\"token punctuation\">:</span>\n    <span class=\"token keyword\">if</span> response<span class=\"token punctuation\">[</span><span class=\"token string\">\"DATA\"</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">:</span>\n        logger<span class=\"token punctuation\">.</span>debug<span class=\"token punctuation\">(</span><span class=\"token string\">\"checking data\"</span><span class=\"token punctuation\">)</span>\n        <span class=\"token keyword\">if</span> print_regular<span class=\"token punctuation\">:</span>\n            print_regular<span class=\"token punctuation\">(</span><span class=\"token string\">\"checking data\"</span><span class=\"token punctuation\">)</span>\n        record_key <span class=\"token operator\">=</span> <span class=\"token string\">\"WA\"</span>\n        data_raw <span class=\"token operator\">=</span> np<span class=\"token punctuation\">.</span>array<span class=\"token punctuation\">(</span>response<span class=\"token punctuation\">[</span><span class=\"token string\">\"DATA\"</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">)</span>\n        \n        <span class=\"token comment\"># Save raw data to CSV file immediately</span>\n        logger<span class=\"token punctuation\">.</span>debug<span class=\"token punctuation\">(</span><span class=\"token string-interpolation\"><span class=\"token string\">f\"Saving </span><span class=\"token interpolation\"><span class=\"token punctuation\">{</span><span class=\"token builtin\">len</span><span class=\"token punctuation\">(</span>data_raw<span class=\"token punctuation\">)</span><span class=\"token punctuation\">}</span></span><span class=\"token string\"> rows to CSV file...\"</span></span><span class=\"token punctuation\">)</span>\n        <span class=\"token keyword\">if</span> print_regular<span class=\"token punctuation\">:</span>\n            print_regular<span class=\"token punctuation\">(</span><span class=\"token string-interpolation\"><span class=\"token string\">f\"Saving </span><span class=\"token interpolation\"><span class=\"token punctuation\">{</span><span class=\"token builtin\">len</span><span class=\"token punctuation\">(</span>data_raw<span class=\"token punctuation\">)</span><span class=\"token punctuation\">}</span></span><span class=\"token string\"> rows to CSV file...\"</span></span><span class=\"token punctuation\">)</span>\n        <span class=\"token keyword\">with</span> <span class=\"token builtin\">open</span><span class=\"token punctuation\">(</span>temp_csv_path<span class=\"token punctuation\">,</span> <span class=\"token string\">'w'</span><span class=\"token punctuation\">,</span> newline<span class=\"token operator\">=</span><span class=\"token string\">''</span><span class=\"token punctuation\">,</span> encoding<span class=\"token operator\">=</span><span class=\"token string\">'utf-8'</span><span class=\"token punctuation\">)</span> <span class=\"token keyword\">as</span> csvfile<span class=\"token punctuation\">:</span>\n            writer <span class=\"token operator\">=</span> csv<span class=\"token punctuation\">.</span>writer<span class=\"token punctuation\">(</span>csvfile<span class=\"token punctuation\">)</span>\n            <span class=\"token comment\"># Write header</span>\n            writer<span class=\"token punctuation\">.</span>writerow<span class=\"token punctuation\">(</span>fields<span class=\"token punctuation\">)</span>\n            <span class=\"token comment\"># Write data rows</span>\n            <span class=\"token keyword\">for</span> row_data <span class=\"token keyword\">in</span> data_raw<span class=\"token punctuation\">:</span>\n                <span class=\"token keyword\">try</span><span class=\"token punctuation\">:</span>\n                    split_data <span class=\"token operator\">=</span> row_data<span class=\"token punctuation\">[</span>record_key<span class=\"token punctuation\">]</span><span class=\"token punctuation\">.</span>split<span class=\"token punctuation\">(</span>sep<span class=\"token punctuation\">)</span>\n                    <span class=\"token keyword\">if</span> <span class=\"token builtin\">len</span><span class=\"token punctuation\">(</span>split_data<span class=\"token punctuation\">)</span> <span class=\"token operator\">==</span> <span class=\"token builtin\">len</span><span class=\"token punctuation\">(</span>fields<span class=\"token punctuation\">)</span><span class=\"token punctuation\">:</span>\n                        writer<span class=\"token punctuation\">.</span>writerow<span class=\"token punctuation\">(</span>split_data<span class=\"token punctuation\">)</span>\n                    <span class=\"token keyword\">else</span><span class=\"token punctuation\">:</span>\n                        logger<span class=\"token punctuation\">.</span>warning<span class=\"token punctuation\">(</span><span class=\"token string-interpolation\"><span class=\"token string\">f\"Row data length mismatch: expected </span><span class=\"token interpolation\"><span class=\"token punctuation\">{</span><span class=\"token builtin\">len</span><span class=\"token punctuation\">(</span>fields<span class=\"token punctuation\">)</span><span class=\"token punctuation\">}</span></span><span class=\"token string\">, got </span><span class=\"token interpolation\"><span class=\"token punctuation\">{</span><span class=\"token builtin\">len</span><span class=\"token punctuation\">(</span>split_data<span class=\"token punctuation\">)</span><span class=\"token punctuation\">}</span></span><span class=\"token string\">\"</span></span><span class=\"token punctuation\">)</span>\n                <span class=\"token keyword\">except</span> Exception <span class=\"token keyword\">as</span> e<span class=\"token punctuation\">:</span>\n                    logger<span class=\"token punctuation\">.</span>error<span class=\"token punctuation\">(</span><span class=\"token string-interpolation\"><span class=\"token string\">f\"Error processing row: </span><span class=\"token interpolation\"><span class=\"token punctuation\">{</span>e<span class=\"token punctuation\">}</span></span><span class=\"token string\">\"</span></span><span class=\"token punctuation\">)</span>\n                    <span class=\"token keyword\">continue</span>\n        \n        logger<span class=\"token punctuation\">.</span>debug<span class=\"token punctuation\">(</span><span class=\"token string-interpolation\"><span class=\"token string\">f\"Saved data to </span><span class=\"token interpolation\"><span class=\"token punctuation\">{</span>temp_csv_path<span class=\"token punctuation\">}</span></span><span class=\"token string\">\"</span></span><span class=\"token punctuation\">)</span>\n        <span class=\"token keyword\">if</span> print_regular<span class=\"token punctuation\">:</span>\n            print_regular<span class=\"token punctuation\">(</span><span class=\"token string-interpolation\"><span class=\"token string\">f\"Saved data to </span><span class=\"token interpolation\"><span class=\"token punctuation\">{</span>temp_csv_path<span class=\"token punctuation\">}</span></span><span class=\"token string\">\"</span></span><span class=\"token punctuation\">)</span>\n        <span class=\"token keyword\">del</span> response\n        <span class=\"token keyword\">del</span> data_raw\n<span class=\"token keyword\">except</span> Exception <span class=\"token keyword\">as</span> e<span class=\"token punctuation\">:</span>\n    logger<span class=\"token punctuation\">.</span>error<span class=\"token punctuation\">(</span><span class=\"token string-interpolation\"><span class=\"token string\">f\"Error: </span><span class=\"token interpolation\"><span class=\"token punctuation\">{</span>e<span class=\"token punctuation\">}</span></span><span class=\"token string\">\"</span></span><span class=\"token punctuation\">)</span>\n    <span class=\"token keyword\">if</span> print_regular<span class=\"token punctuation\">:</span>\n        print_regular<span class=\"token punctuation\">(</span><span class=\"token string-interpolation\"><span class=\"token string\">f\"Error: </span><span class=\"token interpolation\"><span class=\"token punctuation\">{</span>e<span class=\"token punctuation\">}</span></span><span class=\"token string\">\"</span></span><span class=\"token punctuation\">)</span>\n    <span class=\"token keyword\">break</span></code></pre><p><strong>_________</strong></p><h4 id=\"conclusion\"><a href=\"#conclusion\" class=\"heading-anchor\"><strong>Conclusion</strong></a></h4><p>Dominik‚Äôs perspective makes it clear that working with SAP data through RFC is full of opportunities, but far from straightforward. From row size limits and type conversions to performance tuning and incremental loading, every step has its own challenges. His approach ‚Äí combining Python with a C++ connector, careful batching, and smart query design ‚Äí shows what it takes to make ingestion both reliable and efficient.</p><p>With pyRFC being decommissioned, these insights feel especially timely. They point to what‚Äôs needed in the next generation of connectors: tools that handle scale gracefully, integrate naturally into Python workflows, and make SAP data easier to work with daily.</p></div>",
      "date_published": "2025-09-15T08:00:00Z"
    }
    ,{
      "id": "http://localhost:8080/blog/ci/cd-for-data-workflows-automating-prefect-deployments-with-github-actions/",
      "url": "http://localhost:8080/blog/ci/cd-for-data-workflows-automating-prefect-deployments-with-github-actions/",
      "title": "CI/CD for Data Workflows: Automating Prefect Deployments with GitHub Actions",
      "content_html": "<nav id=\"toc\" class=\"table-of-contents prose\"><ol><li class=\"flow\"><a href=\"#lets-recap\">Let‚Äôs recap‚Ä¶</a></li><li class=\"flow\"><a href=\"#ci/cd-foundations-for-data-platforms\">CI/CD Foundations for Data Platforms</a><ol><li class=\"flow\"><a href=\"#why-is-ci/cd-so-important\">Why is CI/CD so important?</a></li><li class=\"flow\"><a href=\"#what-makes-data-pipeline-ci/cd-different-from-traditional-software\">What makes data pipeline CI/CD different from traditional software?</a></li></ol></li><li class=\"flow\"><a href=\"#github-ci/cd-workflows-for-a-data-platform\">GitHub CI/CD Workflows for a Data Platform</a><ol><li class=\"flow\"><a href=\"#repository-structure\">Repository structure</a></li><li class=\"flow\"><a href=\"#ci/cd-workflows-overview\">CI/CD Workflows overview</a><ol><li class=\"flow\"><a href=\"#workflow-1-flows-image-builder\">Workflow 1: Flows Image Builder</a><ol><li class=\"flow\"><a href=\"#pull-request-steps\">Pull request steps</a></li><li class=\"flow\"><a href=\"#after-pr-marge\">After PR Marge</a></li></ol></li><li class=\"flow\"><a href=\"#workflow-2-prefect-worker-updates\">Workflow 2: Prefect Worker Updates</a><ol><li class=\"flow\"><a href=\"#pull-request-steps-1\">Pull Request Steps</a></li><li class=\"flow\"><a href=\"#after-pr-merge\">After PR Merge</a></li></ol></li><li class=\"flow\"><a href=\"#workflow-3-prefect-deployment-orchestration\">Workflow 3: Prefect Deployment Orchestration</a><ol><li class=\"flow\"><a href=\"#pull-request-steps-2\">Pull Request Steps</a></li><li class=\"flow\"><a href=\"#after-pr-merge-1\">After PR Merge</a></li></ol></li></ol></li></ol></li><li class=\"flow\"><a href=\"#conclusion-and-series-summary\">Conclusion &amp; series summary</a></li></ol></nav><p><span id=\"toc-skipped\" class=\"visually-hidden\"></span></p><div class=\"flow prose\"><p></p><p>You‚Äôve built a Prefect flow that runs, but wondering what‚Äôs next?</p><p>If deploying it means copying files, running CLI commands, or manually registering deployments, you‚Äôre doing too much. In this guide, we‚Äôll walk through how to automate Prefect deployments using GitHub Actions and Docker, so your flows move from dev to prod with zero manual steps. Cleaner workflows, fewer errors, and no more ‚Äúdid I forget to deploy that?\" moments.</p><p>Welcome to Part 4 of our data platform series, where we bring automation and resilience to the forefront by introducing CI/CD for your data workflows. If you‚Äôve followed along, you‚Äôve seen how each layer builds on the last: from architecture through infrastructure to operational readiness. But just to be sure we‚Äôre on the same page‚Ä¶</p><h2 id=\"lets-recap\"><a href=\"#lets-recap\" class=\"heading-anchor\">Let‚Äôs recap‚Ä¶</a></h2><p>In <strong>Part 1</strong> (<a href=\"https://thescalableway.com/blog/deploying-prefect-on-any-cloud-using-a-single-virtual-machine/\" rel=\"noopener\">Deploying Prefect on any Cloud Using a Single Virtual Machine</a>), we explored the architectural foundations of a modern data platform. We discussed why simplicity, flexibility, and scalability matter, and how a lightweight Kubernetes setup on a single VM can deliver immediate value while laying the groundwork for future growth.</p><p><strong>Part 2</strong> (<a href=\"https://thescalableway.com/blog/how-to-setup-data-platform-infrastructure-on-google-cloud-platform-with-terraform/\" rel=\"noopener\">How to Setup Data Platform Infrastructure on Google Cloud Platform with Terraform</a>) moved us from theory to practice, automating cloud infrastructure using Terraform. It emphasized cloud-agnostic design while preserving the architectural principles from the first article.</p><p>We operationalized the platform in <strong>Part 3</strong> (<a href=\"https://thescalableway.com/blog/getting-to-your-first-flow-run-prefect-worker-and-deployment-setup/\" rel=\"noopener\">Getting to Your First Flow Run: Prefect Worker and Deployment Setup</a>). You learned how to build a containerized execution environment, configure Prefect workers, and organize deployment code, culminating in your first successful data ingestion flow.</p><p>Now, in <strong>Part 4</strong>, we turn our attention to automation. You‚Äôll learn how to implement a robust CI/CD pipeline using GitHub Actions, tailored for data platforms. We‚Äôll break down three essential types of workflows (covering container management, infrastructure updates, and workflow orchestration) that form a scalable, resilient, and low-maintenance deployment system together.</p><p>Whether your goal is to reduce manual intervention, boost reliability, or accelerate delivery, this final part will equip you with patterns and real-world guidance to make your data platform production-ready. Let‚Äôs delve in!</p><h2 id=\"ci/cd-foundations-for-data-platforms\"><a href=\"#ci/cd-foundations-for-data-platforms\" class=\"heading-anchor\">CI/CD Foundations for Data Platforms</a></h2><h3 id=\"why-is-ci/cd-so-important\"><a href=\"#why-is-ci/cd-so-important\" class=\"heading-anchor\">Why is CI/CD so important?</a></h3><p>CI/CD (Continuous Integration and Continuous Deployment) is key to building reliable, scalable, and collaborative data platforms. Manual processes, such as registering deployments by hand or managing configurations outside of version control, can easily slow things down and lead to mistakes. Here‚Äôs how automation makes a big difference in modern data workflows:</p><ul class=\"list\"><li><strong>Manual deployment steps are easy to mess up.</strong></li></ul><p>When people manually update deployments or configurations, there‚Äôs a higher chance of errors, inconsistencies, or missed steps. Automation ensures each deployment follows the same steps every time, cutting down on mistakes and keeping things running smoothly.</p><ul class=\"list\"><li><strong>Without a shared codebase, it‚Äôs hard to stay aligned.</strong></li></ul><p>If local development isn‚Äôt tied closely to a shared, version-controlled code repository, keeping track of changes, rolling back mistakes, or working well as a team is tough. CI/CD pipelines enforce the use of a central repository, making the entire platform transparent and auditable.</p><ul class=\"list\"><li><strong>Testing only locally hides problems.</strong></li></ul><p>Without automated workflows to provision and test in staging or development environments, teams often default to running tests locally. This limits visibility into how changes will behave in real-world conditions and increases the risk of production issues.</p><ul class=\"list\"><li><strong>Manual processes don‚Äôt scale well.</strong></li></ul><p>As your data platform and team grow, manual processes quickly become bottlenecks. Automated CI/CD pipelines make it easier to bring on new team members, keep deployments consistent, and move faster, without losing quality or stability.</p><p>Setting up a strong CI/CD foundation creates a more stable, transparent, and scalable environment. It lets your data engineers spend more time delivering value and less time fixing avoidable problems.</p><h3 id=\"what-makes-data-pipeline-ci/cd-different-from-traditional-software\"><a href=\"#what-makes-data-pipeline-ci/cd-different-from-traditional-software\" class=\"heading-anchor\">What makes data pipeline CI/CD different from traditional software?</a></h3><p>CI/CD for data platforms comes with its own set of challenges, especially compared to traditional software development. This is mainly because tools like Prefect act as orchestrators, running workflows inside separate, containerized environments. This setup introduces a few extra layers to manage:</p><ol class=\"list\"><li><strong>Different Workflows for Different Parts</strong></li></ol><ul class=\"list\"><li><strong>Docker Image Pipeline:</strong> Dedicated workflow for building, testing, and deploying container images, including all the flow dependencies.</li><li><strong>Prefect Worker Management:</strong> The only long-living process requiring separate CI/CD for updates of the application itself and the base job template used in flow runs.</li><li><strong>Deployment Configuration:</strong> Independent workflow for managing Prefect deployment definitions and versioning.</li></ul><ol start=\"2\" class=\"list\"><li><strong>More Moving Parts to Coordinate</strong></li></ol><ul class=\"list\"><li>Each part of the system might have its own release schedule, so updates must be carefully timed to avoid breaking things.</li><li>The container environments used for development, testing, and production must match, or you risk inconsistencies and surprises.</li></ul><p>Unlike in traditional software, where a single build moves through environments, data platforms rely on multiple components that each need to be managed separately. Because of this, automation has to be thoughtfully designed to keep everything in sync. Done right, it helps teams move faster without sacrificing reliability.</p><h2 id=\"github-ci/cd-workflows-for-a-data-platform\"><a href=\"#github-ci/cd-workflows-for-a-data-platform\" class=\"heading-anchor\">GitHub CI/CD Workflows for a Data Platform</a></h2><h3 id=\"repository-structure\"><a href=\"#repository-structure\" class=\"heading-anchor\">Repository structure</a></h3><p>Before diving into the workflows, here‚Äôs a quick look at the repository structure established so far. While this isn‚Äôt the complete repository, the following directories and files are the main ones that trigger CI/CD processes:</p><pre class=\"language-bash\"><code class=\"language-bash\">üì¶ repository\n ‚î£ üìÇ .github\n ‚îÉ ‚î£ üìÇ workflows\n ‚îÉ ‚îÉ ‚î£ üìú workflow-x.yml\n ‚îÉ ‚îÉ ‚î£ üìú template-x.yml\n ‚î£ üìÇ etc\n ‚îÉ ‚î£ üìÇ <span class=\"token function\">docker</span>\n ‚îÉ ‚îÉ ‚î£ üìú Dockerfile\n ‚îÉ ‚î£ üìÇ helm_values\n ‚îÉ ‚î£ ‚î£ üìÇ prefect-worker\n ‚îÉ ‚îÉ ‚îÉ ‚î£ üìú values-dev.yaml\n ‚îÉ ‚îÉ ‚îÉ ‚î£ üìú values-prod.yaml\n ‚î£ üìÇ src\n ‚îÉ ‚î£ üìÇ edp_flows\n ‚îÉ ‚î£ ‚î£ üìÇ flows\n ‚îÉ ‚îÉ ‚îÉ ‚î£ üìú flow-x.yml\n ‚î£ üìú pyproject.toml\n ‚îó üìú prefect.yml</code></pre><h3 id=\"ci/cd-workflows-overview\"><a href=\"#ci/cd-workflows-overview\" class=\"heading-anchor\">CI/CD Workflows overview</a></h3><p>With the structure in place, let‚Äôs walk through the <strong>three main workflows</strong> that drive CI/CD for this data platform. This setup is designed to be portable and not tied specifically to GitHub Actions, and its goal is to automate flow deployment with Prefect, avoid unnecessary Docker builds, and use Prefect‚Äôs GitHub Repository Block to manage flows cleanly. The three key workflows are:</p><ul class=\"list\"><li><strong>Flows Image Workflow:</strong> Triggered when updates to flow image dependencies are needed.</li><li><strong>Prefect Worker Workflow:</strong> Runs when changes are made to the base job template or worker configuration.</li><li><strong>Prefect Deployments CI/CD:</strong> Used for developing and managing new deployments and flows; this is the main workflow during development.</li></ul><p>Let‚Äôs take a closer look at all of them.</p><h4 id=\"workflow-1-flows-image-builder\"><a href=\"#workflow-1-flows-image-builder\" class=\"heading-anchor\">Workflow 1: Flows Image Builder</a></h4><p><is-land on:idle></is-land></p><dialog class=\"flow modal7\"><button autofocus class=\"button\">Close</button><picture><source type=\"image/webp\" srcset=\"/img/2za6kUqSCH-960.webp 960w, /img/2za6kUqSCH-1600.webp 1600w\" sizes=\"auto\"><img loading=\"lazy\" decoding=\"async\" src=\"/img/2za6kUqSCH-960.jpeg\" alt=\"flows image builder\" width=\"1600\" height=\"748\" srcset=\"/img/2za6kUqSCH-960.jpeg 960w, /img/2za6kUqSCH-1600.jpeg 1600w\" sizes=\"auto\"></picture></dialog><button data-index=\"7\"><picture><source type=\"image/webp\" srcset=\"/img/2za6kUqSCH-960.webp 960w, /img/2za6kUqSCH-1600.webp 1600w\" sizes=\"auto\"><img loading=\"lazy\" decoding=\"async\" src=\"/img/2za6kUqSCH-960.jpeg\" alt=\"flows image builder\" width=\"1600\" height=\"748\" srcset=\"/img/2za6kUqSCH-960.jpeg 960w, /img/2za6kUqSCH-1600.jpeg 1600w\" sizes=\"auto\"></picture></button><p></p><p>This workflow is triggered by changes to any of the following files:</p><ul class=\"list\"><li><code>etc/docker/Dockerfile</code></li><li><code>pyproject.toml</code></li></ul><h5 id=\"pull-request-steps\"><a href=\"#pull-request-steps\" class=\"heading-anchor\">Pull request steps</a></h5><ol class=\"list\"><li><strong>Check version increase:</strong> Validates that the version number was bumped correctly (e.g., 1.2.3 ‚Üí 1.2.4, 1.3.0, or 2.0.0). To calculate acceptable versions after an increase, there is the <code>christian-draeger/increment-semantic-version@1.2.3</code> action for GitHub that can calculate the next patch, minor, and major version, which can later be compared with the actual version that was manually increased by the developer.</li><li><strong>Build DEV Image:</strong> Builds a unique, versioned DEV image for the edp-flows. It can be tagged using the pattern:</li></ol><p><code>${VERSION}-pr-${{ github.event.number }}-run-${{ github.run_number }}</code></p><p>The image is then pushed to the GitHub Container Registry. In the <a href=\"https://thescalableway.com/blog/getting-to-your-first-flow-run-prefect-worker-and-deployment-setup/\" rel=\"noopener\">previous blog post</a>, we prepared an example Dockerfile. Here‚Äôs what a GitHub workflow to handle it might look like:</p><pre class=\"language-yaml\"><code class=\"language-yaml\"><span class=\"token key atrule\">jobs</span><span class=\"token punctuation\">:</span>\n  <span class=\"token key atrule\">build-and-push</span><span class=\"token punctuation\">:</span>\n    <span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span> Build and push docker image\n    <span class=\"token key atrule\">runs-on</span><span class=\"token punctuation\">:</span> ubuntu<span class=\"token punctuation\">-</span>latest\n    <span class=\"token key atrule\">steps</span><span class=\"token punctuation\">:</span>\n      <span class=\"token punctuation\">-</span> <span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span> Checkout Repository\n        <span class=\"token key atrule\">uses</span><span class=\"token punctuation\">:</span> actions/checkout@v4\n\n      <span class=\"token punctuation\">-</span> <span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span> Set up Docker Buildx\n        <span class=\"token key atrule\">uses</span><span class=\"token punctuation\">:</span> docker/setup<span class=\"token punctuation\">-</span>buildx<span class=\"token punctuation\">-</span>action@v3\n\n      <span class=\"token punctuation\">-</span> <span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span> Create a multi<span class=\"token punctuation\">-</span>platform builder\n        <span class=\"token key atrule\">run</span><span class=\"token punctuation\">:</span> <span class=\"token punctuation\">|</span><span class=\"token scalar string\">\n          docker buildx create --name builder --driver docker-container --use\n          docker buildx inspect --bootstrap</span>\n\n      <span class=\"token punctuation\">-</span> <span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span> Login to GitHub Container Registry\n        <span class=\"token key atrule\">uses</span><span class=\"token punctuation\">:</span> docker/login<span class=\"token punctuation\">-</span>action@v3\n        <span class=\"token key atrule\">with</span><span class=\"token punctuation\">:</span>\n          <span class=\"token key atrule\">registry</span><span class=\"token punctuation\">:</span> $<span class=\"token punctuation\">{</span><span class=\"token punctuation\">{</span> inputs.registry <span class=\"token punctuation\">}</span><span class=\"token punctuation\">}</span>\n          <span class=\"token key atrule\">username</span><span class=\"token punctuation\">:</span> $<span class=\"token punctuation\">{</span><span class=\"token punctuation\">{</span> github.actor <span class=\"token punctuation\">}</span><span class=\"token punctuation\">}</span>\n          <span class=\"token key atrule\">password</span><span class=\"token punctuation\">:</span> $<span class=\"token punctuation\">{</span><span class=\"token punctuation\">{</span> secrets.GITHUB_TOKEN <span class=\"token punctuation\">}</span><span class=\"token punctuation\">}</span>\n\n      <span class=\"token punctuation\">-</span> <span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span> <span class=\"token string\">\"Build and Push Docker Image\"</span>\n        <span class=\"token key atrule\">uses</span><span class=\"token punctuation\">:</span> docker/build<span class=\"token punctuation\">-</span>push<span class=\"token punctuation\">-</span>action@v6\n        <span class=\"token key atrule\">with</span><span class=\"token punctuation\">:</span>\n          <span class=\"token key atrule\">context</span><span class=\"token punctuation\">:</span> ./\n          <span class=\"token key atrule\">file</span><span class=\"token punctuation\">:</span> $<span class=\"token punctuation\">{</span><span class=\"token punctuation\">{</span> inputs.dockerfile <span class=\"token punctuation\">}</span><span class=\"token punctuation\">}</span>\n          <span class=\"token key atrule\">platforms</span><span class=\"token punctuation\">:</span> $<span class=\"token punctuation\">{</span><span class=\"token punctuation\">{</span> inputs.platforms <span class=\"token punctuation\">}</span><span class=\"token punctuation\">}</span>\n          <span class=\"token key atrule\">push</span><span class=\"token punctuation\">:</span> $<span class=\"token punctuation\">{</span><span class=\"token punctuation\">{</span> inputs.push <span class=\"token punctuation\">}</span><span class=\"token punctuation\">}</span>\n          <span class=\"token key atrule\">tags</span><span class=\"token punctuation\">:</span> $<span class=\"token punctuation\">{</span><span class=\"token punctuation\">{</span> inputs.registry <span class=\"token punctuation\">}</span><span class=\"token punctuation\">}</span>/$<span class=\"token punctuation\">{</span><span class=\"token punctuation\">{</span> inputs.organization<span class=\"token punctuation\">}</span><span class=\"token punctuation\">}</span>/$<span class=\"token punctuation\">{</span><span class=\"token punctuation\">{</span> inputs.image_name <span class=\"token punctuation\">}</span><span class=\"token punctuation\">}</span><span class=\"token punctuation\">:</span>$<span class=\"token punctuation\">{</span><span class=\"token punctuation\">{</span> inputs.image_tag <span class=\"token punctuation\">}</span><span class=\"token punctuation\">}</span>\n\n      <span class=\"token punctuation\">-</span> <span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span> Cleanup the builder\n        <span class=\"token key atrule\">run</span><span class=\"token punctuation\">:</span> docker buildx rm builder</code></pre><ol start=\"3\" class=\"list\"><li><strong>Update DEV Prefect work pool:</strong> Updates the base job template on the DEV environment to reference the newly built Docker image. During this step, it is essential to update the image inside the <code>baseJobTemplate</code> definition to the newly created one, which can be handled even with a simple replacement:</li></ol><pre class=\"language-yaml\"><code class=\"language-yaml\"><span class=\"token key atrule\">jobs</span><span class=\"token punctuation\">:</span>\n  <span class=\"token key atrule\">prefect-worker-helm</span><span class=\"token punctuation\">:</span>\n    <span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span> Prepare prefect worker\n    <span class=\"token key atrule\">runs-on</span><span class=\"token punctuation\">:</span> $<span class=\"token punctuation\">{</span><span class=\"token punctuation\">{</span> inputs.runs_on <span class=\"token punctuation\">}</span><span class=\"token punctuation\">}</span>\n    <span class=\"token key atrule\">steps</span><span class=\"token punctuation\">:</span>\n      <span class=\"token punctuation\">-</span> <span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span> Checkout Repository\n        <span class=\"token key atrule\">uses</span><span class=\"token punctuation\">:</span> actions/checkout@v4\n\n      <span class=\"token punctuation\">-</span> <span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span> Add necessary dependencies\n        <span class=\"token key atrule\">run</span><span class=\"token punctuation\">:</span> <span class=\"token punctuation\">|</span><span class=\"token scalar string\">\n          helm repo add prefect https://prefecthq.github.io/prefect-helm\n          helm repo update prefect</span>\n\n      <span class=\"token punctuation\">-</span> <span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span> Create prefect namespace\n        <span class=\"token key atrule\">run</span><span class=\"token punctuation\">:</span> <span class=\"token punctuation\">|</span><span class=\"token scalar string\">\n          cat &lt;&lt;EOF | kubectl apply -f -\n          apiVersion: v1\n          kind: Namespace\n          metadata:\n            name: ${{ inputs.namespace }}\n          EOF</span>\n\n      <span class=\"token punctuation\">-</span> <span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span> Replace default flow image\n        <span class=\"token key atrule\">run</span><span class=\"token punctuation\">:</span> <span class=\"token punctuation\">|</span><span class=\"token scalar string\">\n          sed -i \"s|DEFAULT_FLOW_IMAGE|${{ inputs.default_flow_image }}|g\" ${{ inputs.helm_values_path }}</span>\n\n      <span class=\"token punctuation\">-</span> <span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span> Run Helm upgrade commands\n        <span class=\"token key atrule\">run</span><span class=\"token punctuation\">:</span> <span class=\"token punctuation\">|</span><span class=\"token scalar string\">\n          helm upgrade prefect-worker --install prefect/prefect-worker \\\n            -n ${{ inputs.namespace }} \\\n            --version ${{ inputs.prefect_chart_version }} \\\n            -f ${{ inputs.helm_values_path }}</span></code></pre><p>Just like with creating a namespace, any missing resources can also be created. Usually, this includes a secret with the <code>PREFECT_API_KEY</code> and <code>registry credentials</code> secret for downloading private flow images, but additional secrets or configurations may also be needed.</p><h5 id=\"after-pr-marge\"><a href=\"#after-pr-marge\" class=\"heading-anchor\">After PR Marge</a></h5><ol class=\"list\"><li><strong>Tag and Release:</strong> A new GitHub tag and release are created for the updated version. Many ready-made actions are available online. For example, we can define a job like this:</li></ol><pre class=\"language-yaml\"><code class=\"language-yaml\"><span class=\"token key atrule\">jobs</span><span class=\"token punctuation\">:</span>\n  <span class=\"token key atrule\">prepare</span><span class=\"token punctuation\">:</span>\n    <span class=\"token key atrule\">if</span><span class=\"token punctuation\">:</span> $<span class=\"token punctuation\">{</span><span class=\"token punctuation\">{</span> github.event.pull_request.merged <span class=\"token punctuation\">}</span><span class=\"token punctuation\">}</span>\n    <span class=\"token key atrule\">runs-on</span><span class=\"token punctuation\">:</span> ubuntu<span class=\"token punctuation\">-</span>latest\n    <span class=\"token key atrule\">steps</span><span class=\"token punctuation\">:</span>\n      <span class=\"token punctuation\">-</span> <span class=\"token key atrule\">uses</span><span class=\"token punctuation\">:</span> actions/checkout@v4\n\n      <span class=\"token punctuation\">-</span> <span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span> Get version from pyproject.toml\n        <span class=\"token key atrule\">id</span><span class=\"token punctuation\">:</span> get<span class=\"token punctuation\">-</span>version\n        <span class=\"token key atrule\">run</span><span class=\"token punctuation\">:</span> echo \"version=$(grep <span class=\"token punctuation\">-</span>Po '(<span class=\"token punctuation\">?</span>&lt;=^version = \")<span class=\"token punctuation\">[</span>^\"<span class=\"token punctuation\">]</span><span class=\"token important\">*'</span> pyproject.toml)\" <span class=\"token punctuation\">|</span> tee <span class=\"token punctuation\">-</span>a $GITHUB_OUTPUT\n\n    <span class=\"token key atrule\">outputs</span><span class=\"token punctuation\">:</span>\n      <span class=\"token key atrule\">VERSION</span><span class=\"token punctuation\">:</span> $<span class=\"token punctuation\">{</span><span class=\"token punctuation\">{</span> steps.get<span class=\"token punctuation\">-</span>version.outputs.version <span class=\"token punctuation\">}</span><span class=\"token punctuation\">}</span>\n\n  <span class=\"token key atrule\">tag-and-release</span><span class=\"token punctuation\">:</span>\n    <span class=\"token key atrule\">if</span><span class=\"token punctuation\">:</span> $<span class=\"token punctuation\">{</span><span class=\"token punctuation\">{</span> github.event.pull_request.merged <span class=\"token punctuation\">}</span><span class=\"token punctuation\">}</span>\n    <span class=\"token key atrule\">needs</span><span class=\"token punctuation\">:</span> <span class=\"token punctuation\">[</span>prepare<span class=\"token punctuation\">]</span>\n    <span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span> Tag and release new version (if applicable)\n    <span class=\"token key atrule\">runs-on</span><span class=\"token punctuation\">:</span> ubuntu<span class=\"token punctuation\">-</span>latest\n    <span class=\"token key atrule\">permissions</span><span class=\"token punctuation\">:</span>\n      <span class=\"token key atrule\">contents</span><span class=\"token punctuation\">:</span> write\n    <span class=\"token key atrule\">outputs</span><span class=\"token punctuation\">:</span>\n      <span class=\"token key atrule\">TAG_CREATED</span><span class=\"token punctuation\">:</span> $<span class=\"token punctuation\">{</span><span class=\"token punctuation\">{</span> steps.check<span class=\"token punctuation\">-</span>tag.outputs.exists <span class=\"token tag\">!=</span> 'true' <span class=\"token punctuation\">}</span><span class=\"token punctuation\">}</span>\n\n    <span class=\"token key atrule\">steps</span><span class=\"token punctuation\">:</span>\n      <span class=\"token punctuation\">-</span> <span class=\"token key atrule\">uses</span><span class=\"token punctuation\">:</span> actions/checkout@v4\n        <span class=\"token key atrule\">with</span><span class=\"token punctuation\">:</span>\n          <span class=\"token key atrule\">fetch-depth</span><span class=\"token punctuation\">:</span> <span class=\"token number\">0</span>\n\n      <span class=\"token punctuation\">-</span> <span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span> Check if a tag for this version already exists in the repo\n        <span class=\"token key atrule\">uses</span><span class=\"token punctuation\">:</span> mukunku/tag<span class=\"token punctuation\">-</span>exists<span class=\"token punctuation\">-</span>action@v1.6.0\n        <span class=\"token key atrule\">id</span><span class=\"token punctuation\">:</span> check<span class=\"token punctuation\">-</span>tag\n        <span class=\"token key atrule\">with</span><span class=\"token punctuation\">:</span>\n          <span class=\"token key atrule\">tag</span><span class=\"token punctuation\">:</span> v$<span class=\"token punctuation\">{</span><span class=\"token punctuation\">{</span> needs.prepare.outputs.VERSION <span class=\"token punctuation\">}</span><span class=\"token punctuation\">}</span>\n\n      <span class=\"token punctuation\">-</span> <span class=\"token key atrule\">uses</span><span class=\"token punctuation\">:</span> fregante/setup<span class=\"token punctuation\">-</span>git<span class=\"token punctuation\">-</span>user@v2\n        <span class=\"token key atrule\">if</span><span class=\"token punctuation\">:</span> steps.check<span class=\"token punctuation\">-</span>tag.outputs.exists <span class=\"token tag\">!=</span> 'true'\n\n      <span class=\"token punctuation\">-</span> <span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span> Publish the new tag\n        <span class=\"token key atrule\">if</span><span class=\"token punctuation\">:</span> steps.check<span class=\"token punctuation\">-</span>tag.outputs.exists <span class=\"token tag\">!=</span> 'true'\n        <span class=\"token key atrule\">run</span><span class=\"token punctuation\">:</span> <span class=\"token punctuation\">|</span><span class=\"token scalar string\">\n          git tag -a v${{ needs.prepare.outputs.VERSION }} -m \"Release v${{ needs.prepare.outputs.VERSION }}\"\n          git push origin v${{ needs.prepare.outputs.VERSION }}</span>\n\n      <span class=\"token punctuation\">-</span> <span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span> Create a release\n        <span class=\"token key atrule\">if</span><span class=\"token punctuation\">:</span> steps.check<span class=\"token punctuation\">-</span>tag.outputs.exists <span class=\"token tag\">!=</span> 'true'\n        <span class=\"token key atrule\">uses</span><span class=\"token punctuation\">:</span> ncipollo/release<span class=\"token punctuation\">-</span>action@v1\n        <span class=\"token key atrule\">with</span><span class=\"token punctuation\">:</span>\n          <span class=\"token key atrule\">generateReleaseNotes</span><span class=\"token punctuation\">:</span> <span class=\"token boolean important\">true</span>\n          <span class=\"token key atrule\">tag</span><span class=\"token punctuation\">:</span> v$<span class=\"token punctuation\">{</span><span class=\"token punctuation\">{</span> needs.prepare.outputs.VERSION <span class=\"token punctuation\">}</span><span class=\"token punctuation\">}</span></code></pre><p>We can utilize an additional <code>prepare</code> job that will pre-define values used later in our workflow, simplifying its logic.</p><ol class=\"list\"><li><strong>Build PROD Image:</strong> Builds PROD image for the edp-flows, tagging it with <code>${VERSION}</code> and pushing it to the GitHub container registry. The same template used for building the DEV image can be reused, but only the image tag can be changed.</li><li><strong>Update DEV Prefect work pool:</strong> Updates the base job template in the DEV environment to use the new Docker image.</li><li><strong>Update PROD Prefect work pool:</strong> Updates the base job template in the PROD environment to use the new Docker image.</li></ol><h4 id=\"workflow-2-prefect-worker-updates\"><a href=\"#workflow-2-prefect-worker-updates\" class=\"heading-anchor\">Workflow 2: Prefect Worker Updates</a></h4><p><is-land on:idle></is-land></p><dialog class=\"flow modal8\"><button autofocus class=\"button\">Close</button><picture><source type=\"image/webp\" srcset=\"/img/6XE6y67Dgw-960.webp 960w, /img/6XE6y67Dgw-1600.webp 1600w\" sizes=\"auto\"><img loading=\"lazy\" decoding=\"async\" src=\"/img/6XE6y67Dgw-960.jpeg\" alt=\"prefect worker updates\" width=\"1600\" height=\"741\" srcset=\"/img/6XE6y67Dgw-960.jpeg 960w, /img/6XE6y67Dgw-1600.jpeg 1600w\" sizes=\"auto\"></picture></dialog><button data-index=\"8\"><picture><source type=\"image/webp\" srcset=\"/img/6XE6y67Dgw-960.webp 960w, /img/6XE6y67Dgw-1600.webp 1600w\" sizes=\"auto\"><img loading=\"lazy\" decoding=\"async\" src=\"/img/6XE6y67Dgw-960.jpeg\" alt=\"prefect worker updates\" width=\"1600\" height=\"741\" srcset=\"/img/6XE6y67Dgw-960.jpeg 960w, /img/6XE6y67Dgw-1600.jpeg 1600w\" sizes=\"auto\"></picture></button><p></p><p>This workflow is triggered by changes to any of the following files:</p><ul class=\"list\"><li><code>etc/helm_values/prefect-worker/values-dev.yaml</code></li><li><code>etc/helm_values/prefect-worker/values-prod.yaml</code></li></ul><h5 id=\"pull-request-steps-1\"><a href=\"#pull-request-steps-1\" class=\"heading-anchor\">Pull Request Steps</a></h5><ol class=\"list\"><li><strong>Binary packages check/install</strong>: Installs any required binaries (like K3s and Helm) on the DEV virtual machine.</li><li><strong>Update DEV Prefect work pool:</strong> Updates the Prefect worker‚Äôs base job template on DEV environment to apply any required configuration changes.</li></ol><h5 id=\"after-pr-merge\"><a href=\"#after-pr-merge\" class=\"heading-anchor\">After PR Merge</a></h5><ol class=\"list\"><li><strong>Binary packages check/install</strong>: During the first execution, the necessary binaries (K3s and Helm) are installed on the PROD virtual machine.</li><li><strong>Update DEV Prefect work pool:</strong> Updates the Prefect worker‚Äôs base job template on DEV environment to apply any required configuration changes.</li><li><strong>Update PROD Prefect work pool:</strong> Updates the Prefect worker‚Äôs base job template on PROD environment to apply any required configuration changes.</li></ol><p><strong><em>Note:</em></strong><em> These first two workflows affect infrastructure only; they don‚Äôt touch actual Prefect deployments. The next workflow handles that.</em></p><h4 id=\"workflow-3-prefect-deployment-orchestration\"><a href=\"#workflow-3-prefect-deployment-orchestration\" class=\"heading-anchor\">Workflow 3: Prefect Deployment Orchestration</a></h4><p><is-land on:idle></is-land></p><dialog class=\"flow modal9\"><button autofocus class=\"button\">Close</button><picture><source type=\"image/webp\" srcset=\"/img/jOHcunnyPS-960.webp 960w, /img/jOHcunnyPS-1600.webp 1600w\" sizes=\"auto\"><img loading=\"lazy\" decoding=\"async\" src=\"/img/jOHcunnyPS-960.jpeg\" alt=\"prefect deployment orchestration\" width=\"1600\" height=\"598\" srcset=\"/img/jOHcunnyPS-960.jpeg 960w, /img/jOHcunnyPS-1600.jpeg 1600w\" sizes=\"auto\"></picture></dialog><button data-index=\"9\"><picture><source type=\"image/webp\" srcset=\"/img/jOHcunnyPS-960.webp 960w, /img/jOHcunnyPS-1600.webp 1600w\" sizes=\"auto\"><img loading=\"lazy\" decoding=\"async\" src=\"/img/jOHcunnyPS-960.jpeg\" alt=\"prefect deployment orchestration\" width=\"1600\" height=\"598\" srcset=\"/img/jOHcunnyPS-960.jpeg 960w, /img/jOHcunnyPS-1600.jpeg 1600w\" sizes=\"auto\"></picture></button><p></p><p>Triggered by changes to <code>prefect.yaml</code> or any source code within src directory.</p><h5 id=\"pull-request-steps-2\"><a href=\"#pull-request-steps-2\" class=\"heading-anchor\">Pull Request Steps</a></h5><p><strong>1. Identify Modified Deployments:</strong> In a simplified version, we can register all existing deployments. It can be handled with a helper script with such logic:</p><pre class=\"language-yaml\"><code class=\"language-yaml\"><span class=\"token key atrule\">jobs</span><span class=\"token punctuation\">:</span>\n  <span class=\"token key atrule\">apply-deployments</span><span class=\"token punctuation\">:</span>\n    <span class=\"token key atrule\">runs-on</span><span class=\"token punctuation\">:</span> ubuntu<span class=\"token punctuation\">-</span>latest\n    <span class=\"token key atrule\">steps</span><span class=\"token punctuation\">:</span>\n      <span class=\"token punctuation\">-</span> <span class=\"token key atrule\">uses</span><span class=\"token punctuation\">:</span> actions/checkout@v4\n        <span class=\"token key atrule\">with</span><span class=\"token punctuation\">:</span>\n          <span class=\"token key atrule\">fetch-depth</span><span class=\"token punctuation\">:</span> <span class=\"token number\">0</span>\n      \n      <span class=\"token punctuation\">-</span> <span class=\"token key atrule\">uses</span><span class=\"token punctuation\">:</span> actions/setup<span class=\"token punctuation\">-</span>python@v5\n        <span class=\"token key atrule\">with</span><span class=\"token punctuation\">:</span>\n          <span class=\"token key atrule\">python-version-file</span><span class=\"token punctuation\">:</span> <span class=\"token string\">\".python-version\"</span>\n\n      <span class=\"token punctuation\">-</span> <span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span> Install dependencies\n        <span class=\"token key atrule\">run</span><span class=\"token punctuation\">:</span> pip install <span class=\"token punctuation\">-</span>q PyYAML prefect\n\n      <span class=\"token punctuation\">-</span> <span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span> Get all deployments from prefect.yaml\n        <span class=\"token key atrule\">if</span><span class=\"token punctuation\">:</span> $<span class=\"token punctuation\">{</span><span class=\"token punctuation\">{</span> inputs.deployments == 'all' <span class=\"token punctuation\">}</span><span class=\"token punctuation\">}</span>\n        <span class=\"token key atrule\">run</span><span class=\"token punctuation\">:</span> echo \"DEPLOYMENT_NAMES=$(cat prefect.yaml <span class=\"token punctuation\">|</span> yq '.deployments<span class=\"token punctuation\">[</span><span class=\"token punctuation\">]</span>.name' <span class=\"token punctuation\">|</span> paste <span class=\"token punctuation\">-</span>sd \"<span class=\"token punctuation\">,</span>\")\" <span class=\"token punctuation\">|</span> tee <span class=\"token punctuation\">-</span>a $GITHUB_ENV</code></pre><p>As a target solution, we want a script that detects changes to deployments in the <code>prefect.yaml</code> by comparing the main branch with the pull request branch. This way, <code>DEPLOYMENT_NAMES</code> will include only the modified deployments. The script can also detect removed deployments, helping with housekeeping in Prefect Cloud.</p><p><strong>2. Apply to DEV:</strong> Applies the modified deployments to the DEV Prefect workspace, referencing the pull request branch. Assuming that we have only modified deployments provided in <code>DEPLOYMENT_NAMES</code>, the registration script can look like this:</p><pre class=\"language-yaml\"><code class=\"language-yaml\">      <span class=\"token punctuation\">-</span> <span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span> Set branch '$<span class=\"token punctuation\">{</span>GITHUB_HEAD_REF<span class=\"token punctuation\">}</span>' in prefect.yaml\n        <span class=\"token key atrule\">run</span><span class=\"token punctuation\">:</span> <span class=\"token punctuation\">|</span><span class=\"token scalar string\">\n          yq -i '.pull[] |= (select(has(\"prefect.deployments.steps.git_clone\")) \n            | .[\"prefect.deployments.steps.git_clone\"].branch = \"${GITHUB_HEAD_REF}\" | .) // .' prefect.yaml</span>\n\n      <span class=\"token punctuation\">-</span> <span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span> Register all deployments\n        <span class=\"token key atrule\">run</span><span class=\"token punctuation\">:</span> <span class=\"token punctuation\">|</span><span class=\"token scalar string\">\n              for deployment_name in $(echo ${{ env.DEPLOYMENT_NAMES }} | tr ',' '\\n'); do</span>\n\n        prefect <span class=\"token punctuation\">-</span><span class=\"token punctuation\">-</span>no<span class=\"token punctuation\">-</span>prompt deploy \\\n            <span class=\"token punctuation\">-</span>n \"$deployment_name\"\n            <span class=\"token punctuation\">-</span><span class=\"token punctuation\">-</span>tag $<span class=\"token punctuation\">{</span>GITHUB_HEAD_REF<span class=\"token punctuation\">}</span>\n    done</code></pre><p>Deployments are created with scheduling disabled to allow manual testing in the DEV environment. The preparation step sets <code>GITHUB_HEAD_REF</code> as a branch reference for the <code>git_clone</code> step at runtime.</p><p><strong>3. Testing &amp; Merge:</strong> Modified deployments and flows are tested. As there is no schedule enabled on the deployment on DEV workspace, it needs to be triggered and tested manually. Upon approval, changes are merged into the main branch.</p><h5 id=\"after-pr-merge-1\"><a href=\"#after-pr-merge-1\" class=\"heading-anchor\">After PR Merge</a></h5><p><strong>1. Identify Modified Deployments:</strong> Detects all updated deployments in the prefect.yaml, just like in step 1.</p><p><strong>2. Apply to PROD:</strong> Apply the deployments to the PROD Prefect workspace, using the main branch. Scheduling is enabled with this command:</p><pre class=\"language-bash\"><code class=\"language-bash\"><span class=\"token function-name function\">get_flow_name_for_deployment_from_prefect_yaml</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span> <span class=\"token punctuation\">{</span>\n    <span class=\"token comment\"># Fetch flow name for a deployment from prefect.yaml.</span>\n\n    <span class=\"token assign-left variable\">deployment_name</span><span class=\"token operator\">=</span><span class=\"token variable\">$1</span>\n\n    <span class=\"token builtin class-name\">echo</span> <span class=\"token string\">\"Retrieving flow name for deployment '<span class=\"token variable\">$deployment_name</span>'...\"</span> <span class=\"token operator\">&gt;</span><span class=\"token file-descriptor important\">&amp;2</span>\n\n    <span class=\"token assign-left variable\">deployment_entrypoint</span><span class=\"token operator\">=</span><span class=\"token variable\"><span class=\"token variable\">$(</span><span class=\"token function\">cat</span> prefect.yaml <span class=\"token operator\">|</span> yq <span class=\"token string\">'.deployments[] | select(.name == \"'</span>$deployment_name<span class=\"token string\">'\") | .entrypoint'</span><span class=\"token variable\">)</span></span>\n    <span class=\"token assign-left variable\">flow_name</span><span class=\"token operator\">=</span><span class=\"token variable\"><span class=\"token variable\">$(</span><span class=\"token builtin class-name\">echo</span> $deployment_entrypoint <span class=\"token operator\">|</span> <span class=\"token function\">cut</span> -d<span class=\"token string\">':'</span> <span class=\"token parameter variable\">-f2</span><span class=\"token variable\">)</span></span>\n    <span class=\"token assign-left variable\">flow_name_kebab_case</span><span class=\"token operator\">=</span><span class=\"token variable\"><span class=\"token variable\">$(</span><span class=\"token builtin class-name\">echo</span> $flow_name <span class=\"token operator\">|</span> <span class=\"token function\">sed</span> <span class=\"token string\">'s/_/-/g'</span><span class=\"token variable\">)</span></span>\n\n    <span class=\"token comment\"># Return the flow name converted to kebab case, as this is what prefect CLI commands expect.</span>\n    <span class=\"token builtin class-name\">echo</span> <span class=\"token variable\">$flow_name_kebab_case</span>\n<span class=\"token punctuation\">}</span>\n\n<span class=\"token assign-left variable\">flow_name</span><span class=\"token operator\">=</span><span class=\"token variable\"><span class=\"token variable\">$(</span>get_flow_name_for_deployment_from_prefect_yaml <span class=\"token string\">\"<span class=\"token variable\">$deployment_name</span>\"</span><span class=\"token variable\">)</span></span>\n<span class=\"token assign-left variable\">schedule_id</span><span class=\"token operator\">=</span><span class=\"token variable\"><span class=\"token variable\">$(</span>prefect deployment schedule <span class=\"token function\">ls</span> $flow_name/$deployment_name <span class=\"token operator\">|</span> <span class=\"token function\">grep</span> <span class=\"token string\">'‚îÇ'</span> <span class=\"token operator\">|</span> <span class=\"token function\">awk</span> <span class=\"token parameter variable\">-F</span> <span class=\"token string\">'‚îÇ'</span> <span class=\"token string\">'{print $2}'</span> <span class=\"token operator\">|</span> <span class=\"token function\">sed</span> <span class=\"token string\">'s/^[[:space:]]*//;s/[[:space:]]*$//'</span><span class=\"token variable\">)</span></span>\nprefect deployment schedule resume <span class=\"token variable\">$flow_name</span>/<span class=\"token variable\">$deployment_name</span> <span class=\"token variable\">$schedule_id</span></code></pre><p>This script can be added to the workflow to automate schedule enabling.</p><p><strong>3. Synv DEV:</strong> After deleting the pull request branch, reapply deployments to the DEV Prefect workspace, now referencing the <code>main</code> branch. Scheduling remains disabled for DEV.</p><p>You can verify the <strong>branch references</strong> used in deployments by checking the assigned tag or the Configuration tab in Prefect Cloud. For example, in the screenshot below, the deployment runs on branch <code>feature_branch_1</code>.</p><p><is-land on:idle></is-land></p><dialog class=\"flow modal10\"><button autofocus class=\"button\">Close</button><picture><source type=\"image/webp\" srcset=\"/img/U2Nw6rtxZH-605.webp 605w\" sizes=\"auto\"><img loading=\"lazy\" decoding=\"async\" src=\"/img/U2Nw6rtxZH-605.jpeg\" alt=\"verify branch references\" width=\"605\" height=\"408\"></picture></dialog><button data-index=\"10\"><picture><source type=\"image/webp\" srcset=\"/img/U2Nw6rtxZH-605.webp 605w\" sizes=\"auto\"><img loading=\"lazy\" decoding=\"async\" src=\"/img/U2Nw6rtxZH-605.jpeg\" alt=\"verify branch references\" width=\"605\" height=\"408\"></picture></button><p></p><h2 id=\"conclusion-and-series-summary\"><a href=\"#conclusion-and-series-summary\" class=\"heading-anchor\">Conclusion &amp; series summary</a></h2><p>This article wraps up our four-part journey to building a modern, automated data platform‚Äîfrom high-level architecture to fully hands-off, production-ready operations. Along the way, we‚Äôve shown how each layer, including architecture, infrastructure, orchestration, and automation, works together to create a resilient, scalable foundation for data engineering.</p><p>Let‚Äôs quickly recap:</p><p><a href=\"https://thescalableway.com/blog/deploying-prefect-on-any-cloud-using-a-single-virtual-machine/\" rel=\"noopener\"><strong>Part 1</strong></a> focused on architectural decisions, demonstrating how a lightweight Kubernetes setup on a single VM can enable rapid adoption and growth, even for teams just starting with cloud-native data platforms.</p><p><a href=\"https://thescalableway.com/blog/how-to-setup-data-platform-infrastructure-on-google-cloud-platform-with-terraform/\" rel=\"noopener\"><strong>Part 2</strong></a> moved from design to implementation, automating cloud infrastructure provisioning with Terraform to ensure consistency, reproducibility, and cloud-agnostic flexibility.</p><p><a href=\"https://thescalableway.com/blog/getting-to-your-first-flow-run-prefect-worker-and-deployment-setup/\" rel=\"noopener\"><strong>Part 3</strong></a> took us deeper into operations, guiding you through containerized flow execution, Prefect worker configuration, and deployment management‚Äîhelping you run your first data ingestion flows confidently.</p><p>And here in <strong>Part 4</strong>, we brought everything together by introducing CI/CD automation. We showed how three specialized workflows for Docker images, Prefect workers, and deployment orchestration help reduce manual errors, maintain a single source of truth, and scale both your platform and your team. This kind of automation makes development smoother, testing more reliable, and production releases faster and safer.</p><p>The main takeaway is that adopting CI/CD for data platforms is not just about tools but about changing how your team works. Automation connects development and production, reduces risk, and frees your engineers to focus on data rather than infrastructure.</p><p>Thanks so much for following along. For more tips and updates, check out my <a href=\"https://thescalableway.com/author/karol-wolski/\" rel=\"noopener\"><strong>other articles</strong></a>, subscribe to our <strong>newsletter</strong>, and connect with me on <a href=\"https://www.linkedin.com/in/wolski-karol/\" rel=\"noopener\"><strong>LinkedIn</strong></a>. Let‚Äôs keep the conversation about smarter data platforms going.</p></div>",
      "date_published": "2025-07-03T11:00:00Z"
    }
    ,{
      "id": "http://localhost:8080/blog/scaling-secure-data-access-a-systematic-rbac-approach-using-entra-id/",
      "url": "http://localhost:8080/blog/scaling-secure-data-access-a-systematic-rbac-approach-using-entra-id/",
      "title": "Scaling Secure Data Access: A Systematic RBAC Approach Using Entra ID",
      "content_html": "<nav id=\"toc\" class=\"table-of-contents prose\"><ol><li class=\"flow\"><a href=\"#what-is-role-based-access-control-rbac\">What is Role-Based Access Control (RBAC)?</a></li><li class=\"flow\"><a href=\"#why-rbac-matters-in-modular-data-platforms\">Why RBAC Matters in Modular Data Platforms</a></li><li class=\"flow\"><a href=\"#phase-1-defining-user-personas\">Phase 1: Defining User Personas</a></li><li class=\"flow\"><a href=\"#phase-2-creating-entraid-groups\">Phase 2: Creating EntraID Groups</a></li><li class=\"flow\"><a href=\"#phase-3-mapping-users-to-entraid-groups\">Phase 3: Mapping Users to EntraID Groups</a></li><li class=\"flow\"><a href=\"#phase-4-access-configuration-and-permissions-setup\">Phase 4: Access Configuration and Permissions Setup</a></li><li class=\"flow\"><a href=\"#phase-5-audit-and-continuous-monitoring\">Phase 5: Audit and Continuous Monitoring</a></li><li class=\"flow\"><a href=\"#final-thoughts\">Final Thoughts</a></li></ol></nav><p><span id=\"toc-skipped\" class=\"visually-hidden\"></span></p><div class=\"flow prose\"><p></p><p>When data platforms grow in scale and complexity, so do the risks. Suddenly, you‚Äôre juggling dozens of tools, a growing number of users, and multiple layers of sensitive data, while trying to balance it all with security controls. Managing who gets access to what (and making sure they only get only what they need) quickly becomes a full-time job.</p><p>This reality requires flexible and manageable access controls. That‚Äôs exactly where <strong>Role-Based Access Control (RBAC)</strong> enters the scene.</p><h2 id=\"what-is-role-based-access-control-rbac\"><a href=\"#what-is-role-based-access-control-rbac\" class=\"heading-anchor\">What is Role-Based Access Control (RBAC)?</a></h2><p>RBAC is a foundational security model that has evolved from military protocols to become a standard in IT systems. In this paradigm, user permissions are assigned to roles, rather than managing access for each user individually. This approach brings several advantages:</p><ul class=\"list\"><li><strong>Scalability:</strong> Manage access for large user bases efficiently.</li><li><strong>Consistency:</strong> Reduces the risk of misconfiguration.</li><li><strong>Auditability:</strong> Makes it easier to track who has access to what.</li><li><strong>Modularity:</strong> Aligns well with component-based data architectures.</li><li><strong>Savings</strong> Cuts down on repetitive access management tasks.</li></ul><p>At its core, RBAC secures both organizational assets and resources‚Äîa distinction that‚Äôs especially relevant in modular data platforms.</p><p><strong>Assets</strong> are digital, valuable objects used across the organization to support data-driven decisions. These include data models, dashboards, reports, and machine learning models.</p><p><strong>Resources</strong> are the underlying technical components or capabilities that enable the creation and delivery of those assets. These include EKS clusters, storage systems (e.g., S3, ADLS), databases, orchestrators, pipelines, and monitoring tools.</p><p>Access to both <strong>assets and resources</strong> should be managed consistently and securely. In most cases, Role-Based Access Control provides a solid and sufficient framework to do exactly that.</p><h2 id=\"why-rbac-matters-in-modular-data-platforms\"><a href=\"#why-rbac-matters-in-modular-data-platforms\" class=\"heading-anchor\">Why RBAC Matters in Modular Data Platforms</a></h2><p>RBAC is essential in <strong>modular data platforms</strong>. These platforms serve diverse users (data engineers, analysts, scientists, AI engineers, and business users), each with specific access needs. The challenge lies in designing a system that grants appropriate access without compromising security or compliance.</p><p>This article walks through a structured, five-phase RBAC implementation using Microsoft Entra ID‚Äîa widely adopted identity and access management tool. The end goal of this process is to establish a streamlined, scalable access workflow, just like the one visualized below:</p><p><is-land on:idle></is-land></p><dialog class=\"flow modal46\"><button autofocus class=\"button\">Close</button><picture><source type=\"image/webp\" srcset=\"/img/SbIhpGpl98-960.webp 960w, /img/SbIhpGpl98-1600.webp 1600w\" sizes=\"auto\"><img loading=\"lazy\" decoding=\"async\" src=\"/img/SbIhpGpl98-960.jpeg\" alt=\"data accesswith entra id\" width=\"1600\" height=\"916\" srcset=\"/img/SbIhpGpl98-960.jpeg 960w, /img/SbIhpGpl98-1600.jpeg 1600w\" sizes=\"auto\"></picture></dialog><button data-index=\"46\"><picture><source type=\"image/webp\" srcset=\"/img/SbIhpGpl98-960.webp 960w, /img/SbIhpGpl98-1600.webp 1600w\" sizes=\"auto\"><img loading=\"lazy\" decoding=\"async\" src=\"/img/SbIhpGpl98-960.jpeg\" alt=\"data accesswith entra id\" width=\"1600\" height=\"916\" srcset=\"/img/SbIhpGpl98-960.jpeg 960w, /img/SbIhpGpl98-1600.jpeg 1600w\" sizes=\"auto\"></picture></button><p></p><p><is-land on:idle></is-land></p><dialog class=\"flow modal47\"><button autofocus class=\"button\">Close</button><picture><source type=\"image/webp\" srcset=\"/img/ZwcOIWG-rZ-960.webp 960w, /img/ZwcOIWG-rZ-1600.webp 1600w\" sizes=\"auto\"><img loading=\"lazy\" decoding=\"async\" src=\"/img/ZwcOIWG-rZ-960.jpeg\" alt=\"data access to an enterprise data platform via entra id\" width=\"1600\" height=\"711\" srcset=\"/img/ZwcOIWG-rZ-960.jpeg 960w, /img/ZwcOIWG-rZ-1600.jpeg 1600w\" sizes=\"auto\"></picture></dialog><button data-index=\"47\"><picture><source type=\"image/webp\" srcset=\"/img/ZwcOIWG-rZ-960.webp 960w, /img/ZwcOIWG-rZ-1600.webp 1600w\" sizes=\"auto\"><img loading=\"lazy\" decoding=\"async\" src=\"/img/ZwcOIWG-rZ-960.jpeg\" alt=\"data access to an enterprise data platform via entra id\" width=\"1600\" height=\"711\" srcset=\"/img/ZwcOIWG-rZ-960.jpeg 960w, /img/ZwcOIWG-rZ-1600.jpeg 1600w\" sizes=\"auto\"></picture></button><p></p><h2 id=\"phase-1-defining-user-personas\"><a href=\"#phase-1-defining-user-personas\" class=\"heading-anchor\">Phase 1: Defining User Personas</a></h2><p>RBAC starts with clear role definitions, but many teams get stuck here. <em>Where to start? Which roles do we need? What defines a good role? Let‚Äôs stick to 77/755/744‚Ä¶</em></p><p>This is a critical step, as your entire access model depends on getting this right.</p><p>The best approach begins by listing <strong>key user personas</strong>‚Äîbroad categories of users with shared access needs, security considerations, and operational patterns‚Äîand refining them into sub-categories if necessary. Here are typical personas in a modern data platform:</p><ul class=\"list\"><li><strong>Data Platform Engineer/DevOps</strong> ‚Äì Needs broad access across infrastructure components. Responsible for developing, deploying, and maintaining platform services. Typically requires admin-level permissions across environments and tooling.</li><li><strong>Data Engineer</strong> ‚Äì Focuses on building and managing pipelines, data flows, and transformations. Requires read/write access in development and staging environments, limited access to production, and admin permissions in orchestration tools.</li><li><strong>Data Scientist </strong>‚Äì Engages in exploratory data analysis and model development. Needs read access across multiple domains, access to computational resources, and controlled write access to model deployment tools.</li><li><strong>Data Analyst</strong> ‚Äì Primarily interacts with modeled or normalized data. Requires read access to curated datasets and limited write access for storing results. May need orchestrator access to refresh or trigger reporting workflows.</li><li><strong>Business Analysts, BI Developers, and Report Creators</strong> ‚Äì Work mostly in presentation layers. They typically require read-only access to business-ready data and tools for dashboard creation. Their access often spans multiple domains, which makes clear scoping important.</li></ul><p><strong>Pro tip:</strong> If your organization uses data domains (e.g., Finance, HR, Sales), use them as attributes to refine personas. A matrix of personas vs. domains can simplify access mapping without adding unnecessary complexity.</p><h2 id=\"phase-2-creating-entraid-groups\"><a href=\"#phase-2-creating-entraid-groups\" class=\"heading-anchor\">Phase 2: Creating EntraID Groups</a></h2><p>With personas defined, the next step is to translate them into <strong>Microsoft EntraID security groups</strong>. They serve as the operational layer of the RBAC model, forming the backbone for scalable, repeatable permission management.</p><p>Entra ID groups do much more than just bundle users. They support critical capabilities for maintaining control as the platform scales, allowing audit trails, access reviews, and user lifecycle automation.</p><p>Each group should correspond to a specific persona, have a clearly defined owner, and include a domain context where applicable. For example, instead of creating a generic DataAnalyst group, consider a more specific DataAnalyst_Marketing group. Naming conventions should be meaningful, consistent, and designed to support future growth. A poorly named group today can create significant technical debt later if it needs to be split or redefined.</p><p>Security groups offer clear advantages over individual user permission assignments. They enable systematic and repeatable permission management that scales with organizational growth. When users move between roles or teams, administrators can adjust group memberships rather than managing permissions across individual services, reducing complexity, administrative overhead, and the risk of errors.</p><p>Some organizations also choose to create hierarchies of groups, where a parent group holds shared access, and child groups inherit those permissions while adding more specific scopes. This approach can work well if aligned with organizational structure, but it must be handled carefully to avoid unintentionally granting broader access than intended.</p><h2 id=\"phase-3-mapping-users-to-entraid-groups\"><a href=\"#phase-3-mapping-users-to-entraid-groups\" class=\"heading-anchor\">Phase 3: Mapping Users to EntraID Groups</a></h2><p>With groups in place, the focus shifts to assigning users to appropriate groups based on their roles and domain access requirements, if applicable.</p><p>The <strong>principle of least privilege</strong> should guide all user assignments, ensuring that individuals receive only the minimum access necessary to perform their current job responsibilities. Start conservatively and expand access only when it‚Äôs validated by actual business need and approved by the data platform owner.</p><p>Many organizations struggle with group management and hesitate to adopt group-based assignments until they can fully automate the process. However, waiting for perfect automation can delay real security improvements. Even a manual group assignment following the audit rules is more secure and manageable than using user-resource access management.</p><p>Entra ID supports <strong>dynamic membership rules</strong>, which allow users to be automatically assigned to groups based on attributes like department, job title, or location. For example, analysts in the Finance department can be automatically added to the DataAnalyst_Finance group using:</p><p><code>(user.department -eq \"Finance\") and (user.role -contains \"analyst\")</code></p><p>These rules can also be extended by integrating Entra ID with external systems like Workday, allowing logic based on a richer organizational context to be applied.</p><h2 id=\"phase-4-access-configuration-and-permissions-setup\"><a href=\"#phase-4-access-configuration-and-permissions-setup\" class=\"heading-anchor\">Phase 4: Access Configuration and Permissions Setup</a></h2><p>The access configuration phase <strong>translates group memberships into specific permissions</strong> across the modular data platform components. This phase requires a tools inventory, listing the components in use, and a data assets inventory, where the data catalog proves invaluable.</p><p>With the BoM and inventory in place, the next step is to enable EntraID as the identity provider. This can be achieved either through native support or via SSO, SAML, or OAuth. Once configured, RBAC is implemented at the component level by granting permissions to the appropriate groups.</p><p>This process should remain transparent to users while ensuring comprehensive logging for security monitoring and compliance purposes.</p><h2 id=\"phase-5-audit-and-continuous-monitoring\"><a href=\"#phase-5-audit-and-continuous-monitoring\" class=\"heading-anchor\">Phase 5: Audit and Continuous Monitoring</a></h2><p>Regular auditing is essential for effective RBAC maintenance, ensuring that access permissions remain appropriate and aligned with organizational policies. This phase systematically reviews user assignments, group configurations, and access patterns to detect potential security risks or compliance issues. Organizations should define <strong>regular audit schedules</strong>, typically at monthly or quarterly intervals.</p><p>To maintain a robust RBAC setup, the following steps should be followed:</p><p><strong>Step 1: Generate (new) User Overview</strong></p><ul class=\"list\"><li>The Data Platform Owner initiates the generation of EntraID audit reports, which is typically carried out by the EntraID Team.</li><li>The Data Platform Owner verifies the BoM and access levels per role.</li><li>The resulting report is shared with the respective data and tool owners.</li></ul><p><strong>Step 2: Review User Overview</strong></p><ul class=\"list\"><li>The Data Owner reviews the user access report.</li><li>They assess whether each user‚Äôs access is still appropriate.</li></ul><p><strong>Step 3: User Access Approval</strong></p><ul class=\"list\"><li>If all user access is deemed appropriate (approval), the process concludes with a notification sent to relevant stakeholders.</li><li>If any user is rejected:<ul class=\"list\"><li>The Data/Tool Owner requests that specific users be removed from relevant dashboards or reports.</li><li>The affected users are notified of their access removal.</li></ul></li></ul><p><strong>Step 4: Remove User Access</strong></p><ul class=\"list\"><li>The EntraID Team removes the specified users from the corresponding EntraID Group.</li><li>A confirmation notification is sent to finalize the update.</li></ul><p><is-land on:idle></is-land></p><dialog class=\"flow modal48\"><button autofocus class=\"button\">Close</button><picture><source type=\"image/webp\" srcset=\"/img/wYL-aCJeEe-960.webp 960w\" sizes=\"auto\"><img loading=\"lazy\" decoding=\"async\" src=\"/img/wYL-aCJeEe-960.jpeg\" alt=\"rbac workflow\" width=\"960\" height=\"540\"></picture></dialog><button data-index=\"48\"><picture><source type=\"image/webp\" srcset=\"/img/wYL-aCJeEe-960.webp 960w\" sizes=\"auto\"><img loading=\"lazy\" decoding=\"async\" src=\"/img/wYL-aCJeEe-960.jpeg\" alt=\"rbac workflow\" width=\"960\" height=\"540\"></picture></button><p></p><h2 id=\"final-thoughts\"><a href=\"#final-thoughts\" class=\"heading-anchor\">Final Thoughts</a></h2><p>Implementing RBAC in modular data platforms is not a one-time effort but a continuous, <strong>cyclical process</strong>. The five-phase methodology outlined here offers a scalable, repeatable framework that enhances security, reduces operational overhead, and improves transparency across the data ecosystem.</p><p>With Microsoft Entra ID as the backbone, organizations gain enterprise-grade identity management, automation support, and auditable group control. It enables sustainable, compliant access control by supporting the full user and permission lifecycle. When paired with regular audits, this approach ensures that RBAC remains aligned with evolving business needs and security risks, with audit findings feeding back into earlier phases for ongoing refinement.</p></div>",
      "date_published": "2025-06-23T09:30:00Z"
    }
    ,{
      "id": "http://localhost:8080/blog/getting-to-your-first-flow-run-prefect-worker-and-deployment-setup/",
      "url": "http://localhost:8080/blog/getting-to-your-first-flow-run-prefect-worker-and-deployment-setup/",
      "title": "Getting to Your First Flow Run: Prefect Worker &amp; Deployment Setup",
      "content_html": "<nav id=\"toc\" class=\"table-of-contents prose\"><ol><li class=\"flow\"><a href=\"#data-platform-components-overview\">Data Platform Components Overview</a></li><li class=\"flow\"><a href=\"#docker-image-for-flows-execution\">Docker Image for Flows Execution</a></li><li class=\"flow\"><a href=\"#prefect-worker\">Prefect Worker</a><ol><li class=\"flow\"><a href=\"#how-prefect-executes-flow-runs\">How Prefect Executes Flow Runs</a></li><li class=\"flow\"><a href=\"#prefect-worker-configuration\">Prefect Worker Configuration</a></li><li class=\"flow\"><a href=\"#example-base-job-template\">Example Base Job Template</a></li></ol></li><li class=\"flow\"><a href=\"#prefect-configuration-files\">Prefect Configuration Files</a><ol><li class=\"flow\"><a href=\"#example-prefect-flow\">Example Prefect Flow</a></li></ol></li><li class=\"flow\"><a href=\"#conclusion\">Conclusion</a><ol><li class=\"flow\"><a href=\"#whats-next\">What‚Äôs next?</a></li></ol></li></ol></nav><p><span id=\"toc-skipped\" class=\"visually-hidden\"></span></p><div class=\"flow prose\"><p></p><p>You‚Äôve laid the groundwork: the infrastructure is in place. The next logical step is turning that foundation into something functional, running your first data ingestion workflow. That moment when everything connects for the first time can feel like crossing an invisible line: from setup to real-world execution.<br>This article picks up where we left off. It‚Äôs the third part in a series designed to guide data engineers through the complete journey of building a modern data platform.</p><p>In <a href=\"https://thescalableway.com/blog/deploying-prefect-on-any-cloud-using-a-single-virtual-machine/\" rel=\"noopener\">Part 1</a>, I explored architectural approaches and proposed a lightweight Kubernetes setup running on a single VM. While it doesn‚Äôt offer full high availability, this setup has proven to be a practical starting point, especially for teams with limited cloud-native experience. It allows organizations to grow along the data maturity curve without the overhead of more complex solutions.</p><p><a href=\"https://thescalableway.com/blog/how-to-setup-data-platform-infrastructure-on-google-cloud-platform-with-terraform/\" rel=\"noopener\">Part 2</a> focused on provisioning the infrastructure using Terraform on Google Cloud Platform (GCP). We used GCP as an example, but the underlying architectural principles are cloud-agnostic and applicable across providers.</p><p>Now that the infrastructure is ready, this article walks through the next milestone: configuring all the components required to execute your first data ingestion workflow.</p><h2 id=\"data-platform-components-overview\"><a href=\"#data-platform-components-overview\" class=\"heading-anchor\">Data Platform Components Overview</a></h2><p>Here‚Äôs a high-level overview of a generic data platform architecture (as discussed in previous articles).</p><p><is-land on:idle></is-land></p><dialog class=\"flow modal16\"><button autofocus class=\"button\">Close</button><picture><source type=\"image/webp\" srcset=\"/img/NvijIMTwVi-960.webp 960w, /img/NvijIMTwVi-1600.webp 1600w\" sizes=\"auto\"><img loading=\"lazy\" decoding=\"async\" src=\"/img/NvijIMTwVi-960.jpeg\" alt=\"data platform architecture\" width=\"1600\" height=\"761\" srcset=\"/img/NvijIMTwVi-960.jpeg 960w, /img/NvijIMTwVi-1600.jpeg 1600w\" sizes=\"auto\"></picture></dialog><button data-index=\"16\"><picture><source type=\"image/webp\" srcset=\"/img/NvijIMTwVi-960.webp 960w, /img/NvijIMTwVi-1600.webp 1600w\" sizes=\"auto\"><img loading=\"lazy\" decoding=\"async\" src=\"/img/NvijIMTwVi-960.jpeg\" alt=\"data platform architecture\" width=\"1600\" height=\"761\" srcset=\"/img/NvijIMTwVi-960.jpeg 960w, /img/NvijIMTwVi-1600.jpeg 1600w\" sizes=\"auto\"></picture></button><p></p><p>While this article won‚Äôt cover the data warehouse or data lake, we will zoom in on the other key components that power workflow orchestration:</p><ul class=\"list\"><li><strong>GitHub</strong> as the version control system</li><li><strong>Prefect Cloud</strong> as the orchestration environment</li><li><strong>Prefect worker</strong> as the workflow orchestration system</li></ul><p>To prepare these, we‚Äôll walk through these essential elements that serve as the backbone of a modern data platform:</p><ul class=\"list\"><li><strong>Docker Image for Flows Execution</strong></li></ul><p>Every Prefect flow needs a controlled and consistent environment to run. Using a Docker container is an ideal solution for this purpose, as it provides isolation and ensures that all dependencies and runtime configurations are reproducible. Containerization is a standard practice in modern data platforms to guarantee reliable flow execution across different environments.</p><ul class=\"list\"><li><strong>Prefect Worker</strong></li></ul><p>Prefect Cloud handles all deployment schedules, but a process within our infrastructure is required to pull and execute these scheduled tasks. This is the responsibility of the Prefect worker. Before diving into more advanced topics, it‚Äôs important to understand how to configure and manage the Prefect worker. Later, I will explain further how to set up a base job template that will be used to execute all deployments, ensuring consistent and scalable workflow orchestration.</p><ul class=\"list\"><li><strong>Prefect Configuration Files</strong></li></ul><p>A well-structured repository is essential for managing deployments and flows efficiently. Ideally, adding a new deployment should require only a few lines of code, making it easy for any team member to contribute. The <code>prefect.yaml</code> file plays a key role in this process by organizing and codifying deployment configurations, clearly connecting flows, deployments, and infrastructure in a maintainable way.</p><p>By focusing on containerized execution, robust workflow orchestration, and clear configuration management, we lay the groundwork for a scalable and maintainable data platform that ensures reliable data ingestion and processing.</p><h2 id=\"docker-image-for-flows-execution\"><a href=\"#docker-image-for-flows-execution\" class=\"heading-anchor\">Docker Image for Flows Execution</a></h2><p>Having each Prefect flow executed in its own isolated environment is essential. Without isolation, two flows running side-by-side could conflict‚Äîthink mismatched library versions, breaking changes, or dependency clashes. Suddenly, what worked yesterday doesn‚Äôt work today. Containerization solves this problem elegantly.</p><p>Building a dedicated Docker image ensures that flow runs are reproducible and decoupled from the host system. You can test new dependencies in dev, tag the image, and confidently promote it to prod. No more ‚Äúit worked on my machine‚Äù surprises.</p><p>For most data teams, starting with the official Prefect image is the way to go. It includes all the Prefect orchestration tools out of the box, so you don‚Äôt have to reinvent the wheel. Manage dependencies via <code>uv</code> and a <code>pyproject.toml</code>:</p><pre class=\"language-toml\"><code class=\"language-toml\"><span class=\"token key property\">dependencies</span> <span class=\"token punctuation\">=</span> <span class=\"token punctuation\">[</span>\n    <span class=\"token string\">\"prefect[docker,github,gcp]&gt;=3.3.7\"</span><span class=\"token punctuation\">,</span>\n    <span class=\"token string\">\"dlt[mssql,parquet]&gt;=1.8.1\"</span><span class=\"token punctuation\">,</span>\n    <span class=\"token string\">\"pymssql&gt;=2.3.2\"</span><span class=\"token punctuation\">,</span>\n<span class=\"token punctuation\">]</span></code></pre><p>Once dependencies are set, run <code>uv sync</code>. This generates a <code>uv.lock</code> file, locking all versions for reproducibility. This file, along with your <code>pyproject.toml</code>, is all you need for the build process.</p><p>The Dockerfile itself is rather straightforward, assuming we need to prepare the <code>uv</code> to install system dependencies to be used inside the container without an additional virtual environment:</p><pre class=\"language-docker\"><code class=\"language-docker\"><span class=\"token instruction\"><span class=\"token keyword\">FROM</span> prefecthq/prefect:3.3.7-python3.12</span>\n\n<span class=\"token instruction\"><span class=\"token keyword\">RUN</span> pip install uv --no-cache</span>\n\n<span class=\"token instruction\"><span class=\"token keyword\">COPY</span> pyproject.toml uv.lock README.md ./</span>\n\n<span class=\"token instruction\"><span class=\"token keyword\">RUN</span> uv export --frozen --no-dev --no-editable &gt; requirements.txt</span>\n\n<span class=\"token instruction\"><span class=\"token keyword\">RUN</span> uv pip install --no-cache --system --pre -r requirements.txt</span></code></pre><p>Notice we don‚Äôt copy the entire repo, only dependency files. The flow code lives outside the image, so we only rebuild the image when dependencies change. To push the image to the GitHub Container Registry, you can use:</p><pre class=\"language-bash\"><code class=\"language-bash\"><span class=\"token function\">docker</span> build <span class=\"token parameter variable\">-t</span> ghcr.io/<span class=\"token operator\">&lt;</span>your_github_organisation<span class=\"token operator\">&gt;</span>/edp-flows:<span class=\"token operator\">&lt;</span>tag<span class=\"token operator\">&gt;</span> <span class=\"token builtin class-name\">.</span>\n\n<span class=\"token function\">docker</span> push ghcr.io/<span class=\"token operator\">&lt;</span>your_github_organisation<span class=\"token operator\">&gt;</span>/edp-flows:<span class=\"token operator\">&lt;</span>tag<span class=\"token operator\">&gt;</span></code></pre><p>This is a manual step for now, but don‚Äôt worry‚Äîwe‚Äôll automate it with GitHub Actions soon. For now, you‚Äôve got a solid, reliable foundation for running flows in a clean, isolated environment every single time.</p><h2 id=\"prefect-worker\"><a href=\"#prefect-worker\" class=\"heading-anchor\">Prefect Worker</a></h2><p>Once the Docker image is ready, the next step is orchestrating flows execution. That‚Äôs where the Prefect worker comes in. Workers are long-running processes that poll work pools for scheduled flow runs and execute them. If you want to dive deeper into the range of possibilities, <a href=\"https://docs.prefect.io/v3/deploy/infrastructure-concepts/workers\" rel=\"noopener\">Prefect‚Äôs documentation</a> is a great resource.</p><p>For our setup, we‚Äôll use the Kubernetes worker type and deploy it with the official <a href=\"https://github.com/PrefectHQ/prefect-helm/tree/main/charts/prefect-worker\" rel=\"noopener\">Prefect Helm Chart</a>.</p><h3 id=\"how-prefect-executes-flow-runs\"><a href=\"#how-prefect-executes-flow-runs\" class=\"heading-anchor\">How Prefect Executes Flow Runs</a></h3><p>Prefect workers are responsible for:</p><ul class=\"list\"><li><strong>Polling work pools</strong> for scheduled flow runs</li><li><strong>Spinning up job-specific infrastructure</strong> using base templates</li><li><strong>Enabling environment-specific configurations</strong> via Helm charts</li><li><strong>Supporting zero-downtime updates</strong> when modifying worker settings</li></ul><p>This dynamic approach means you can flexibly scale, update, and manage your workflow execution environments.</p><h3 id=\"prefect-worker-configuration\"><a href=\"#prefect-worker-configuration\" class=\"heading-anchor\">Prefect Worker Configuration</a></h3><p>Here‚Äôs a minimal <code>values.yaml</code> example, sufficient to get a Prefect worker running via Helm:</p><pre class=\"language-yaml\"><code class=\"language-yaml\"><span class=\"token key atrule\">worker</span><span class=\"token punctuation\">:</span>\n  <span class=\"token key atrule\">image</span><span class=\"token punctuation\">:</span>\n    <span class=\"token key atrule\">repository</span><span class=\"token punctuation\">:</span> prefecthq/prefect\n    <span class=\"token key atrule\">prefectTag</span><span class=\"token punctuation\">:</span> 3.3.7<span class=\"token punctuation\">-</span>python3.12<span class=\"token punctuation\">-</span>kubernetes\n    <span class=\"token key atrule\">pullPolicy</span><span class=\"token punctuation\">:</span> IfNotPresent\n\n  <span class=\"token key atrule\">config</span><span class=\"token punctuation\">:</span>\n    <span class=\"token key atrule\">workPool</span><span class=\"token punctuation\">:</span> <span class=\"token string\">\"edp-work-pool\"</span>\n    <span class=\"token key atrule\">workQueues</span><span class=\"token punctuation\">:</span> <span class=\"token punctuation\">[</span><span class=\"token string\">\"default\"</span><span class=\"token punctuation\">]</span>\n    <span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span> <span class=\"token string\">\"edp-worker\"</span>\n    <span class=\"token key atrule\">baseJobTemplate</span><span class=\"token punctuation\">:</span>\n      <span class=\"token key atrule\">configuration</span><span class=\"token punctuation\">:</span> &lt;&lt; BASE JOB TEMPLATE WILL BE PROVIDED IN THE NEXT SECTION <span class=\"token punctuation\">&gt;</span><span class=\"token punctuation\">&gt;</span>\n\n  <span class=\"token key atrule\">cloudApiConfig</span><span class=\"token punctuation\">:</span>\n    <span class=\"token key atrule\">accountId</span><span class=\"token punctuation\">:</span> <span class=\"token string\">\"7e3c367b-143a-86e2-b92f-6i414816c39b\"</span>\n    <span class=\"token key atrule\">workspaceId</span><span class=\"token punctuation\">:</span> <span class=\"token string\">\"c506815f-qe83-42dc-b905-re6bcfb68c52\"</span>\n    <span class=\"token key atrule\">apiKeySecret</span><span class=\"token punctuation\">:</span>\n      <span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span> prefect<span class=\"token punctuation\">-</span>api<span class=\"token punctuation\">-</span>key<span class=\"token punctuation\">-</span>secret\n      <span class=\"token key atrule\">key</span><span class=\"token punctuation\">:</span> PREFECT_API_KEY</code></pre><p><strong>Each of the three key sections plays a vital role:</strong></p><ul class=\"list\"><li><strong>Image:</strong> Specifies the Prefect image used to run the worker. This is not the flow‚Äôs image that runs your business logic; that will be defined in the <code>baseJobTemplate</code>.</li><li><strong>Config:</strong> Defines work pool, queues, worker name, and most importantly, the <code>baseJobTemplate</code>. An example of a working base job template is shown in the next section.</li><li><strong>cloudApiConfig:</strong> Provides the cloud workspace details, such as account and workspace IDs. You‚Äôll also need to configure a service account in Prefect Cloud and store its API token as a Kubernetes secret (<code>prefect-api-key-secret</code> with the key <code>PREFECT_API_KEY</code>).</li></ul><h3 id=\"example-base-job-template\"><a href=\"#example-base-job-template\" class=\"heading-anchor\">Example Base Job Template</a></h3><p>In a Kubernetes environment, the base job template defines how Prefect spins up infrastructure for each flow run. The minimal job template for a single-pod job below will allow you to set:</p><ul class=\"list\"><li>The job name (helpful for distinguishing jobs and pods in Kubernetes)</li><li>The image tag used in the pod</li><li>The namespace for job creation</li><li>Other Kubernetes settings include image pull secrets, retry limits, and job cleanup.</li></ul><pre class=\"language-json\"><code class=\"language-json\"><span class=\"token punctuation\">{</span>\n  <span class=\"token property\">\"variables\"</span><span class=\"token operator\">:</span> <span class=\"token punctuation\">{</span>\n    <span class=\"token property\">\"type\"</span><span class=\"token operator\">:</span> <span class=\"token string\">\"object\"</span><span class=\"token punctuation\">,</span>\n    <span class=\"token property\">\"properties\"</span><span class=\"token operator\">:</span> <span class=\"token punctuation\">{</span>\n      <span class=\"token property\">\"name\"</span><span class=\"token operator\">:</span> <span class=\"token punctuation\">{</span>\n        <span class=\"token property\">\"type\"</span><span class=\"token operator\">:</span> <span class=\"token string\">\"string\"</span><span class=\"token punctuation\">,</span>\n        <span class=\"token property\">\"title\"</span><span class=\"token operator\">:</span> <span class=\"token string\">\"Name given to job created by a worker (key: name)\"</span><span class=\"token punctuation\">,</span>\n        <span class=\"token property\">\"default\"</span><span class=\"token operator\">:</span> <span class=\"token string\">\"edp-k8s-job\"</span>\n      <span class=\"token punctuation\">}</span><span class=\"token punctuation\">,</span>\n      <span class=\"token property\">\"image\"</span><span class=\"token operator\">:</span> <span class=\"token punctuation\">{</span>\n        <span class=\"token property\">\"type\"</span><span class=\"token operator\">:</span> <span class=\"token string\">\"string\"</span><span class=\"token punctuation\">,</span>\n        <span class=\"token property\">\"title\"</span><span class=\"token operator\">:</span> <span class=\"token string\">\"Image name and tag that will execute flows, to be provided from deployment (key: image)\"</span><span class=\"token punctuation\">,</span>\n        <span class=\"token property\">\"default\"</span><span class=\"token operator\">:</span> <span class=\"token string\">\"ghcr.io/&lt;your_github_organisation&gt;/edp-flows:&lt;tag&gt;\"</span>\n      <span class=\"token punctuation\">}</span><span class=\"token punctuation\">,</span>\n      <span class=\"token property\">\"namespace\"</span><span class=\"token operator\">:</span> <span class=\"token punctuation\">{</span>\n        <span class=\"token property\">\"type\"</span><span class=\"token operator\">:</span> <span class=\"token string\">\"string\"</span><span class=\"token punctuation\">,</span>\n        <span class=\"token property\">\"title\"</span><span class=\"token operator\">:</span> <span class=\"token string\">\"Namespace name where jobs will be scheduled (key: namespace)\"</span><span class=\"token punctuation\">,</span>\n        <span class=\"token property\">\"default\"</span><span class=\"token operator\">:</span> <span class=\"token string\">\"prefect\"</span>\n      <span class=\"token punctuation\">}</span>\n    <span class=\"token punctuation\">}</span>\n  <span class=\"token punctuation\">}</span><span class=\"token punctuation\">,</span>\n  <span class=\"token property\">\"job_configuration\"</span><span class=\"token operator\">:</span> <span class=\"token punctuation\">{</span>\n    <span class=\"token property\">\"env\"</span><span class=\"token operator\">:</span> <span class=\"token punctuation\">{</span><span class=\"token punctuation\">}</span><span class=\"token punctuation\">,</span>\n    <span class=\"token property\">\"name\"</span><span class=\"token operator\">:</span> <span class=\"token string\">\"{{ name }}\"</span><span class=\"token punctuation\">,</span>\n    <span class=\"token property\">\"labels\"</span><span class=\"token operator\">:</span> <span class=\"token punctuation\">{</span><span class=\"token punctuation\">}</span><span class=\"token punctuation\">,</span>\n    <span class=\"token property\">\"command\"</span><span class=\"token operator\">:</span> <span class=\"token string\">\"\"</span><span class=\"token punctuation\">,</span>\n    <span class=\"token property\">\"namespace\"</span><span class=\"token operator\">:</span> <span class=\"token string\">\"{{ namespace }}\"</span><span class=\"token punctuation\">,</span>\n    <span class=\"token property\">\"job_manifest\"</span><span class=\"token operator\">:</span> <span class=\"token punctuation\">{</span>\n      <span class=\"token property\">\"kind\"</span><span class=\"token operator\">:</span> <span class=\"token string\">\"Job\"</span><span class=\"token punctuation\">,</span>\n      <span class=\"token property\">\"spec\"</span><span class=\"token operator\">:</span> <span class=\"token punctuation\">{</span>\n        <span class=\"token property\">\"template\"</span><span class=\"token operator\">:</span> <span class=\"token punctuation\">{</span>\n          <span class=\"token property\">\"spec\"</span><span class=\"token operator\">:</span> <span class=\"token punctuation\">{</span>\n            <span class=\"token property\">\"volumes\"</span><span class=\"token operator\">:</span> <span class=\"token punctuation\">[</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span>\n            <span class=\"token property\">\"containers\"</span><span class=\"token operator\">:</span> <span class=\"token punctuation\">[</span>\n              <span class=\"token punctuation\">{</span>\n                <span class=\"token property\">\"env\"</span><span class=\"token operator\">:</span> <span class=\"token punctuation\">[</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span>\n                <span class=\"token property\">\"name\"</span><span class=\"token operator\">:</span> <span class=\"token string\">\"prefect-job\"</span><span class=\"token punctuation\">,</span>\n                <span class=\"token property\">\"image\"</span><span class=\"token operator\">:</span> <span class=\"token string\">\"{{ image }}\"</span><span class=\"token punctuation\">,</span>\n                <span class=\"token property\">\"imagePullPolicy\"</span><span class=\"token operator\">:</span> <span class=\"token string\">\"Always\"</span><span class=\"token punctuation\">,</span>\n                <span class=\"token property\">\"envFrom\"</span><span class=\"token operator\">:</span> <span class=\"token punctuation\">[</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span>\n                <span class=\"token property\">\"volumeMounts\"</span><span class=\"token operator\">:</span> <span class=\"token punctuation\">[</span><span class=\"token punctuation\">]</span>\n              <span class=\"token punctuation\">}</span>\n            <span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span>\n            <span class=\"token property\">\"completions\"</span><span class=\"token operator\">:</span> <span class=\"token number\">1</span><span class=\"token punctuation\">,</span>\n            <span class=\"token property\">\"parallelism\"</span><span class=\"token operator\">:</span> <span class=\"token number\">1</span><span class=\"token punctuation\">,</span>\n            <span class=\"token property\">\"tolerations\"</span><span class=\"token operator\">:</span> <span class=\"token punctuation\">[</span><span class=\"token punctuation\">]</span><span class=\"token punctuation\">,</span>\n            <span class=\"token property\">\"restartPolicy\"</span><span class=\"token operator\">:</span> <span class=\"token string\">\"Never\"</span><span class=\"token punctuation\">,</span>\n            <span class=\"token property\">\"imagePullSecrets\"</span><span class=\"token operator\">:</span> <span class=\"token punctuation\">[</span>\n              <span class=\"token punctuation\">{</span>\n                <span class=\"token property\">\"name\"</span><span class=\"token operator\">:</span> <span class=\"token string\">\"reg-creds\"</span>\n              <span class=\"token punctuation\">}</span>\n            <span class=\"token punctuation\">]</span>\n          <span class=\"token punctuation\">}</span>\n        <span class=\"token punctuation\">}</span><span class=\"token punctuation\">,</span>\n        <span class=\"token property\">\"backoffLimit\"</span><span class=\"token operator\">:</span> <span class=\"token number\">0</span><span class=\"token punctuation\">,</span>\n        <span class=\"token property\">\"ttlSecondsAfterFinished\"</span><span class=\"token operator\">:</span> <span class=\"token number\">7200</span>\n      <span class=\"token punctuation\">}</span><span class=\"token punctuation\">,</span>\n      <span class=\"token property\">\"metadata\"</span><span class=\"token operator\">:</span> <span class=\"token punctuation\">{</span>\n        <span class=\"token property\">\"namespace\"</span><span class=\"token operator\">:</span> <span class=\"token string\">\"{{ namespace }}\"</span>\n      <span class=\"token punctuation\">}</span><span class=\"token punctuation\">,</span>\n      <span class=\"token property\">\"apiVersion\"</span><span class=\"token operator\">:</span> <span class=\"token string\">\"batch/v1\"</span>\n    <span class=\"token punctuation\">}</span><span class=\"token punctuation\">,</span>\n    <span class=\"token property\">\"stream_output\"</span><span class=\"token operator\">:</span> <span class=\"token boolean\">true</span>\n  <span class=\"token punctuation\">}</span>\n<span class=\"token punctuation\">}</span></code></pre><p>You can customize this further as needed, and include the template in your Prefect worker configuration. To deploy a worker, it‚Äôs enough to run the helm command:</p><pre class=\"language-bash\"><code class=\"language-bash\">helm upgrade prefect-worker <span class=\"token parameter variable\">--install</span> prefect/prefect-worker <span class=\"token punctuation\">\\</span>\n            <span class=\"token parameter variable\">-n</span> prefect <span class=\"token punctuation\">\\</span>\n            <span class=\"token parameter variable\">-f</span> <span class=\"token variable\">${{ helm_values_path }</span><span class=\"token punctuation\">}</span></code></pre><p>Once deployed, your worker should appear in the Work Pool section in Prefect Cloud.</p><p><is-land on:idle></is-land></p><dialog class=\"flow modal17\"><button autofocus class=\"button\">Close</button><picture><source type=\"image/webp\" srcset=\"/blog/getting-to-your-first-flow-run-prefect-worker-and-deployment-setup/BKcAkHKq2_-639.webp 639w\" sizes=\"auto\"><img loading=\"lazy\" decoding=\"async\" src=\"/blog/getting-to-your-first-flow-run-prefect-worker-and-deployment-setup/BKcAkHKq2_-639.jpeg\" alt=\"prefect work pool\" width=\"639\" height=\"294\"></picture></dialog><button data-index=\"17\"><picture><source type=\"image/webp\" srcset=\"/blog/getting-to-your-first-flow-run-prefect-worker-and-deployment-setup/BKcAkHKq2_-639.webp 639w\" sizes=\"auto\"><img loading=\"lazy\" decoding=\"async\" src=\"/blog/getting-to-your-first-flow-run-prefect-worker-and-deployment-setup/BKcAkHKq2_-639.jpeg\" alt=\"prefect work pool\" width=\"639\" height=\"294\"></picture></button><p></p><p>The <code>baseJobTemplate</code> is also exposed as a config map in your Kubernetes cluster. To follow best practices, manage all worker configuration changes as infrastructure-as-code. Use GitHub workflows to apply updates automatically, reducing the risk of human error from manual changes in Prefect Cloud.</p><p>For more security, assign the service account used to register the worker the ‚ÄúDeveloper‚Äù role, and limit regular developers to read-only access within the Work Pool. These permission settings can be configured in your Prefect Cloud account settings:</p><p><is-land on:idle></is-land></p><dialog class=\"flow modal18\"><button autofocus class=\"button\">Close</button><picture><source type=\"image/webp\" srcset=\"/blog/getting-to-your-first-flow-run-prefect-worker-and-deployment-setup/uBynYwIUGs-322.webp 322w\" sizes=\"auto\"><img loading=\"lazy\" decoding=\"async\" src=\"/blog/getting-to-your-first-flow-run-prefect-worker-and-deployment-setup/uBynYwIUGs-322.jpeg\" alt=\"prefect cloud workers permission\" width=\"322\" height=\"123\"></picture></dialog><button data-index=\"18\"><picture><source type=\"image/webp\" srcset=\"/blog/getting-to-your-first-flow-run-prefect-worker-and-deployment-setup/uBynYwIUGs-322.webp 322w\" sizes=\"auto\"><img loading=\"lazy\" decoding=\"async\" src=\"/blog/getting-to-your-first-flow-run-prefect-worker-and-deployment-setup/uBynYwIUGs-322.jpeg\" alt=\"prefect cloud workers permission\" width=\"322\" height=\"123\"></picture></button><p></p><p>Once your Prefect worker is up and running, you‚Äôre ready to register your first deployment.</p><h2 id=\"prefect-configuration-files\"><a href=\"#prefect-configuration-files\" class=\"heading-anchor\">Prefect Configuration Files</a></h2><p>The <code>prefect.yaml</code> file describes base settings for all deployments, with additional instructions for preparing the execution environment for a deployment run. It can be initialized with the <code>prefect init</code> command, and after filling in the data, you might end up with a file like this:</p><pre class=\"language-yaml\"><code class=\"language-yaml\"><span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span> prefect<span class=\"token punctuation\">-</span>deployments\n<span class=\"token key atrule\">prefect-version</span><span class=\"token punctuation\">:</span> 3.3.7\n\n<span class=\"token key atrule\">definitions</span><span class=\"token punctuation\">:</span>\n  schedules<span class=\"token punctuation\">:</span>\n    cron_default<span class=\"token punctuation\">:</span> <span class=\"token important\">&amp;cron_default</span>\n      cron<span class=\"token punctuation\">:</span> <span class=\"token string\">\"0 0 * * *\"</span>\n      timezone<span class=\"token punctuation\">:</span> <span class=\"token string\">\"UTC\"</span>\n      active<span class=\"token punctuation\">:</span> <span class=\"token boolean important\">false</span>\n\n<span class=\"token key atrule\">build</span><span class=\"token punctuation\">:</span> <span class=\"token null important\">null</span>\n<span class=\"token key atrule\">push</span><span class=\"token punctuation\">:</span> <span class=\"token null important\">null</span>\n\n<span class=\"token key atrule\">pull</span><span class=\"token punctuation\">:</span>\n  <span class=\"token punctuation\">-</span> <span class=\"token key atrule\">prefect.deployments.steps.set_working_directory</span><span class=\"token punctuation\">:</span>\n      directory<span class=\"token punctuation\">:</span> /opt/prefect\n  <span class=\"token punctuation\">-</span> <span class=\"token key atrule\">prefect.deployments.steps.git_clone</span><span class=\"token punctuation\">:</span>\n      repository<span class=\"token punctuation\">:</span> https<span class=\"token punctuation\">:</span>//github.com/&lt;your_github_organisation<span class=\"token punctuation\">&gt;</span>/edp<span class=\"token punctuation\">-</span>flows.git\n      access_token<span class=\"token punctuation\">:</span> <span class=\"token string\">\"{{ prefect.blocks.github-credentials.edp-github-credentials.token }}\"</span>\n  <span class=\"token punctuation\">-</span> <span class=\"token key atrule\">prefect.deployments.steps.run_shell_script</span><span class=\"token punctuation\">:</span>\n      directory<span class=\"token punctuation\">:</span> <span class=\"token string\">\"/opt/prefect/edp-flows\"</span>\n      script<span class=\"token punctuation\">:</span> <span class=\"token punctuation\">|</span>\n        uv pip install <span class=\"token punctuation\">-</span><span class=\"token punctuation\">-</span>no<span class=\"token punctuation\">-</span>cache <span class=\"token punctuation\">-</span><span class=\"token punctuation\">-</span>system .\n\n<span class=\"token key atrule\">deployments</span><span class=\"token punctuation\">:</span>\n  <span class=\"token punctuation\">-</span> <span class=\"token key atrule\">name</span><span class=\"token punctuation\">:</span> hello_world\n    description<span class=\"token punctuation\">:</span> <span class=\"token string\">\"Test hello-world deployment.\"</span>\n    schedules<span class=\"token punctuation\">:</span>\n      <span class=\"token punctuation\">-</span> <span class=\"token key atrule\">&lt;&lt;</span><span class=\"token punctuation\">:</span> <span class=\"token important\">*cron_default</span>\n        cron<span class=\"token punctuation\">:</span> <span class=\"token string\">\"5 0 * * *\"</span>\n    entrypoint<span class=\"token punctuation\">:</span> src/edp_flows/flows/hello_world.py<span class=\"token punctuation\">:</span>hello_world_flow\n    parameters<span class=\"token punctuation\">:</span>\n      text<span class=\"token punctuation\">:</span> <span class=\"token string\">\"Hello, world!\"</span></code></pre><p><strong>The most notable sections are:</strong></p><ul class=\"list\"><li><strong>Definitions:</strong> Shared properties such as schedules.</li><li><strong>build/push:</strong> Relevant only if you need to build a fresh image with each deployment; not applicable in our case (Docker images are maintained separately from prefect.yaml.)</li><li><strong>Pull:</strong> Clones the repository to ensure each deployment uses the latest code. During development, it can be configured to target specific branches. Finally, it installs all Python modules so they‚Äôre available during execution.</li><li><strong>Deployments</strong>: A structured list of deployments, each with customizable parameters, allowing the same flow to be reused across multiple scenarios.</li></ul><h3 id=\"example-prefect-flow\"><a href=\"#example-prefect-flow\" class=\"heading-anchor\">Example Prefect Flow</a></h3><p>As Prefect flows aren‚Äôt covered in detail here, we‚Äôll use a simple ‚ÄúHello, World!‚Äù example for illustration. In your actual use case, this is where you would implement the logic for your first ingestion workflow, tailored to your specific data source and target (such as a data warehouse or lake). Here is <code>hello_world.py</code>:</p><pre class=\"language-python\"><code class=\"language-python\"><span class=\"token triple-quoted-string string\">\"\"\"A flow to demonstrate how to log messages in Prefect flows.\"\"\"</span>\n\n<span class=\"token keyword\">from</span> prefect <span class=\"token keyword\">import</span> flow<span class=\"token punctuation\">,</span> get_run_logger<span class=\"token punctuation\">,</span> task\n\n<span class=\"token decorator annotation punctuation\">@task</span><span class=\"token punctuation\">(</span>log_prints<span class=\"token operator\">=</span><span class=\"token boolean\">True</span><span class=\"token punctuation\">)</span>\n<span class=\"token keyword\">def</span> <span class=\"token function\">print_log_prints</span><span class=\"token punctuation\">(</span>text<span class=\"token punctuation\">:</span> <span class=\"token builtin\">str</span><span class=\"token punctuation\">)</span> <span class=\"token operator\">-</span><span class=\"token operator\">&gt;</span> <span class=\"token boolean\">None</span><span class=\"token punctuation\">:</span>\n    <span class=\"token triple-quoted-string string\">\"\"\"Attempt to print text, world using regular Python print() function.\n\n    This time, use the `log_prints` task parameter.\n    \"\"\"</span>\n    <span class=\"token keyword\">print</span><span class=\"token punctuation\">(</span><span class=\"token string-interpolation\"><span class=\"token string\">f\"log_prints=True: </span><span class=\"token interpolation\"><span class=\"token punctuation\">{</span>text<span class=\"token punctuation\">}</span></span><span class=\"token string\">\"</span></span><span class=\"token punctuation\">)</span>  <span class=\"token comment\"># noqa: T201</span>\n\n<span class=\"token decorator annotation punctuation\">@task</span>\n<span class=\"token keyword\">def</span> <span class=\"token function\">log_prefect_run_logger</span><span class=\"token punctuation\">(</span>text<span class=\"token punctuation\">:</span> <span class=\"token builtin\">str</span><span class=\"token punctuation\">)</span> <span class=\"token operator\">-</span><span class=\"token operator\">&gt;</span> <span class=\"token boolean\">None</span><span class=\"token punctuation\">:</span>\n    <span class=\"token triple-quoted-string string\">\"\"\"Attempt to log text, world using Prefect's runtime logger.\"\"\"</span>\n    logger <span class=\"token operator\">=</span> get_run_logger<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n    logger<span class=\"token punctuation\">.</span>info<span class=\"token punctuation\">(</span><span class=\"token string-interpolation\"><span class=\"token string\">f\"Prefect runtime logger: </span><span class=\"token interpolation\"><span class=\"token punctuation\">{</span>text<span class=\"token punctuation\">}</span></span><span class=\"token string\">\"</span></span><span class=\"token punctuation\">)</span>\n\n\n<span class=\"token decorator annotation punctuation\">@flow</span>\n<span class=\"token keyword\">def</span> <span class=\"token function\">hello_world_flow</span><span class=\"token punctuation\">(</span>text<span class=\"token punctuation\">:</span> <span class=\"token builtin\">str</span><span class=\"token punctuation\">)</span> <span class=\"token operator\">-</span><span class=\"token operator\">&gt;</span> <span class=\"token boolean\">None</span><span class=\"token punctuation\">:</span>\n    <span class=\"token triple-quoted-string string\">\"\"\"Demonstrate how to log messages in Prefect flows.\"\"\"</span>\n    print_log_prints<span class=\"token punctuation\">(</span>text<span class=\"token punctuation\">)</span>\n    log_prefect_run_logger<span class=\"token punctuation\">(</span>text<span class=\"token punctuation\">)</span></code></pre><p>Register the flow with:</p><pre class=\"language-bash\"><code class=\"language-bash\">prefect --no-prompt deploy <span class=\"token punctuation\">\\</span>\n        <span class=\"token parameter variable\">--name</span> <span class=\"token string\">\"<span class=\"token variable\">$deployment_name</span>\"</span> <span class=\"token punctuation\">\\</span>\n        <span class=\"token parameter variable\">--tag</span> <span class=\"token variable\">$tag</span> <span class=\"token punctuation\">\\</span>\n        <span class=\"token parameter variable\">--pool</span> <span class=\"token variable\">$work_pool</span> <span class=\"token punctuation\">\\</span>\n        --job-variable <span class=\"token assign-left variable\">name</span><span class=\"token operator\">=</span><span class=\"token variable\">$deployment_name</span></code></pre><p>The registered deployment includes a schedule, but it‚Äôs disabled by default. To enable it, either do so manually in Prefect Cloud or use the following commands:</p><pre class=\"language-bash\"><code class=\"language-bash\">prefect deployment schedule <span class=\"token function\">ls</span> <span class=\"token operator\">&lt;</span>flow_name<span class=\"token operator\">&gt;</span>/<span class=\"token operator\">&lt;</span>deployment_name<span class=\"token operator\">&gt;</span>\nprefect deployment schedule resume <span class=\"token operator\">&lt;</span>flow_name<span class=\"token operator\">&gt;</span>/<span class=\"token operator\">&lt;</span>deployment_name<span class=\"token operator\">&gt;</span></code></pre><p>After running the first command, you should see a view like this:</p><p><is-land on:idle></is-land></p><dialog class=\"flow modal19\"><button autofocus class=\"button\">Close</button><picture><source type=\"image/webp\" srcset=\"/blog/getting-to-your-first-flow-run-prefect-worker-and-deployment-setup/mFAqNsbicn-720.webp 720w\" sizes=\"auto\"><img loading=\"lazy\" decoding=\"async\" src=\"/blog/getting-to-your-first-flow-run-prefect-worker-and-deployment-setup/mFAqNsbicn-720.jpeg\" alt=\"prefect deployment schedule\" width=\"720\" height=\"98\"></picture></dialog><button data-index=\"19\"><picture><source type=\"image/webp\" srcset=\"/blog/getting-to-your-first-flow-run-prefect-worker-and-deployment-setup/mFAqNsbicn-720.webp 720w\" sizes=\"auto\"><img loading=\"lazy\" decoding=\"async\" src=\"/blog/getting-to-your-first-flow-run-prefect-worker-and-deployment-setup/mFAqNsbicn-720.jpeg\" alt=\"prefect deployment schedule\" width=\"720\" height=\"98\"></picture></button><p></p><p>After enabling the schedule, it should appear as Active, and the Prefect worker will trigger it every day at noon. To test a new deployment, you can manually trigger it from Prefect Cloud. Once your deployment runs successfully, you will see logs from it like the following:</p><p><is-land on:idle></is-land></p><dialog class=\"flow modal20\"><button autofocus class=\"button\">Close</button><picture><source type=\"image/webp\" srcset=\"/blog/getting-to-your-first-flow-run-prefect-worker-and-deployment-setup/fI-tbv8ODV-960.webp 960w\" sizes=\"auto\"><img loading=\"lazy\" decoding=\"async\" src=\"/blog/getting-to-your-first-flow-run-prefect-worker-and-deployment-setup/fI-tbv8ODV-960.jpeg\" alt=\"deployment run prefect\" width=\"960\" height=\"411\"></picture></dialog><button data-index=\"20\"><picture><source type=\"image/webp\" srcset=\"/blog/getting-to-your-first-flow-run-prefect-worker-and-deployment-setup/fI-tbv8ODV-960.webp 960w\" sizes=\"auto\"><img loading=\"lazy\" decoding=\"async\" src=\"/blog/getting-to-your-first-flow-run-prefect-worker-and-deployment-setup/fI-tbv8ODV-960.jpeg\" alt=\"deployment run prefect\" width=\"960\" height=\"411\"></picture></button><p></p><h2 id=\"conclusion\"><a href=\"#conclusion\" class=\"heading-anchor\">Conclusion</a></h2><p>With this setup, you now have everything in place to execute your first data ingestion flow. You‚Äôve:</p><ul class=\"list\"><li>Built a containerized flow execution environment</li><li>Deployed a scalable Prefect worker</li><li>Defined clean, reusable deployment configurations.</li></ul><p>This architecture gives you flexibility and control across environments and sets the stage for more advanced workflows.</p><p>While the example used here is a simple ‚ÄúHello World!‚Äù flow, the same deployment structure can be applied to your real data ingestion workflows. To run your first actual ingestion pipeline, all you need to do is replace the flow logic with code that connects to your data source and writes to your destination (like a data warehouse or lake). The orchestration, environment, and deployment pieces remain the same.</p><h4 id=\"whats-next\"><a href=\"#whats-next\" class=\"heading-anchor\">What‚Äôs next?</a></h4><p>In the next article, I‚Äôll show you how to automate this entire process using GitHub Actions, turning this manual setup into a streamlined CI/CD pipeline your whole team can rely on.</p></div>",
      "date_published": "2025-06-10T11:00:00Z"
    }
    ,{
      "id": "http://localhost:8080/blog/roles-in-the-context-of-the-analytics-workflow/",
      "url": "http://localhost:8080/blog/roles-in-the-context-of-the-analytics-workflow/",
      "title": "Roles in the Context of the Analytics Workflow",
      "content_html": "<nav id=\"toc\" class=\"table-of-contents prose\"><ol><li class=\"flow\"><a href=\"#key-roles\">Key Roles</a></li><li class=\"flow\"><a href=\"#measuring-the-impact-of-each-role\">Measuring the Impact of Each Role</a></li><li class=\"flow\"><a href=\"#understanding-the-need-for-roles\">Understanding the Need for Roles</a></li><li class=\"flow\"><a href=\"#working-with-an-external-partner\">Working With an External Partner</a></li><li class=\"flow\"><a href=\"#conclusions\">Conclusions</a><ol><li class=\"flow\"><a href=\"#resources\">Resources</a></li></ol></li></ol></nav><p><span id=\"toc-skipped\" class=\"visually-hidden\"></span></p><div class=\"flow prose\"><p></p><p>An analytics workflow documents the journey from raw data to production-ready data models, encompassing development and testing phases. A critical component is governance, implemented through a Pull Request approval process that facilitates regular code reviews and prevents technical debt accumulation. This structured approach ensures quality and maintainability while supporting collaborative development.</p><p>To manage this governance effectively, several roles are typically involved. It‚Äôs becoming increasingly rare to see a single analyst handle the entire end-to-end process of creating a report or data model. Instead, the trend is moving toward a growing number of specialized roles, each with distinct responsibilities.</p><p><is-land on:idle></is-land></p><dialog class=\"flow modal2\"><button autofocus class=\"button\">Close</button><picture><source type=\"image/webp\" srcset=\"/img/MpiK_Rgewv-960.webp 960w, /img/MpiK_Rgewv-1600.webp 1600w\" sizes=\"auto\"><img loading=\"lazy\" decoding=\"async\" src=\"/img/MpiK_Rgewv-960.jpeg\" alt=\"typical code-based analytics workflow\" width=\"1600\" height=\"520\" srcset=\"/img/MpiK_Rgewv-960.jpeg 960w, /img/MpiK_Rgewv-1600.jpeg 1600w\" sizes=\"auto\"></picture></dialog><button data-index=\"2\"><picture><source type=\"image/webp\" srcset=\"/img/MpiK_Rgewv-960.webp 960w, /img/MpiK_Rgewv-1600.webp 1600w\" sizes=\"auto\"><img loading=\"lazy\" decoding=\"async\" src=\"/img/MpiK_Rgewv-960.jpeg\" alt=\"typical code-based analytics workflow\" width=\"1600\" height=\"520\" srcset=\"/img/MpiK_Rgewv-960.jpeg 960w, /img/MpiK_Rgewv-1600.jpeg 1600w\" sizes=\"auto\"></picture></button><p></p><h2 id=\"key-roles\"><a href=\"#key-roles\" class=\"heading-anchor\">Key Roles</a></h2><p>The analytics workflow involves several key roles, with data analysts playing a particularly key position. Data analysts combine technical skills with deep business domain knowledge, giving them unique insight into business models and challenges.</p><p>In contrast, data engineers and data platform engineers typically focus on technical implementation rather than direct business interaction. While aligning the data platform roadmap and investments with business value remains strategically important for leadership, this alignment happens at a higher level and doesn‚Äôt directly impact the day-to-day analytics workflow operations.</p><table><thead><tr><th>Role</th><th>Tasks</th></tr></thead><tbody><tr><td>Data Analyst</td><td>- Understand business requirements<br>- Analyze data in intermediate and mart layers<br>- Develop SQL queries and transformations<br>- Create and maintain metadata documentation</td></tr><tr><td>Data Platform Engineer</td><td>- Monitor and support infrastructure resources<br>- Maintain CI/CD pipelines<br>- Manage network infrastructure<br>- Implement cybersecurity measures</td></tr><tr><td>Data Engineer</td><td>- Design and develop data pipelines<br>- Maintain and optimize data flows<br>- Schedule and orchestrate data processing<br>- Implement data ingestion processes</td></tr></tbody></table><h2 id=\"measuring-the-impact-of-each-role\"><a href=\"#measuring-the-impact-of-each-role\" class=\"heading-anchor\">Measuring the Impact of Each Role</a></h2><p>Because roles usually tackle different problems, it is a good idea to measure performance and impact differently. Measuring how a role is doing is also important for creating rules such as notification rules, issue and incident prioritization rules, and other operational matters to increase the reliability of production.</p><table><thead><tr><th>Role</th><th>Measure</th></tr></thead><tbody><tr><td>Data Analyst</td><td>- Understanding of business domain<br>- Business Satisfaction<br>- ROI from data initiatives</td></tr><tr><td>Data Platform Engineer</td><td>- Speed of new data platform features<br>- Reliability of the data platform<br>- Data analysts‚Äô support and satisfaction</td></tr><tr><td>Data Engineer</td><td>- Speed of new data ingestions<br>- Reliability of data pipelines<br>- Data analysts‚Äô support and satisfaction</td></tr></tbody></table><h2 id=\"understanding-the-need-for-roles\"><a href=\"#understanding-the-need-for-roles\" class=\"heading-anchor\">Understanding the Need for Roles</a></h2><p>Analytics teams often tend to combine roles or leave them loosely defined. This approach is understandable, and sometimes even beneficial, in the early stages of an analytics initiative. After all, when starting out, the priority is delivering business value quickly, and formal roles and approval processes can slow things down.</p><p>However, this lack of clearly defined roles and boundaries typically creates challenges as the analytics function matures. Common issues include:</p><ul class=\"list\"><li>Blurred lines between exploratory analytics work and production pipeline operations make it difficult to maintain service levels</li><li>Insufficient knowledge transfer mechanisms, including limited documentation, unclear onboarding processes, and a lack of backup coverage for key roles</li><li>Team friction arising from ambiguous responsibilities and overlapping ownership</li><li>An overemphasis on technical tools and implementation details, rather than addressing the more fundamental needs of role clarity and process alignment</li></ul><h2 id=\"working-with-an-external-partner\"><a href=\"#working-with-an-external-partner\" class=\"heading-anchor\">Working With an External Partner</a></h2><p>In this workflow, there are already quite a few roles and tasks, and in reality, even more can be involved. For example, handling data privacy in Europe requires a solid understanding of GDPR. Given the range of responsibilities and the breadth of expertise needed to run a data department effectively, many data and analytics teams choose to rely on external partners. However, this isn‚Äôt always straightforward. External teams often create strong and rigid boundaries between themselves (the extended team) and the customer‚Äôs in-house team (the internal team), which can hinder collaboration.</p><p>One practical way to navigate the tension between collaboration and rigid boundaries is to establish a clear RACI matrix from the very beginning. This matrix serves as a shared reference to define roles and responsibilities, helping both internal and extended teams understand who is Responsible, Accountable, Consulted, and Informed for each task. It provides structure without creating silos, enabling smoother handovers and aligned expectations.</p><p><is-land on:idle></is-land></p><dialog class=\"flow modal3\"><button autofocus class=\"button\">Close</button><figure><picture><source type=\"image/webp\" srcset=\"/img/y2O8aXqAtv-960.webp 960w, /img/y2O8aXqAtv-1498.webp 1498w\" sizes=\"auto\"><img loading=\"lazy\" decoding=\"async\" src=\"/img/y2O8aXqAtv-960.jpeg\" alt=\"data analytics team RACI Matrix\" title=\"Example of RACI Matrix in the Analytics Workflow\" width=\"1498\" height=\"678\" srcset=\"/img/y2O8aXqAtv-960.jpeg 960w, /img/y2O8aXqAtv-1498.jpeg 1498w\" sizes=\"auto\"></picture><figcaption>Example of RACI Matrix in the Analytics Workflow</figcaption></figure></dialog><button data-index=\"3\"><figure><picture><source type=\"image/webp\" srcset=\"/img/y2O8aXqAtv-960.webp 960w, /img/y2O8aXqAtv-1498.webp 1498w\" sizes=\"auto\"><img loading=\"lazy\" decoding=\"async\" src=\"/img/y2O8aXqAtv-960.jpeg\" alt=\"data analytics team RACI Matrix\" title=\"Example of RACI Matrix in the Analytics Workflow\" width=\"1498\" height=\"678\" srcset=\"/img/y2O8aXqAtv-960.jpeg 960w, /img/y2O8aXqAtv-1498.jpeg 1498w\" sizes=\"auto\"></picture><figcaption>Example of RACI Matrix in the Analytics Workflow</figcaption></figure></button><p></p><h2 id=\"conclusions\"><a href=\"#conclusions\" class=\"heading-anchor\">Conclusions</a></h2><p>A well-structured analytics workflow is essential for turning raw data into reliable insights. As analytics initiatives mature, the need for governance, role clarity, and collaboration becomes increasingly important. Specialized roles such as data analysts, data engineers, and platform engineers each contribute unique expertise, and their impact should be measured differently to reflect their responsibilities.</p><p>Clearly defined roles and processes‚Äîsuch as code reviews, CI/CD practices, and RACI matrices‚Äînot only support governance and maintainability but also foster collaboration across internal and external teams. While early-stage flexibility is useful, long-term success in data and analytics depends on thoughtful structure, cross-functional alignment, and a shared understanding of who does what, and why.</p><h3 id=\"resources\"><a href=\"#resources\" class=\"heading-anchor\">Resources</a></h3><ul class=\"list\"><li><a href=\"https://docs.google.com/spreadsheets/d/1UFddBx-2mKSE8h-TypmYH4sdM7HN2B6TWr0AgWqzLcU/edit?usp=sharing\" rel=\"noopener\">Example of RACI Matrix</a></li></ul></div>",
      "date_published": "2025-03-27T10:47:00Z"
    }
    ,{
      "id": "http://localhost:8080/blog/how-to-setup-data-platform-infrastructure-on-google-cloud-platform-with-terraform/",
      "url": "http://localhost:8080/blog/how-to-setup-data-platform-infrastructure-on-google-cloud-platform-with-terraform/",
      "title": "How to Setup Data Platform Infrastructure on Google Cloud Platform with Terraform",
      "content_html": "<nav id=\"toc\" class=\"table-of-contents prose\"><ol><li class=\"flow\"><a href=\"#why-choose-a-server-based-approach-with-a-single-vm\">Why Choose a Server-Based Approach with a Single VM?</a></li><li class=\"flow\"><a href=\"#infrastructure-overview\">Infrastructure Overview</a><ol><li class=\"flow\"><a href=\"#understanding-google-cloud-identity-aware-proxy-iap\">Understanding Google Cloud Identity-Aware Proxy (IAP)</a></li></ol></li><li class=\"flow\"><a href=\"#phase-1-securing-the-essentials\">Phase 1: Securing the Essentials</a><ol><li class=\"flow\"><a href=\"#step-1-installing-terraform\">Step 1: Installing Terraform</a></li><li class=\"flow\"><a href=\"#step-2-gcloud-cli-installation\">Step 2: gcloud CLI installation</a></li><li class=\"flow\"><a href=\"#step-3-setting-up-gcp-service-account\">Step 3: Setting up GCP Service Account</a></li><li class=\"flow\"><a href=\"#step-4-downloading-the-json-key-for-the-service-account\">Step 4: Downloading the JSON Key for the Service Account</a></li><li class=\"flow\"><a href=\"#step-5-activating-the-service-account\">Step 5: Activating the Service Account</a></li><li class=\"flow\"><a href=\"#step-6-generating-hmac-key-to-buckets\">Step 6: Generating HMAC Key to Buckets</a></li><li class=\"flow\"><a href=\"#step-7-enabling-compute-engine-api-and-cloud-resource-manager-api\">Step 7: Enabling Compute Engine API and Cloud Resource Manager API</a></li><li class=\"flow\"><a href=\"#step-8-setting-up-a-remote-state-for-terraform\">Step 8: Setting Up a Remote State for Terraform</a></li><li class=\"flow\"><a href=\"#step-9-exporting-credentials-and-setting-up-a-new-bucket\">Step 9: Exporting Credentials and Setting up a New Bucket</a></li></ol></li><li class=\"flow\"><a href=\"#phase-2-installing-and-deploying-infrastructure-with-terraform\">Phase 2: Installing &amp; Deploying Infrastructure with Terraform</a><ol><li class=\"flow\"><a href=\"#terraform-files\">Terraform Files</a></li><li class=\"flow\"><a href=\"#step-1-infrastructure-provisioning-with-terraform\">Step 1: Infrastructure Provisioning with Terraform</a></li><li class=\"flow\"><a href=\"#step-2-set-up-verification\">Step 2: Set up Verification</a></li></ol></li><li class=\"flow\"><a href=\"#conclusion\">Conclusion</a></li></ol></nav><p><span id=\"toc-skipped\" class=\"visually-hidden\"></span></p><div class=\"flow prose\"><p></p><p>Setting up a solid, scalable data platform is crucial for organizations looking to get the most out of their data. Building upon our previous discussion on architectural considerations for <a href=\"https://thescalableway.com/insights/deploying-prefect-on-any-cloud-using-a-single-virtual-machine/\" rel=\"noopener\">deploying Prefect on various cloud platforms</a>, this article will walk you through building your data platform infrastructure on Google Cloud Platform (GCP) using Terraform.</p><p>Our focus is on creating a server-based approach utilizing a single Virtual Machine (VM)‚Äîa simple yet powerful starting point for organizations that don‚Äôt need to dive into complex source systems or full-blown data warehouses just yet. This approach offers an easy entry point with plenty of room to grow as your data needs evolve.</p><p>As this article will use a substantial amount of code, you can find all the relevant code samples in our<a href=\"https://github.com/thescalableway/dataplatform-gcp-terraform\" rel=\"noopener\"> dedicated repository.</a></p><h2 id=\"why-choose-a-server-based-approach-with-a-single-vm\"><a href=\"#why-choose-a-server-based-approach-with-a-single-vm\" class=\"heading-anchor\">Why Choose a Server-Based Approach with a Single VM?</a></h2><p>Choosing a server-based approach with a single VM comes with several advantages:</p><ol class=\"list\"><li><strong>Cost-effectiveness:</strong> A single VM setup is often a more budget-friendly option for initial deployments or smaller-scale projects.</li><li><strong>Simplified management:</strong> Fewer components mean easier maintenance and troubleshooting.</li><li><strong>Flexibility:</strong> This approach offers the ability to easily expand or modify your infrastructure as requirements change.</li><li><strong>Learning curve:</strong> For teams new to cloud infrastructure, starting with a single VM can be less overwhelming and serve as a stepping stone to more complex architectures.</li></ol><p>This guide will walk you through the process of setting up key components of our data platform infrastructure on GCP. You‚Äôll learn how to configure the VPC and subnets, set up Compute Engine instances, configure firewall rules, secure SSH access with Identity-Aware Proxy (IAP), establish internet connectivity with Cloud Router and NAT, and store the state files in Cloud Storage. We‚Äôll also dive into the specifics of GCP‚Äôs Identity-Aware Proxy, exploring its crucial role in enhancing the security of our data platform.</p><p>By using Terraform to manage infrastructure as code, we ensure that our setup is reproducible, version-controlled, and easy to manage. This not only streamlines the initial deployment but also makes scaling and future updates much more efficient.</p><p>Let‚Äôs get started on building a solid, scalable data platform infrastructure on GCP‚Äîone that will grow with your organization‚Äôs data needs.</p><h2 id=\"infrastructure-overview\"><a href=\"#infrastructure-overview\" class=\"heading-anchor\">Infrastructure Overview</a></h2><p>Before we dive into the step-by-step process of setting up your data platform on GCP, let‚Äôs take a first look at the key components that make up the environment we‚Äôll be building:</p><ul class=\"list\"><li><strong>Virtual Private Cloud (VPC)</strong>: A private network that will serve as the foundation of your environment, providing isolation and security.</li><li><strong>Subnet</strong>: A private subnet where the virtual machine will reside.</li><li><strong>Compute Engine Virtual Machine</strong>: The instance where both the GitHub Runner and Prefect Worker will be set up.</li><li><strong>Firewall</strong>: Configured with rules to allow inbound access exclusively through Google Cloud Identity-Aware Proxy (IAP), blocking all other traffic.</li><li><strong>IAP SSH Permissions</strong>: Enables secure access to the virtual machine.</li><li><strong>Cloud Router</strong>: Provides internet connectivity for the virtual machine.</li><li><strong>Cloud NAT</strong>: Configures a NAT gateway that directs the virtual machine to the Cloud Router for outbound internet access. It also ensures that the public IP is fixed as long as the Cloud NAT object is not destroyed and configured for the same zone.</li><li><strong>Cloud Storage</strong>: Sets up a Google Cloud Storage bucket to store ingested data as Parquet files before transforming and loading it into the database as tables.</li></ul><p><is-land on:idle></is-land></p><dialog class=\"flow modal21\"><button autofocus class=\"button\">Close</button><picture><source type=\"image/webp\" srcset=\"/img/ITUd3RhFds-960.webp 960w, /img/ITUd3RhFds-1600.webp 1600w\" sizes=\"auto\"><img loading=\"lazy\" decoding=\"async\" src=\"/img/ITUd3RhFds-960.jpeg\" alt=\"google cloud platform infrastructure\" width=\"1600\" height=\"943\" srcset=\"/img/ITUd3RhFds-960.jpeg 960w, /img/ITUd3RhFds-1600.jpeg 1600w\" sizes=\"auto\"></picture></dialog><button data-index=\"21\"><picture><source type=\"image/webp\" srcset=\"/img/ITUd3RhFds-960.webp 960w, /img/ITUd3RhFds-1600.webp 1600w\" sizes=\"auto\"><img loading=\"lazy\" decoding=\"async\" src=\"/img/ITUd3RhFds-960.jpeg\" alt=\"google cloud platform infrastructure\" width=\"1600\" height=\"943\" srcset=\"/img/ITUd3RhFds-960.jpeg 960w, /img/ITUd3RhFds-1600.jpeg 1600w\" sizes=\"auto\"></picture></button><p></p><h3 id=\"understanding-google-cloud-identity-aware-proxy-iap\"><a href=\"#understanding-google-cloud-identity-aware-proxy-iap\" class=\"heading-anchor\">Understanding Google Cloud Identity-Aware Proxy (IAP)</a></h3><p>To ensure a secure environment, all public access should be completely blocked. With this configuration, resources within the environment can be accessed using two main options:</p><ul class=\"list\"><li><strong>VPN Connection</strong>: In this setup, at least one resource within the VPC must be exposed to the internet to host a VPN endpoint. Alternatively, a separate VPC can be configured solely for VPN purposes, with VPC Network Peering into the main environment. This way, only the VPN-hosting VPC is exposed to the internet, while the main environment remains accessible only internally. Although effective, this configuration is more complex and falls outside the scope of this documentation.</li><li><strong>Google Cloud Identity-Aware Proxy (IAP)</strong>: This option offers a similar secure access model to a VPN but with simplified management through Google Cloud. As outlined in the <a href=\"https://cloud.google.com/iap/docs/concepts-overview#how_iap_works\" rel=\"noopener\">official documentation</a>:</li></ul><blockquote><p><em>‚ÄúWhen an application or resource is protected by IAP, it can only be accessed through the proxy by principals, also known as users, who have the correct Identity and Access Management (IAM) role. When you grant a user access to an application or resource by IAP, they‚Äôre subject to the fine-grained access controls implemented by the product in use without requiring a VPN. When a user tries to access an IAP-secured resource, IAP performs authentication and authorization checks.‚Äù</em></p></blockquote><p>This <a href=\"https://cloud.google.com/iap/images/iap-load-balancer.png\" rel=\"noopener\">diagram from Google</a> further illustrates the components required to implement this configuration:</p><p><is-land on:idle></is-land></p><dialog class=\"flow modal22\"><button autofocus class=\"button\">Close</button><picture><source type=\"image/webp\" srcset=\"/img/2CFZG6gV0g-960.webp 960w, /img/2CFZG6gV0g-1600.webp 1600w\" sizes=\"auto\"><img loading=\"lazy\" decoding=\"async\" src=\"/img/2CFZG6gV0g-960.jpeg\" alt=\"google configuration diagram\" width=\"1600\" height=\"1257\" srcset=\"/img/2CFZG6gV0g-960.jpeg 960w, /img/2CFZG6gV0g-1600.jpeg 1600w\" sizes=\"auto\"></picture></dialog><button data-index=\"22\"><picture><source type=\"image/webp\" srcset=\"/img/2CFZG6gV0g-960.webp 960w, /img/2CFZG6gV0g-1600.webp 1600w\" sizes=\"auto\"><img loading=\"lazy\" decoding=\"async\" src=\"/img/2CFZG6gV0g-960.jpeg\" alt=\"google configuration diagram\" width=\"1600\" height=\"1257\" srcset=\"/img/2CFZG6gV0g-960.jpeg 960w, /img/2CFZG6gV0g-1600.jpeg 1600w\" sizes=\"auto\"></picture></button><p></p><p>With a solid understanding of Google Cloud Identity and its role in managing users and access, let‚Äôs now dive into the practical steps for setting it up and implementing it effectively.</p><h2 id=\"phase-1-securing-the-essentials\"><a href=\"#phase-1-securing-the-essentials\" class=\"heading-anchor\">Phase 1: Securing the Essentials</a></h2><p>Before starting the Terraform configuration, make sure you have the following tools and setups in place:</p><ul class=\"list\"><li><strong>Terraform</strong>: You‚Äôll need Terraform installed on your local machine. This will be your main tool for provisioning infrastructure.</li><li><strong>gcloud CLI</strong>: The gcloud CLI tool should be installed and configured to interact with your Google Cloud account.</li><li><strong>GCP Service Account</strong>: A Google Cloud Platform service account needs to be created.</li><li><strong>Enabled APIs</strong>: Make sure the Compute Engine API and Cloud Resource Manager API are enabled on your GCP account.</li></ul><p>Let‚Äôs take a detailed look at these prerequisites:</p><h3 id=\"step-1-installing-terraform\"><a href=\"#step-1-installing-terraform\" class=\"heading-anchor\">Step 1: Installing Terraform</a></h3><p>Terraform can be installed in various ways, which are outlined <a href=\"https://developer.hashicorp.com/terraform/install\" rel=\"noopener\">by Hashicorp here</a>. For Ubuntu, installation can be done with the following commands:</p><pre class=\"language-bash\"><code class=\"language-bash\"><span class=\"token function\">wget</span> <span class=\"token parameter variable\">-O</span> - https://apt.releases.hashicorp.com/gpg <span class=\"token operator\">|</span> <span class=\"token function\">sudo</span> gpg <span class=\"token parameter variable\">--dearmor</span> <span class=\"token parameter variable\">-o</span>\n/usr/share/keyrings/hashicorp-archive-keyring.gpg\n<span class=\"token builtin class-name\">echo</span> <span class=\"token string\">\"deb [signed-by=/usr/share/keyrings/hashicorp-archive-keyring.gpg]\nhttps://apt.releases.hashicorp.com <span class=\"token variable\"><span class=\"token variable\">$(</span>lsb_release <span class=\"token parameter variable\">-cs</span><span class=\"token variable\">)</span></span> main\"</span> <span class=\"token operator\">|</span> <span class=\"token function\">sudo</span> <span class=\"token function\">tee</span>\n/etc/apt/sources.list.d/hashicorp.list\n<span class=\"token function\">sudo</span> <span class=\"token function\">apt</span> update <span class=\"token operator\">&amp;&amp;</span> <span class=\"token function\">sudo</span> <span class=\"token function\">apt</span> <span class=\"token function\">install</span> terraform</code></pre><h3 id=\"step-2-gcloud-cli-installation\"><a href=\"#step-2-gcloud-cli-installation\" class=\"heading-anchor\">Step 2: gcloud CLI installation</a></h3><p>Similarly to Terraform, the gcloud CLI can be installed as per the <a href=\"https://cloud.google.com/sdk/docs/install\" rel=\"noopener\">official instructions</a>. For Ubuntu, run:</p><pre class=\"language-bash\"><code class=\"language-bash\"> \n<span class=\"token function\">sudo</span> <span class=\"token function\">apt-get</span> update\n<span class=\"token function\">sudo</span> <span class=\"token function\">apt-get</span> <span class=\"token function\">install</span> apt-transport-https ca-certificates gnupg <span class=\"token function\">curl</span>\n<span class=\"token function\">curl</span> https://packages.cloud.google.com/apt/doc/apt-key.gpg <span class=\"token operator\">|</span> <span class=\"token function\">sudo</span> gpg <span class=\"token parameter variable\">--dearmor</span>\n<span class=\"token parameter variable\">-o</span> /usr/share/keyrings/cloud.google.gpg\n<span class=\"token builtin class-name\">echo</span> <span class=\"token string\">\"deb [signed-by=/usr/share/keyrings/cloud.google.gpg]\nhttps://packages.cloud.google.com/apt cloud-sdk main\"</span> <span class=\"token operator\">|</span> <span class=\"token function\">sudo</span> <span class=\"token function\">tee</span> <span class=\"token parameter variable\">-a</span>\n/etc/apt/sources.list.d/google-cloud-sdk.list\n<span class=\"token function\">sudo</span> <span class=\"token function\">apt-get</span> update <span class=\"token operator\">&amp;&amp;</span> <span class=\"token function\">sudo</span> <span class=\"token function\">apt-get</span> <span class=\"token function\">install</span> google-cloud-cli</code></pre><p>After installation, initialize the gcloud CLI by providing the ‚Äúgcloud init‚Äù command and setting up a new account by opening the provided URL.</p><pre class=\"language-bash\"><code class=\"language-bash\">gcloud init\n\n\n<span class=\"token comment\"># gcloud init</span>\nWelcome<span class=\"token operator\">!</span> This <span class=\"token builtin class-name\">command</span> will take you through the configuration of gcloud.\n\nYour current configuration has been <span class=\"token builtin class-name\">set</span> to: <span class=\"token punctuation\">[</span>default<span class=\"token punctuation\">]</span>\n\nYou can skip diagnostics next <span class=\"token function\">time</span> by using the following flag:\n  gcloud init --skip-diagnostics\n\nNetwork diagnostic detects and fixes <span class=\"token builtin class-name\">local</span> network connection issues.\nChecking network connection<span class=\"token punctuation\">..</span>.done.\nReachability Check passed.\nNetwork diagnostic passed <span class=\"token punctuation\">(</span><span class=\"token number\">1</span>/1 checks passed<span class=\"token punctuation\">)</span>.\n\nYou must sign <span class=\"token keyword\">in</span> to continue. Would you like to sign <span class=\"token keyword\">in</span> <span class=\"token punctuation\">(</span>Y/n<span class=\"token punctuation\">)</span>?  Y\n\nGo to the following <span class=\"token function\">link</span> <span class=\"token keyword\">in</span> your browser, and complete the sign-in prompts:\n\n    https://accounts.google.com/o/oauth2/auth<span class=\"token operator\">&lt;</span>URL_TO_OPEN_IN_BROWSER<span class=\"token operator\">&gt;</span>\n\nOnce finished, enter the verification code provided <span class=\"token keyword\">in</span> your browser:\n<span class=\"token operator\">&lt;</span>PROVIDE_VERIFICATION_CODE<span class=\"token operator\">&gt;</span></code></pre><p>Subsequently, configure the desired Cloud project, default Compute Region, and Zone for your environment.</p><h3 id=\"step-3-setting-up-gcp-service-account\"><a href=\"#step-3-setting-up-gcp-service-account\" class=\"heading-anchor\">Step 3: Setting up GCP Service Account</a></h3><p>To create a GCP Service Account, navigate to the <a href=\"https://console.cloud.google.com/\" rel=\"noopener\">Google Cloud Console</a>, select the correct project, and go to <code>Navigation Menu (3 lines) &gt; IAM &amp; Admin &gt; Service Accounts.</code></p><p><is-land on:idle></is-land></p><dialog class=\"flow modal23\"><button autofocus class=\"button\">Close</button><picture><source type=\"image/webp\" srcset=\"/blog/how-to-setup-data-platform-infrastructure-on-google-cloud-platform-with-terraform/0OGIJK8OA6-487.webp 487w\" sizes=\"auto\"><img loading=\"lazy\" decoding=\"async\" src=\"/blog/how-to-setup-data-platform-infrastructure-on-google-cloud-platform-with-terraform/0OGIJK8OA6-487.jpeg\" alt=\"how to set up a google cloud platform service account\" width=\"487\" height=\"326\"></picture></dialog><button data-index=\"23\"><picture><source type=\"image/webp\" srcset=\"/blog/how-to-setup-data-platform-infrastructure-on-google-cloud-platform-with-terraform/0OGIJK8OA6-487.webp 487w\" sizes=\"auto\"><img loading=\"lazy\" decoding=\"async\" src=\"/blog/how-to-setup-data-platform-infrastructure-on-google-cloud-platform-with-terraform/0OGIJK8OA6-487.jpeg\" alt=\"how to set up a google cloud platform service account\" width=\"487\" height=\"326\"></picture></button><p></p><p>Click <code>Create Service Account</code> and provide the required information.</p><p><is-land on:idle></is-land></p><dialog class=\"flow modal24\"><button autofocus class=\"button\">Close</button><picture><source type=\"image/webp\" srcset=\"/blog/how-to-setup-data-platform-infrastructure-on-google-cloud-platform-with-terraform/cg275sFJs--552.webp 552w\" sizes=\"auto\"><img loading=\"lazy\" decoding=\"async\" src=\"/blog/how-to-setup-data-platform-infrastructure-on-google-cloud-platform-with-terraform/cg275sFJs--552.jpeg\" alt=\"how to create service account on goocle cloud platform\" width=\"552\" height=\"532\"></picture></dialog><button data-index=\"24\"><picture><source type=\"image/webp\" srcset=\"/blog/how-to-setup-data-platform-infrastructure-on-google-cloud-platform-with-terraform/cg275sFJs--552.webp 552w\" sizes=\"auto\"><img loading=\"lazy\" decoding=\"async\" src=\"/blog/how-to-setup-data-platform-infrastructure-on-google-cloud-platform-with-terraform/cg275sFJs--552.jpeg\" alt=\"how to create service account on goocle cloud platform\" width=\"552\" height=\"532\"></picture></button><p></p><p>When prompted to <strong>Grant this service account access to a project</strong>, select the appropriate role. In this guide, we use the <strong>Owner</strong> role for simplicity, but it‚Äôs advisable to limit permissions to only what‚Äôs necessary.</p><p>Finally, in the <strong>Grant users access to this service account</strong> step, assign access to the users who will need to interact with the Kubernetes cluster (not part of this article) or VM. Once done, verify that the service account is correctly set up. Its email should follow this pattern:</p><p><code>{service_account_name}@{project}.iam.gserviceaccount.com</code></p><h3 id=\"step-4-downloading-the-json-key-for-the-service-account\"><a href=\"#step-4-downloading-the-json-key-for-the-service-account\" class=\"heading-anchor\">Step 4: Downloading the JSON Key for the Service Account</a></h3><p>In the service account interface, go to <code>Actions (3 dots) &gt; Manage keys</code>.</p><p><is-land on:idle></is-land></p><dialog class=\"flow modal25\"><button autofocus class=\"button\">Close</button><picture><source type=\"image/webp\" srcset=\"/blog/how-to-setup-data-platform-infrastructure-on-google-cloud-platform-with-terraform/JG7MD0ilyl-742.webp 742w\" sizes=\"auto\"><img loading=\"lazy\" decoding=\"async\" src=\"/blog/how-to-setup-data-platform-infrastructure-on-google-cloud-platform-with-terraform/JG7MD0ilyl-742.jpeg\" alt=\"how to set up json key in google cloud platform\" width=\"742\" height=\"339\"></picture></dialog><button data-index=\"25\"><picture><source type=\"image/webp\" srcset=\"/blog/how-to-setup-data-platform-infrastructure-on-google-cloud-platform-with-terraform/JG7MD0ilyl-742.webp 742w\" sizes=\"auto\"><img loading=\"lazy\" decoding=\"async\" src=\"/blog/how-to-setup-data-platform-infrastructure-on-google-cloud-platform-with-terraform/JG7MD0ilyl-742.jpeg\" alt=\"how to set up json key in google cloud platform\" width=\"742\" height=\"339\"></picture></button><p></p><p>Select <code>Add Key &gt; Create new key &gt; JSON</code> to download the JSON key file. Keep this file secure, as it will be required for Terraform configuration.</p><h3 id=\"step-5-activating-the-service-account\"><a href=\"#step-5-activating-the-service-account\" class=\"heading-anchor\">Step 5: Activating the Service Account</a></h3><p>After downloading the JSON key, activate the service account locally with the following command:</p><pre class=\"language-bash\"><code class=\"language-bash\">gcloud auth activate-service-account\n<span class=\"token punctuation\">{</span>service_account_name<span class=\"token punctuation\">}</span>@<span class=\"token punctuation\">{</span>project<span class=\"token punctuation\">}</span>.iam.gserviceaccount.com\n--key-file<span class=\"token operator\">=</span><span class=\"token punctuation\">{</span>json_file<span class=\"token punctuation\">}</span>.json</code></pre><p>This step enables the service account for use in the local environment, ensuring access to necessary GCP resources with IAP tunnel functionality.</p><h3 id=\"step-6-generating-hmac-key-to-buckets\"><a href=\"#step-6-generating-hmac-key-to-buckets\" class=\"heading-anchor\">Step 6: Generating HMAC Key to Buckets</a></h3><p>To enable file uploads to Cloud Storage, some libraries require an HMAC token in addition to a JSON key. To generate an HMAC token:</p><ul class=\"list\"><li>Go to <code>Cloud Storage &gt; Settings &gt; Interoperability</code>.</li><li>Under <code>Service account HMAC</code>, click <code>Create a key for service account</code>.</li><li>Once created, the token will be marked as ‚ÄòActive‚Äô.</li></ul><p><is-land on:idle></is-land></p><dialog class=\"flow modal26\"><button autofocus class=\"button\">Close</button><picture><source type=\"image/webp\" srcset=\"/blog/how-to-setup-data-platform-infrastructure-on-google-cloud-platform-with-terraform/eXA4O9qWt9-479.webp 479w\" sizes=\"auto\"><img loading=\"lazy\" decoding=\"async\" src=\"/blog/how-to-setup-data-platform-infrastructure-on-google-cloud-platform-with-terraform/eXA4O9qWt9-479.jpeg\" alt=\"how to generate HMAC key in google cloud platform\" width=\"479\" height=\"392\"></picture></dialog><button data-index=\"26\"><picture><source type=\"image/webp\" srcset=\"/blog/how-to-setup-data-platform-infrastructure-on-google-cloud-platform-with-terraform/eXA4O9qWt9-479.webp 479w\" sizes=\"auto\"><img loading=\"lazy\" decoding=\"async\" src=\"/blog/how-to-setup-data-platform-infrastructure-on-google-cloud-platform-with-terraform/eXA4O9qWt9-479.jpeg\" alt=\"how to generate HMAC key in google cloud platform\" width=\"479\" height=\"392\"></picture></button><p></p><p>For basic configuration, this step is not required. However, for ingestion, the key must be added to Google Secret Manager to ensure it‚Äôs accessible for flow runs.</p><h3 id=\"step-7-enabling-compute-engine-api-and-cloud-resource-manager-api\"><a href=\"#step-7-enabling-compute-engine-api-and-cloud-resource-manager-api\" class=\"heading-anchor\">Step 7: Enabling Compute Engine API and Cloud Resource Manager API</a></h3><p>Before Terraform can interact with GCP, make sure these APIs are enabled for your project:</p><ul class=\"list\"><li><a href=\"https://console.cloud.google.com/marketplace/product/google/compute.googleapis.com\" rel=\"noopener\">Compute Engine API</a></li><li><a href=\"https://console.cloud.google.com/marketplace/product/google/cloudresourcemanager.googleapis.com\" rel=\"noopener\">Cloud Resource Manager API</a></li><li><a href=\"https://console.cloud.google.com/marketplace/product/google-cloud-platform/cloud-storage\" rel=\"noopener\">Cloud Storage</a></li></ul><p>If they‚Äôre not enabled, head to the Google Cloud Console and enable them.</p><h3 id=\"step-8-setting-up-a-remote-state-for-terraform\"><a href=\"#step-8-setting-up-a-remote-state-for-terraform\" class=\"heading-anchor\">Step 8: Setting Up a Remote State for Terraform</a></h3><p>By default, Terraform stores its state locally in .tfstate files. While this works for development, for any persistent environment, even if you‚Äôre the only one working on the project, it‚Äôs crucial to store this state in a centralized and reliable location. A common best practice is to use a Google Cloud Storage (GCS) bucket to keep the state safe and accessible, avoiding potential issues with local file loss or conflicts.</p><p>However, Terraform itself cannot create the bucket required for storing its state, leading to what‚Äôs called a ‚Äúchicken-and-egg‚Äù problem. The bucket must be created manually before running any Terraform code. Tools like Terragrunt can solve this by simplifying environment management and reducing code duplication <a href=\"https://blog.alterway.fr/en/manage-multiple-kubernetes-clusters-on-gke-with-terragrunt.html\" rel=\"noopener\">(example setup)</a>. However, for the sake of simplicity, we are not introducing such tools in this context.</p><p>To create a new bucket using the <code>gcloud CLI</code>, export the necessary credentials and then proceed with the bucket creation process.</p><h3 id=\"step-9-exporting-credentials-and-setting-up-a-new-bucket\"><a href=\"#step-9-exporting-credentials-and-setting-up-a-new-bucket\" class=\"heading-anchor\">Step 9: Exporting Credentials and Setting up a New Bucket</a></h3><p>Once we have our service account JSON prepared, an export of credentials is necessary to provide Application Default Credentials (ADC):</p><p><code>export GOOGLE_APPLICATION_CREDENTIALS=test-project-32206692d146.json</code></p><p>Then, with the usage of gcloud CLI, a new bucket with the applied policy should be created:</p><pre class=\"language-bash\"><code class=\"language-bash\">gcloud storage buckets create gs://test-project-tfstate <span class=\"token parameter variable\">--location</span><span class=\"token operator\">=</span>us-central1 \n--uniform-bucket-level-access\n\ngcloud storage buckets add-iam-policy-binding gs://test-project-tfstate <span class=\"token punctuation\">\\</span>\n<span class=\"token parameter variable\">--member</span><span class=\"token operator\">=</span><span class=\"token string\">\"serviceAccount:test-service-account@test-project.iam.gserviceaccount.com\"</span> <span class=\"token punctuation\">\\</span>\n  <span class=\"token parameter variable\">--role</span><span class=\"token operator\">=</span><span class=\"token string\">\"roles/storage.objectAdmin\"</span></code></pre><p>Once completed, it should be available in the GCP Console. To check it, go to <code>Navigation Menu &gt; Cloud Storage &gt; Buckets</code>:</p><p><is-land on:idle></is-land></p><dialog class=\"flow modal27\"><button autofocus class=\"button\">Close</button><picture><source type=\"image/webp\" srcset=\"/blog/how-to-setup-data-platform-infrastructure-on-google-cloud-platform-with-terraform/DwEN8YF8e3-960.webp 960w\" sizes=\"auto\"><img loading=\"lazy\" decoding=\"async\" src=\"/blog/how-to-setup-data-platform-infrastructure-on-google-cloud-platform-with-terraform/DwEN8YF8e3-960.jpeg\" alt=\"how to set up a new bucket on google cloud platform\" width=\"960\" height=\"133\"></picture></dialog><button data-index=\"27\"><picture><source type=\"image/webp\" srcset=\"/blog/how-to-setup-data-platform-infrastructure-on-google-cloud-platform-with-terraform/DwEN8YF8e3-960.webp 960w\" sizes=\"auto\"><img loading=\"lazy\" decoding=\"async\" src=\"/blog/how-to-setup-data-platform-infrastructure-on-google-cloud-platform-with-terraform/DwEN8YF8e3-960.jpeg\" alt=\"how to set up a new bucket on google cloud platform\" width=\"960\" height=\"133\"></picture></button><p></p><h2 id=\"phase-2-installing-and-deploying-infrastructure-with-terraform\"><a href=\"#phase-2-installing-and-deploying-infrastructure-with-terraform\" class=\"heading-anchor\">Phase 2: Installing &amp; Deploying Infrastructure with Terraform</a></h2><p>To set up the environment, Terraform will handle provisioning all the required GCP resources. By the end of this process, your directory structure will look like this:</p><pre class=\"language-bash\"><code class=\"language-bash\">$ tree\n\n<span class=\"token operator\">|</span>-- backend.tf\n<span class=\"token operator\">|</span>-- main.tf\n<span class=\"token operator\">|</span>-- provider.tf\n<span class=\"token operator\">|</span>-- test-project-32206692d146.json\n<span class=\"token operator\">|</span>-- variable.tf</code></pre><p>For managing both DEV and PROD environments, you can duplicate the files as shown:</p><pre class=\"language-bash\"><code class=\"language-bash\">$ tree\n‚îú‚îÄ‚îÄ dev\n    ‚îú‚îÄ‚îÄ backend.tf\n‚îÇ   ‚îú‚îÄ‚îÄ main.tf\n‚îÇ   ‚îú‚îÄ‚îÄ test-project-32206692d146.json\n‚îÇ   ‚îú‚îÄ‚îÄ provider.tf\n‚îÇ   ‚îî‚îÄ‚îÄ variable.tf\n‚îî‚îÄ‚îÄ prod\n    ‚îú‚îÄ‚îÄ backend.tf\n    ‚îú‚îÄ‚îÄ main.tf\n    ‚îú‚îÄ‚îÄ test-project-32206692d146.json\n    ‚îú‚îÄ‚îÄ provider.tf\n    ‚îî‚îÄ‚îÄ variable.tf</code></pre><h4 id=\"terraform-files\"><a href=\"#terraform-files\" class=\"heading-anchor\">Terraform Files</a></h4><p>The main difference between environments lies in the <code>backend.tf</code> and <code>variables.tf</code> files. In larger projects, using Terraform modules or tools like Terragrunt is recommended for reusable configurations. However, for simplicity, this example uses code duplication, which is also a valid approach.</p><p>The content of <code>provider.tf</code> should look like this:</p><pre class=\"language-hcl\"><code class=\"language-hcl\"><span class=\"token keyword\">provider<span class=\"token type variable\"> \"google\" </span></span><span class=\"token punctuation\">{</span>\n  <span class=\"token property\">region</span>      <span class=\"token punctuation\">=</span> var.region\n  <span class=\"token property\">project</span>     <span class=\"token punctuation\">=</span> var.project_name\n  <span class=\"token property\">credentials</span> <span class=\"token punctuation\">=</span> file(var.credentials_file)\n  <span class=\"token property\">zone</span>        <span class=\"token punctuation\">=</span> var.zone\n<span class=\"token punctuation\">}</span></code></pre><p><code>backend.tf</code> should point to a bucket with a shared tfstate file created in step 9 of the first phase. It needs to be manually configured because it is the first block loaded when running <code>terraform init</code>, and variables from <code>variables.tf</code> cannot be referenced here:</p><pre class=\"language-hcl\"><code class=\"language-hcl\"><span class=\"token keyword\">terraform</span> <span class=\"token punctuation\">{</span>\n  <span class=\"token keyword\">backend<span class=\"token type variable\"> \"gcs\" </span></span><span class=\"token punctuation\">{</span>\n    <span class=\"token property\">bucket</span>  <span class=\"token punctuation\">=</span> <span class=\"token string\">\"test-project-tfstate\"</span>\n    <span class=\"token property\">prefix</span>  <span class=\"token punctuation\">=</span> <span class=\"token string\">\"terraform/state/prod\"</span>\n  <span class=\"token punctuation\">}</span>\n<span class=\"token punctuation\">}</span></code></pre><p>All variables used in <code>provider.tf</code> and <code>main.tf</code> are defined in <code>variable.tf</code>, as shown below:</p><pre class=\"language-hcl\"><code class=\"language-hcl\"><span class=\"token keyword\">variable<span class=\"token type variable\"> \"credentials_file\" </span></span><span class=\"token punctuation\">{</span>\n  <span class=\"token property\">default</span> <span class=\"token punctuation\">=</span> <span class=\"token string\">\"test-project-32206692d146.json\"</span>\n<span class=\"token punctuation\">}</span>\n\n<span class=\"token keyword\">variable<span class=\"token type variable\"> \"environment\" </span></span><span class=\"token punctuation\">{</span>\n  <span class=\"token property\">default</span> <span class=\"token punctuation\">=</span> <span class=\"token string\">\"prod\"</span>\n<span class=\"token punctuation\">}</span>\n\n<span class=\"token keyword\">variable<span class=\"token type variable\"> \"filesystem\" </span></span><span class=\"token punctuation\">{</span>\n  <span class=\"token property\">default</span> <span class=\"token punctuation\">=</span> <span class=\"token string\">\"ext4\"</span>\n<span class=\"token punctuation\">}</span>\n\n<span class=\"token keyword\">variable<span class=\"token type variable\"> \"image\" </span></span><span class=\"token punctuation\">{</span>\n  <span class=\"token property\">default</span> <span class=\"token punctuation\">=</span> \n<span class=\"token string\">\"projects/ubuntu-os-cloud/global/images/ubuntu-2404-noble-amd64-v20241115\"</span>\n<span class=\"token punctuation\">}</span>\n\n<span class=\"token keyword\">variable<span class=\"token type variable\"> \"ip_cidr_range\" </span></span><span class=\"token punctuation\">{</span>\n  <span class=\"token property\">default</span> <span class=\"token punctuation\">=</span> <span class=\"token string\">\"10.202.0.0/24\"</span>\n<span class=\"token punctuation\">}</span>\n\n<span class=\"token keyword\">variable<span class=\"token type variable\"> \"machine_type\" </span></span><span class=\"token punctuation\">{</span>\n  <span class=\"token property\">default</span> <span class=\"token punctuation\">=</span> <span class=\"token string\">\"c3d-standard-8-lssd\"</span>\n<span class=\"token punctuation\">}</span>\n\n<span class=\"token keyword\">variable<span class=\"token type variable\"> \"project_name\" </span></span><span class=\"token punctuation\">{</span>\n  <span class=\"token property\">default</span> <span class=\"token punctuation\">=</span> <span class=\"token string\">\"test-project\"</span>\n<span class=\"token punctuation\">}</span>\n\n<span class=\"token keyword\">variable<span class=\"token type variable\"> \"region\" </span></span><span class=\"token punctuation\">{</span>\n  <span class=\"token property\">default</span> <span class=\"token punctuation\">=</span> <span class=\"token string\">\"us-central1\"</span>\n<span class=\"token punctuation\">}</span>\n\n<span class=\"token keyword\">variable<span class=\"token type variable\"> \"service_account\" </span></span><span class=\"token punctuation\">{</span>\n  <span class=\"token property\">default</span> <span class=\"token punctuation\">=</span> \n<span class=\"token string\">\"serviceAccount:test-service-account@test-project.iam.gserviceaccount.com\"</span>\n<span class=\"token punctuation\">}</span>\n\n<span class=\"token keyword\">variable<span class=\"token type variable\"> \"zone\" </span></span><span class=\"token punctuation\">{</span>\n  <span class=\"token property\">default</span> <span class=\"token punctuation\">=</span> <span class=\"token string\">\"us-central1-c\"</span>\n<span class=\"token punctuation\">}</span></code></pre><p><code>main.tf</code> defines and initializes all infrastructure components outlined in the <a href=\"#infrastructure-overview\">Infrastructure overview section</a>.</p><pre class=\"language-hcl\"><code class=\"language-hcl\"><span class=\"token keyword\">resource <span class=\"token type variable\">\"google_compute_network\"</span></span> <span class=\"token string\">\"vpc_edp\"</span> <span class=\"token punctuation\">{</span>\n <span class=\"token property\">name</span>                    <span class=\"token punctuation\">=</span> <span class=\"token string\">\"vpc-<span class=\"token interpolation\"><span class=\"token punctuation\">$</span><span class=\"token punctuation\">{</span><span class=\"token keyword\">var</span><span class=\"token punctuation\">.</span><span class=\"token type variable\">project_name</span><span class=\"token punctuation\">}</span></span>-<span class=\"token interpolation\"><span class=\"token punctuation\">$</span><span class=\"token punctuation\">{</span><span class=\"token keyword\">var</span><span class=\"token punctuation\">.</span><span class=\"token type variable\">environment</span><span class=\"token punctuation\">}</span></span>\"</span>\n <span class=\"token property\">auto_create_subnetworks</span> <span class=\"token punctuation\">=</span> <span class=\"token string\">\"false\"</span>\n\n<span class=\"token punctuation\">}</span>\n\n<span class=\"token keyword\">resource <span class=\"token type variable\">\"google_compute_subnetwork\"</span></span> <span class=\"token string\">\"subnet_edp\"</span> <span class=\"token punctuation\">{</span>\n <span class=\"token property\">name</span>          <span class=\"token punctuation\">=</span> <span class=\"token string\">\"subnet-<span class=\"token interpolation\"><span class=\"token punctuation\">$</span><span class=\"token punctuation\">{</span><span class=\"token keyword\">var</span><span class=\"token punctuation\">.</span><span class=\"token type variable\">project_name</span><span class=\"token punctuation\">}</span></span>-<span class=\"token interpolation\"><span class=\"token punctuation\">$</span><span class=\"token punctuation\">{</span><span class=\"token keyword\">var</span><span class=\"token punctuation\">.</span><span class=\"token type variable\">environment</span><span class=\"token punctuation\">}</span></span>\"</span>\n <span class=\"token property\">ip_cidr_range</span> <span class=\"token punctuation\">=</span> var.ip_cidr_range\n <span class=\"token property\">network</span>       <span class=\"token punctuation\">=</span> google_compute_network.vpc_edp.name\n <span class=\"token property\">region</span>        <span class=\"token punctuation\">=</span> var.region\n <span class=\"token property\">depends_on</span>    <span class=\"token punctuation\">=</span> <span class=\"token punctuation\">[</span>google_compute_network.vpc_edp<span class=\"token punctuation\">]</span>\n<span class=\"token punctuation\">}</span>\n\n<span class=\"token keyword\">resource <span class=\"token type variable\">\"google_compute_instance\"</span></span> <span class=\"token string\">\"vm_edp\"</span> <span class=\"token punctuation\">{</span>\n <span class=\"token property\">project</span>      <span class=\"token punctuation\">=</span> var.project_name\n <span class=\"token property\">zone</span>         <span class=\"token punctuation\">=</span> var.zone\n <span class=\"token property\">name</span>         <span class=\"token punctuation\">=</span> <span class=\"token string\">\"<span class=\"token interpolation\"><span class=\"token punctuation\">$</span><span class=\"token punctuation\">{</span><span class=\"token keyword\">var</span><span class=\"token punctuation\">.</span><span class=\"token type variable\">project_name</span><span class=\"token punctuation\">}</span></span>-<span class=\"token interpolation\"><span class=\"token punctuation\">$</span><span class=\"token punctuation\">{</span><span class=\"token keyword\">var</span><span class=\"token punctuation\">.</span><span class=\"token type variable\">environment</span><span class=\"token punctuation\">}</span></span>-01\"</span>\n <span class=\"token property\">machine_type</span> <span class=\"token punctuation\">=</span> var.machine_type\n <span class=\"token keyword\">boot_disk</span> <span class=\"token punctuation\">{</span>\n   <span class=\"token property\">auto_delete</span> <span class=\"token punctuation\">=</span> <span class=\"token boolean\">true</span>\n   <span class=\"token keyword\">initialize_params</span> <span class=\"token punctuation\">{</span>\n     <span class=\"token property\">image</span> <span class=\"token punctuation\">=</span> var.image\n     <span class=\"token property\">size</span>  <span class=\"token punctuation\">=</span> <span class=\"token number\">50</span>\n     <span class=\"token property\">type</span>  <span class=\"token punctuation\">=</span> <span class=\"token string\">\"pd-ssd\"</span>\n   <span class=\"token punctuation\">}</span>\n   <span class=\"token property\">mode</span> <span class=\"token punctuation\">=</span> <span class=\"token string\">\"READ_WRITE\"</span>\n <span class=\"token punctuation\">}</span>\n <span class=\"token keyword\">scratch_disk</span> <span class=\"token punctuation\">{</span>\n   <span class=\"token property\">interface</span> <span class=\"token punctuation\">=</span> <span class=\"token string\">\"NVME\"</span>\n <span class=\"token punctuation\">}</span>\n <span class=\"token keyword\">network_interface</span> <span class=\"token punctuation\">{</span>\n   <span class=\"token property\">network</span>    <span class=\"token punctuation\">=</span> <span class=\"token string\">\"vpc-<span class=\"token interpolation\"><span class=\"token punctuation\">$</span><span class=\"token punctuation\">{</span><span class=\"token keyword\">var</span><span class=\"token punctuation\">.</span><span class=\"token type variable\">project_name</span><span class=\"token punctuation\">}</span></span>-<span class=\"token interpolation\"><span class=\"token punctuation\">$</span><span class=\"token punctuation\">{</span><span class=\"token keyword\">var</span><span class=\"token punctuation\">.</span><span class=\"token type variable\">environment</span><span class=\"token punctuation\">}</span></span>\"</span>\n   <span class=\"token property\">subnetwork</span> <span class=\"token punctuation\">=</span> google_compute_subnetwork.subnet_edp.name\n <span class=\"token punctuation\">}</span>\n <span class=\"token property\">metadata_startup_script</span> <span class=\"token punctuation\">=</span> <span class=\"token heredoc string\">&lt;&lt;-EOT\n   #!/bin/bash\n   set -e\n   sudo mkfs.ext4 -F /dev/disk/by-id/google-local-nvme-ssd-0\n   sudo mkdir -p /mnt/disks/local-nvme-ssd\n   sudo mount /dev/disk/by-id/google-local-nvme-ssd-0 /mnt/disks/local-nvme-ssd\n   sudo chmod a+w /mnt/disks/local-nvme-ssd\n\n   echo UUID=`sudo blkid -s UUID -o value /dev/disk/by-id/google-local-nvme-ssd-0` /mnt/disks/local-nvme-ssd ext4 discard,defaults,nofail 0 2 | sudo tee -a /etc/fstab\n EOT</span>\n <span class=\"token property\">depends_on</span>              <span class=\"token punctuation\">=</span> <span class=\"token punctuation\">[</span>google_compute_network.vpc_edp<span class=\"token punctuation\">]</span>\n<span class=\"token punctuation\">}</span>\n\n<span class=\"token keyword\">resource <span class=\"token type variable\">\"google_compute_firewall\"</span></span> <span class=\"token string\">\"rules\"</span> <span class=\"token punctuation\">{</span>\n <span class=\"token property\">project</span> <span class=\"token punctuation\">=</span> var.project_name\n <span class=\"token property\">name</span>    <span class=\"token punctuation\">=</span> <span class=\"token string\">\"allow-ssh-<span class=\"token interpolation\"><span class=\"token punctuation\">$</span><span class=\"token punctuation\">{</span><span class=\"token keyword\">var</span><span class=\"token punctuation\">.</span><span class=\"token type variable\">environment</span><span class=\"token punctuation\">}</span></span>\"</span>\n <span class=\"token property\">network</span> <span class=\"token punctuation\">=</span> <span class=\"token string\">\"vpc-<span class=\"token interpolation\"><span class=\"token punctuation\">$</span><span class=\"token punctuation\">{</span><span class=\"token keyword\">var</span><span class=\"token punctuation\">.</span><span class=\"token type variable\">project_name</span><span class=\"token punctuation\">}</span></span>-<span class=\"token interpolation\"><span class=\"token punctuation\">$</span><span class=\"token punctuation\">{</span><span class=\"token keyword\">var</span><span class=\"token punctuation\">.</span><span class=\"token type variable\">environment</span><span class=\"token punctuation\">}</span></span>\"</span>\n\n <span class=\"token keyword\">allow</span> <span class=\"token punctuation\">{</span>\n   <span class=\"token property\">protocol</span> <span class=\"token punctuation\">=</span> <span class=\"token string\">\"tcp\"</span>\n   <span class=\"token property\">ports</span>    <span class=\"token punctuation\">=</span> <span class=\"token punctuation\">[</span><span class=\"token string\">\"22\"</span>, <span class=\"token string\">\"6443\"</span><span class=\"token punctuation\">]</span>\n <span class=\"token punctuation\">}</span>\n <span class=\"token property\">source_ranges</span> <span class=\"token punctuation\">=</span> <span class=\"token punctuation\">[</span><span class=\"token string\">\"35.235.240.0/20\"</span><span class=\"token punctuation\">]</span>\n <span class=\"token property\">depends_on</span>    <span class=\"token punctuation\">=</span> <span class=\"token punctuation\">[</span>google_compute_network.vpc_edp<span class=\"token punctuation\">]</span>\n<span class=\"token punctuation\">}</span>\n\n<span class=\"token keyword\">resource <span class=\"token type variable\">\"google_project_iam_member\"</span></span> <span class=\"token string\">\"project\"</span> <span class=\"token punctuation\">{</span>\n <span class=\"token property\">project</span> <span class=\"token punctuation\">=</span> var.project_name\n <span class=\"token property\">role</span>    <span class=\"token punctuation\">=</span> <span class=\"token string\">\"roles/iap.tunnelResourceAccessor\"</span>\n <span class=\"token property\">member</span>  <span class=\"token punctuation\">=</span> var.service_account\n<span class=\"token punctuation\">}</span>\n\n<span class=\"token keyword\">resource <span class=\"token type variable\">\"google_compute_router\"</span></span> <span class=\"token string\">\"router\"</span> <span class=\"token punctuation\">{</span>\n <span class=\"token property\">project</span>    <span class=\"token punctuation\">=</span> var.project_name\n <span class=\"token property\">name</span>       <span class=\"token punctuation\">=</span> <span class=\"token string\">\"nat-router-<span class=\"token interpolation\"><span class=\"token punctuation\">$</span><span class=\"token punctuation\">{</span><span class=\"token keyword\">var</span><span class=\"token punctuation\">.</span><span class=\"token type variable\">environment</span><span class=\"token punctuation\">}</span></span>\"</span>\n <span class=\"token property\">network</span>    <span class=\"token punctuation\">=</span> <span class=\"token string\">\"vpc-<span class=\"token interpolation\"><span class=\"token punctuation\">$</span><span class=\"token punctuation\">{</span><span class=\"token keyword\">var</span><span class=\"token punctuation\">.</span><span class=\"token type variable\">project_name</span><span class=\"token punctuation\">}</span></span>-<span class=\"token interpolation\"><span class=\"token punctuation\">$</span><span class=\"token punctuation\">{</span><span class=\"token keyword\">var</span><span class=\"token punctuation\">.</span><span class=\"token type variable\">environment</span><span class=\"token punctuation\">}</span></span>\"</span>\n <span class=\"token property\">region</span>     <span class=\"token punctuation\">=</span> var.region\n <span class=\"token property\">depends_on</span> <span class=\"token punctuation\">=</span> <span class=\"token punctuation\">[</span>google_compute_network.vpc_edp<span class=\"token punctuation\">]</span>\n<span class=\"token punctuation\">}</span>\n\n<span class=\"token keyword\">resource <span class=\"token type variable\">\"google_compute_router_nat\"</span></span> <span class=\"token string\">\"nat\"</span> <span class=\"token punctuation\">{</span>\n <span class=\"token property\">name</span>                               <span class=\"token punctuation\">=</span> <span class=\"token string\">\"router-nat-<span class=\"token interpolation\"><span class=\"token punctuation\">$</span><span class=\"token punctuation\">{</span><span class=\"token keyword\">var</span><span class=\"token punctuation\">.</span><span class=\"token type variable\">project_name</span><span class=\"token punctuation\">}</span></span>-<span class=\"token interpolation\"><span class=\"token punctuation\">$</span><span class=\"token punctuation\">{</span><span class=\"token keyword\">var</span><span class=\"token punctuation\">.</span><span class=\"token type variable\">environment</span><span class=\"token punctuation\">}</span></span>\"</span>\n <span class=\"token property\">router</span>                             <span class=\"token punctuation\">=</span> google_compute_router.router.name\n <span class=\"token property\">region</span>                             <span class=\"token punctuation\">=</span> var.region\n <span class=\"token property\">nat_ip_allocate_option</span>             <span class=\"token punctuation\">=</span> <span class=\"token string\">\"AUTO_ONLY\"</span>\n <span class=\"token property\">source_subnetwork_ip_ranges_to_nat</span> <span class=\"token punctuation\">=</span> <span class=\"token string\">\"ALL_SUBNETWORKS_ALL_IP_RANGES\"</span>\n\n <span class=\"token keyword\">log_config</span> <span class=\"token punctuation\">{</span>\n   <span class=\"token property\">enable</span> <span class=\"token punctuation\">=</span> <span class=\"token boolean\">true</span>\n   <span class=\"token property\">filter</span> <span class=\"token punctuation\">=</span> <span class=\"token string\">\"ERRORS_ONLY\"</span>\n <span class=\"token punctuation\">}</span>\n<span class=\"token punctuation\">}</span>\n\n<span class=\"token keyword\">resource <span class=\"token type variable\">\"google_storage_bucket\"</span></span> <span class=\"token string\">\"private_bucket\"</span> <span class=\"token punctuation\">{</span>\n <span class=\"token property\">name</span>          <span class=\"token punctuation\">=</span> <span class=\"token string\">\"<span class=\"token interpolation\"><span class=\"token punctuation\">$</span><span class=\"token punctuation\">{</span><span class=\"token keyword\">var</span><span class=\"token punctuation\">.</span><span class=\"token type variable\">project_name</span><span class=\"token punctuation\">}</span></span>-<span class=\"token interpolation\"><span class=\"token punctuation\">$</span><span class=\"token punctuation\">{</span><span class=\"token keyword\">var</span><span class=\"token punctuation\">.</span><span class=\"token type variable\">environment</span><span class=\"token punctuation\">}</span></span>\"</span>\n <span class=\"token property\">location</span>      <span class=\"token punctuation\">=</span> var.region\n <span class=\"token property\">storage_class</span> <span class=\"token punctuation\">=</span> <span class=\"token string\">\"STANDARD\"</span>\n\n <span class=\"token property\">uniform_bucket_level_access</span> <span class=\"token punctuation\">=</span> <span class=\"token boolean\">true</span>\n<span class=\"token punctuation\">}</span>\n\n<span class=\"token keyword\">resource <span class=\"token type variable\">\"google_storage_bucket_iam_binding\"</span></span> <span class=\"token string\">\"bucket_writer\"</span> <span class=\"token punctuation\">{</span>\n <span class=\"token property\">bucket</span> <span class=\"token punctuation\">=</span> google_storage_bucket.private_bucket.name\n\n <span class=\"token property\">role</span> <span class=\"token punctuation\">=</span> <span class=\"token string\">\"roles/storage.objectCreator\"</span>\n <span class=\"token property\">members</span> <span class=\"token punctuation\">=</span> <span class=\"token punctuation\">[</span>\n   <span class=\"token string\">\"<span class=\"token interpolation\"><span class=\"token punctuation\">$</span><span class=\"token punctuation\">{</span><span class=\"token keyword\">var</span><span class=\"token punctuation\">.</span><span class=\"token type variable\">service_account</span><span class=\"token punctuation\">}</span></span>\"</span>\n <span class=\"token punctuation\">]</span>\n<span class=\"token punctuation\">}</span>\n\n<span class=\"token keyword\">resource <span class=\"token type variable\">\"google_storage_bucket_iam_binding\"</span></span> <span class=\"token string\">\"bucket_admin\"</span> <span class=\"token punctuation\">{</span>\n <span class=\"token property\">bucket</span> <span class=\"token punctuation\">=</span> google_storage_bucket.private_bucket.name\n\n <span class=\"token property\">role</span> <span class=\"token punctuation\">=</span> <span class=\"token string\">\"roles/storage.admin\"</span>\n <span class=\"token property\">members</span> <span class=\"token punctuation\">=</span> <span class=\"token punctuation\">[</span>\n   <span class=\"token string\">\"<span class=\"token interpolation\"><span class=\"token punctuation\">$</span><span class=\"token punctuation\">{</span><span class=\"token keyword\">var</span><span class=\"token punctuation\">.</span><span class=\"token type variable\">service_account</span><span class=\"token punctuation\">}</span></span>\"</span>\n <span class=\"token punctuation\">]</span>\n<span class=\"token punctuation\">}</span></code></pre><p>With the environment outlined, let‚Äôs provision the infrastructure by validating, formatting, and applying the configuration.</p><h3 id=\"step-1-infrastructure-provisioning-with-terraform\"><a href=\"#step-1-infrastructure-provisioning-with-terraform\" class=\"heading-anchor\">Step 1: Infrastructure Provisioning with Terraform</a></h3><p>Once the necessary files are prepared, validate and format the configuration:</p><pre class=\"language-bash\"><code class=\"language-bash\">$ terraform validate\nSuccess<span class=\"token operator\">!</span> The configuration is valid.\n$ terraform <span class=\"token function\">fmt</span>\nmain.tf\nprovider.tf</code></pre><p>Before applying changes, inspect them with the <code>plan</code> command:</p><pre class=\"language-bash\"><code class=\"language-bash\">terraform plan\n<span class=\"token comment\"># Check if it's all good</span>\nterraform apply\n<span class=\"token comment\"># Enter a value: yes</span></code></pre><p>After a few minutes, the environment will be ready. To list the created resources, run:</p><pre class=\"language-bash\"><code class=\"language-bash\">$ terraform state list\ngoogle_compute_firewall.rules\ngoogle_compute_instance.vm_edp\ngoogle_compute_network.vpc_edp\ngoogle_compute_router.router\ngoogle_compute_router_nat.nat\ngoogle_compute_subnetwork.subnet_edp\ngoogle_project_iam_member.project</code></pre><h3 id=\"step-2-set-up-verification\"><a href=\"#step-2-set-up-verification\" class=\"heading-anchor\">Step 2: Set up Verification</a></h3><p>Verify the resources by logging into the GCP Console. Confirm the creation of <strong>VPC and Subnet, Virtual Machine, Firewall Rule, IAP SSH Permission, Cloud Router, and NAT Gateway</strong>. Navigate to the following sections:</p><ul class=\"list\"><li><strong>VPC and Subnet</strong></li></ul><p>Go to <code>Navigation Menu &gt; VPC Network &gt; VPC Networks</code>:</p><p><is-land on:idle></is-land></p><dialog class=\"flow modal28\"><button autofocus class=\"button\">Close</button><picture><source type=\"image/webp\" srcset=\"/blog/how-to-setup-data-platform-infrastructure-on-google-cloud-platform-with-terraform/fTt7wxmNGG-960.webp 960w, /blog/how-to-setup-data-platform-infrastructure-on-google-cloud-platform-with-terraform/fTt7wxmNGG-1219.webp 1219w\" sizes=\"auto\"><img loading=\"lazy\" decoding=\"async\" src=\"/blog/how-to-setup-data-platform-infrastructure-on-google-cloud-platform-with-terraform/fTt7wxmNGG-960.jpeg\" alt=\"how to verify set up on google cloud platform with VPC\" width=\"1219\" height=\"144\" srcset=\"/blog/how-to-setup-data-platform-infrastructure-on-google-cloud-platform-with-terraform/fTt7wxmNGG-960.jpeg 960w, /blog/how-to-setup-data-platform-infrastructure-on-google-cloud-platform-with-terraform/fTt7wxmNGG-1219.jpeg 1219w\" sizes=\"auto\"></picture></dialog><button data-index=\"28\"><picture><source type=\"image/webp\" srcset=\"/blog/how-to-setup-data-platform-infrastructure-on-google-cloud-platform-with-terraform/fTt7wxmNGG-960.webp 960w, /blog/how-to-setup-data-platform-infrastructure-on-google-cloud-platform-with-terraform/fTt7wxmNGG-1219.webp 1219w\" sizes=\"auto\"><img loading=\"lazy\" decoding=\"async\" src=\"/blog/how-to-setup-data-platform-infrastructure-on-google-cloud-platform-with-terraform/fTt7wxmNGG-960.jpeg\" alt=\"how to verify set up on google cloud platform with VPC\" width=\"1219\" height=\"144\" srcset=\"/blog/how-to-setup-data-platform-infrastructure-on-google-cloud-platform-with-terraform/fTt7wxmNGG-960.jpeg 960w, /blog/how-to-setup-data-platform-infrastructure-on-google-cloud-platform-with-terraform/fTt7wxmNGG-1219.jpeg 1219w\" sizes=\"auto\"></picture></button><p></p><p>Click on the VPC and check the <code>Subnets</code> tab:</p><p><is-land on:idle></is-land></p><dialog class=\"flow modal29\"><button autofocus class=\"button\">Close</button><picture><source type=\"image/webp\" srcset=\"/blog/how-to-setup-data-platform-infrastructure-on-google-cloud-platform-with-terraform/enz5i7h6Ld-960.webp 960w\" sizes=\"auto\"><img loading=\"lazy\" decoding=\"async\" src=\"/blog/how-to-setup-data-platform-infrastructure-on-google-cloud-platform-with-terraform/enz5i7h6Ld-960.jpeg\" alt=\"verifying google cloud platform setupn on VPC\" width=\"960\" height=\"207\"></picture></dialog><button data-index=\"29\"><picture><source type=\"image/webp\" srcset=\"/blog/how-to-setup-data-platform-infrastructure-on-google-cloud-platform-with-terraform/enz5i7h6Ld-960.webp 960w\" sizes=\"auto\"><img loading=\"lazy\" decoding=\"async\" src=\"/blog/how-to-setup-data-platform-infrastructure-on-google-cloud-platform-with-terraform/enz5i7h6Ld-960.jpeg\" alt=\"verifying google cloud platform setupn on VPC\" width=\"960\" height=\"207\"></picture></button><p></p><ul class=\"list\"><li><strong>Virtual Machine</strong></li></ul><p>Navigate to <code>Navigation Menu &gt; Compute Engine &gt; VM instances</code>:</p><p><is-land on:idle></is-land></p><dialog class=\"flow modal30\"><button autofocus class=\"button\">Close</button><picture><source type=\"image/webp\" srcset=\"/blog/how-to-setup-data-platform-infrastructure-on-google-cloud-platform-with-terraform/Rz29rknMsr-960.webp 960w\" sizes=\"auto\"><img loading=\"lazy\" decoding=\"async\" src=\"/blog/how-to-setup-data-platform-infrastructure-on-google-cloud-platform-with-terraform/Rz29rknMsr-960.jpeg\" alt=\"how to verify set up on google cloud platform on VM\" width=\"960\" height=\"126\"></picture></dialog><button data-index=\"30\"><picture><source type=\"image/webp\" srcset=\"/blog/how-to-setup-data-platform-infrastructure-on-google-cloud-platform-with-terraform/Rz29rknMsr-960.webp 960w\" sizes=\"auto\"><img loading=\"lazy\" decoding=\"async\" src=\"/blog/how-to-setup-data-platform-infrastructure-on-google-cloud-platform-with-terraform/Rz29rknMsr-960.jpeg\" alt=\"how to verify set up on google cloud platform on VM\" width=\"960\" height=\"126\"></picture></button><p></p><ul class=\"list\"><li><strong>Firewall Rule</strong></li></ul><p>Go to <code>Navigation Menu &gt; VPC Network &gt; Firewall</code>:</p><p><is-land on:idle></is-land></p><dialog class=\"flow modal31\"><button autofocus class=\"button\">Close</button><picture><source type=\"image/webp\" srcset=\"/blog/how-to-setup-data-platform-infrastructure-on-google-cloud-platform-with-terraform/ptoJB2fUhV-960.webp 960w\" sizes=\"auto\"><img loading=\"lazy\" decoding=\"async\" src=\"/blog/how-to-setup-data-platform-infrastructure-on-google-cloud-platform-with-terraform/ptoJB2fUhV-960.jpeg\" alt=\"how to verify set up on google cloud platform with Firewall rule\" width=\"960\" height=\"343\"></picture></dialog><button data-index=\"31\"><picture><source type=\"image/webp\" srcset=\"/blog/how-to-setup-data-platform-infrastructure-on-google-cloud-platform-with-terraform/ptoJB2fUhV-960.webp 960w\" sizes=\"auto\"><img loading=\"lazy\" decoding=\"async\" src=\"/blog/how-to-setup-data-platform-infrastructure-on-google-cloud-platform-with-terraform/ptoJB2fUhV-960.jpeg\" alt=\"how to verify set up on google cloud platform with Firewall rule\" width=\"960\" height=\"343\"></picture></button><p></p><ul class=\"list\"><li><strong>IAM Role</strong></li></ul><p>Navigate to <code>Navigation Menu &gt; IAM &amp; Admin &gt; IAM</code>, and <code>View by roles</code>:</p><p><is-land on:idle></is-land></p><dialog class=\"flow modal32\"><button autofocus class=\"button\">Close</button><picture><source type=\"image/webp\" srcset=\"/blog/how-to-setup-data-platform-infrastructure-on-google-cloud-platform-with-terraform/UQrnGOluqb-871.webp 871w\" sizes=\"auto\"><img loading=\"lazy\" decoding=\"async\" src=\"/blog/how-to-setup-data-platform-infrastructure-on-google-cloud-platform-with-terraform/UQrnGOluqb-871.jpeg\" alt=\"how to verify set up on google cloud platform with IAM role\" width=\"871\" height=\"272\"></picture></dialog><button data-index=\"32\"><picture><source type=\"image/webp\" srcset=\"/blog/how-to-setup-data-platform-infrastructure-on-google-cloud-platform-with-terraform/UQrnGOluqb-871.webp 871w\" sizes=\"auto\"><img loading=\"lazy\" decoding=\"async\" src=\"/blog/how-to-setup-data-platform-infrastructure-on-google-cloud-platform-with-terraform/UQrnGOluqb-871.jpeg\" alt=\"how to verify set up on google cloud platform with IAM role\" width=\"871\" height=\"272\"></picture></button><p></p><ul class=\"list\"><li><strong>Cloud NAT gateway</strong></li></ul><p>Go to <code>Navigation Menu &gt; Network Connectivity &gt; Cloud Routers &gt; Open Cloud Router</code>:</p><p><is-land on:idle></is-land></p><dialog class=\"flow modal33\"><button autofocus class=\"button\">Close</button><picture><source type=\"image/webp\" srcset=\"/blog/how-to-setup-data-platform-infrastructure-on-google-cloud-platform-with-terraform/akM5MKhm_V-421.webp 421w\" sizes=\"auto\"><img loading=\"lazy\" decoding=\"async\" src=\"/blog/how-to-setup-data-platform-infrastructure-on-google-cloud-platform-with-terraform/akM5MKhm_V-421.jpeg\" alt=\"how to verify set up on google cloud platform with Cloud NAT gateway\" width=\"421\" height=\"105\"></picture></dialog><button data-index=\"33\"><picture><source type=\"image/webp\" srcset=\"/blog/how-to-setup-data-platform-infrastructure-on-google-cloud-platform-with-terraform/akM5MKhm_V-421.webp 421w\" sizes=\"auto\"><img loading=\"lazy\" decoding=\"async\" src=\"/blog/how-to-setup-data-platform-infrastructure-on-google-cloud-platform-with-terraform/akM5MKhm_V-421.jpeg\" alt=\"how to verify set up on google cloud platform with Cloud NAT gateway\" width=\"421\" height=\"105\"></picture></button><p></p><ul class=\"list\"><li><strong>Cloud NAT</strong></li></ul><p>Navigate to <code>Navigation Menu &gt; Network Services &gt; Cloud NAT</code>:</p><p><is-land on:idle></is-land></p><dialog class=\"flow modal34\"><button autofocus class=\"button\">Close</button><picture><source type=\"image/webp\" srcset=\"/blog/how-to-setup-data-platform-infrastructure-on-google-cloud-platform-with-terraform/DPj09xSoyH-863.webp 863w\" sizes=\"auto\"><img loading=\"lazy\" decoding=\"async\" src=\"/blog/how-to-setup-data-platform-infrastructure-on-google-cloud-platform-with-terraform/DPj09xSoyH-863.jpeg\" alt=\"how to verify set up on google cloud platform with Cloud NAT\" width=\"863\" height=\"107\"></picture></dialog><button data-index=\"34\"><picture><source type=\"image/webp\" srcset=\"/blog/how-to-setup-data-platform-infrastructure-on-google-cloud-platform-with-terraform/DPj09xSoyH-863.webp 863w\" sizes=\"auto\"><img loading=\"lazy\" decoding=\"async\" src=\"/blog/how-to-setup-data-platform-infrastructure-on-google-cloud-platform-with-terraform/DPj09xSoyH-863.jpeg\" alt=\"how to verify set up on google cloud platform with Cloud NAT\" width=\"863\" height=\"107\"></picture></button><p></p><ul class=\"list\"><li><strong>SSH Access to Virtual Machine using GCP Console</strong></li></ul><p>To test SSH connectivity to the Virtual Machine, go to <code>Navigation Menu &gt; Compute Engine &gt; VM instances</code> and click on the SSH option for the created VM. Approve the connection when prompted, and you should be logged in.</p><p>This verification ensures all components are correctly configured and accessible. The next steps in setting up the data platform should be setting up a self-hosted GitHub runner and then a Prefect worker.</p><ul class=\"list\"><li><strong>SSH Access to Virtual Machine Using the gcloud CLI</strong></li></ul><p>You can access a Virtual Vachine securely using only a service account token and gcloud CLI. Follow these steps to set up and establish SSH access:</p><ul class=\"list\"><li>Ensure you have the service account JSON key stored locally.</li><li>Log in to your Google Cloud account using the following command:</li></ul><pre class=\"language-bash\"><code class=\"language-bash\">gcloud auth activate-service-account \ntest-service-account@test-project.iam.gserviceaccount.com --key-file\n ~/.config/gcloud.json\ngcloud config <span class=\"token builtin class-name\">set</span> project test-project</code></pre><p>Once authenticated, execute the following command to initiate the SSH connection:</p><pre class=\"language-bash\"><code class=\"language-bash\">gcloud compute <span class=\"token function\">ssh</span> <span class=\"token variable\">${project_name}</span>-<span class=\"token variable\">${environment}</span>-01</code></pre><p>On the first execution, the gcloud CLI will prompt you to generate a private and public SSH key pair. Follow the instructions to create the key pair.</p><p>Once the keys are created, access to the virtual machine will be automatically established. Subsequent logins will reuse the existing key pair, simplifying future access.</p><h2 id=\"conclusion\"><a href=\"#conclusion\" class=\"heading-anchor\">Conclusion</a></h2><p>Setting up a data platform infrastructure on Google Cloud Platform using Terraform provides a solid foundation for organizations looking to leverage the power of their data. This approach offers several key benefits:</p><ul class=\"list\"><li>Scalability and Flexibility: The server-based approach with a single VM provides an excellent starting point that can easily be expanded as your data needs grow.</li><li>Security: By leveraging Google Cloud‚Äôs Identity-Aware Proxy (IAP), we‚Äôve ensured that access to our resources is tightly controlled and secure.</li><li>Infrastructure as Code: Using Terraform allows for version-controlled, reproducible infrastructure deployments, making it easier to manage and update your environment over time.</li><li>Cost-Effectiveness: Starting with a single VM setup is often more budget-friendly for initial deployments or smaller-scale projects.</li><li>Simplified Management: With fewer components to manage initially, maintenance and troubleshooting become more straightforward.</li></ul><p>By following the steps outlined in this guide, you‚Äôve created a robust infrastructure that includes a VPC, subnet, Compute Engine instance, firewall rules, IAP SSH permissions, Cloud Router, Cloud NAT, and Cloud Storage. This setup provides a solid base for running a data platform, including components like a GitHub Runner and Prefect Worker. The process of setting up these additional components will be covered in the next article of this series, building upon the foundation we‚Äôve established here.</p><p>Our <a href=\"https://github.com/thescalableway/dataplatform-gcp-terraform\" rel=\"noopener\">dedicated repository</a> contains all code examples and implementations discussed in this article, which can be accessed for reference and further exploration. We encourage you to review the repository for a comprehensive understanding of the concepts presented.</p><p>Remember, while this guide provides a strong starting point, it‚Äôs crucial to continually assess and adjust your infrastructure to meet your organization‚Äôs changing needs and to stay aligned with best practices in cloud computing and data management.</p></div>",
      "date_published": "2025-03-05T13:31:00Z"
    }
    ,{
      "id": "http://localhost:8080/blog/organizing-networking-for-data-platforms-key-connectivity-options/",
      "url": "http://localhost:8080/blog/organizing-networking-for-data-platforms-key-connectivity-options/",
      "title": "Organizing Networking for Data Platforms: Key Connectivity Options",
      "content_html": "<nav id=\"toc\" class=\"table-of-contents prose\"><ol><li class=\"flow\"><a href=\"#data-platform-architecture-and-networking\">Data Platform Architecture and Networking</a><ol><li class=\"flow\"><a href=\"#extract\">Extract</a></li><li class=\"flow\"><a href=\"#load\">Load</a></li><li class=\"flow\"><a href=\"#transform\">Transform</a></li><li class=\"flow\"><a href=\"#data-consumption\">Data Consumption</a></li></ol></li><li class=\"flow\"><a href=\"#application-layer-security\">Application Layer Security</a></li><li class=\"flow\"><a href=\"#connectivity-options\">Connectivity Options</a><ol><li class=\"flow\"><a href=\"#public-access\">Public Access</a></li><li class=\"flow\"><a href=\"#public-access-with-access-control-list-acl\">Public Access with Access Control List (ACL)</a></li><li class=\"flow\"><a href=\"#vpc-peering\">VPC Peering</a><ol><li class=\"flow\"><a href=\"#vpc-peering-within-a-single-project\">VPC Peering Within a Single Project</a></li><li class=\"flow\"><a href=\"#vpc-peering-across-separate-projects\">VPC Peering Across Separate Projects</a></li></ol></li><li class=\"flow\"><a href=\"#site-to-site-s2s-vpn\">Site-to-site (S2S) VPN</a></li><li class=\"flow\"><a href=\"#other-networking-possibilities\">Other Networking Possibilities</a><ol><li class=\"flow\"><a href=\"#mpls-multiprotocol-label-switching\">MPLS (Multiprotocol Label Switching)</a></li><li class=\"flow\"><a href=\"#dedicated-link\">Dedicated Link</a></li></ol></li></ol></li><li class=\"flow\"><a href=\"#conclusion\">Conclusion</a></li></ol></nav><p><span id=\"toc-skipped\" class=\"visually-hidden\"></span></p><div class=\"flow prose\"><p></p><p>A poorly designed network can cripple even the most advanced data platform. Slow queries, failed data transfers, and security vulnerabilities often stem from overlooked networking decisions. Yet, networking remains one of the least understood aspects of data architecture.</p><p>The Extract, Load, and Transform (ELT) process has become the standard for data integration. It enables organizations to move raw data from source systems to destinations like data warehouses, where it can be analyzed using Business Intelligence (BI) tools. While many aspects of this process deserve attention, networking is a critical yet often underestimated component.</p><p>Building a data platform that supports ELT processes requires a clear understanding of <strong>how all components communicate</strong>. Whether implementing an on-premise solution with open-source tools, leveraging cloud providers, or utilizing SaaS or PaaS solutions, the common thread is the need for seamless connectivity between all elements.</p><p>In this article, we‚Äôll explore the options of organizing networking in data platforms, covering key connectivity options, security considerations, and best practices. To lay the groundwork for our discussion, let‚Äôs first examine the optimal organization of a data platform.</p><h2 id=\"data-platform-architecture-and-networking\"><a href=\"#data-platform-architecture-and-networking\" class=\"heading-anchor\">Data Platform Architecture and Networking</a></h2><p>The diagram below presents the reference architecture for the ELT process as a whole, outlining the key components and workflows involved. Each stage has its own phases, with Ingest being part of data extraction, Land being the process of loading data, and Prepare with Model being the transformation.</p><p><is-land on:idle></is-land></p><dialog class=\"flow modal38\"><button autofocus class=\"button\">Close</button><picture><source type=\"image/webp\" srcset=\"/img/s6LJnDtkZT-960.webp 960w, /img/s6LJnDtkZT-1600.webp 1600w\" sizes=\"auto\"><img loading=\"lazy\" decoding=\"async\" src=\"/img/s6LJnDtkZT-960.jpeg\" alt=\"modular data platform architecture\" width=\"1600\" height=\"824\" srcset=\"/img/s6LJnDtkZT-960.jpeg 960w, /img/s6LJnDtkZT-1600.jpeg 1600w\" sizes=\"auto\"></picture></dialog><button data-index=\"38\"><picture><source type=\"image/webp\" srcset=\"/img/s6LJnDtkZT-960.webp 960w, /img/s6LJnDtkZT-1600.webp 1600w\" sizes=\"auto\"><img loading=\"lazy\" decoding=\"async\" src=\"/img/s6LJnDtkZT-960.jpeg\" alt=\"modular data platform architecture\" width=\"1600\" height=\"824\" srcset=\"/img/s6LJnDtkZT-960.jpeg 960w, /img/s6LJnDtkZT-1600.jpeg 1600w\" sizes=\"auto\"></picture></button><p></p><p>To better understand how networking ties into a data platform, let‚Äôs examine a second diagram, which shifts focus to the networking aspects of the data platform architecture.</p><p><is-land on:idle></is-land></p><dialog class=\"flow modal39\"><button autofocus class=\"button\">Close</button><picture><source type=\"image/webp\" srcset=\"/img/qlCBwG-fKo-960.webp 960w, /img/qlCBwG-fKo-1600.webp 1600w\" sizes=\"auto\"><img loading=\"lazy\" decoding=\"async\" src=\"/img/qlCBwG-fKo-960.jpeg\" alt=\"data platform networking\" width=\"1600\" height=\"870\" srcset=\"/img/qlCBwG-fKo-960.jpeg 960w, /img/qlCBwG-fKo-1600.jpeg 1600w\" sizes=\"auto\"></picture></dialog><button data-index=\"39\"><picture><source type=\"image/webp\" srcset=\"/img/qlCBwG-fKo-960.webp 960w, /img/qlCBwG-fKo-1600.webp 1600w\" sizes=\"auto\"><img loading=\"lazy\" decoding=\"async\" src=\"/img/qlCBwG-fKo-960.jpeg\" alt=\"data platform networking\" width=\"1600\" height=\"870\" srcset=\"/img/qlCBwG-fKo-960.jpeg 960w, /img/qlCBwG-fKo-1600.jpeg 1600w\" sizes=\"auto\"></picture></button><p></p><p>The diagram illustrates various components of a data platform, each requiring network configuration to ensure smooth and secure data movement. At the core of our setup is <strong>workflow orchestration</strong>, which manages the data integration process. Tools like <strong>Prefect</strong>, <strong>Airflow,</strong> or <strong>Azure Data Factory</strong> can handle this, running data flows across various stages.</p><h4 id=\"extract\"><a href=\"#extract\" class=\"heading-anchor\"><strong>Extract</strong></a></h4><p>The initial phase of the <strong>ELT (Extract, Load, Transform)</strong> process is data extraction. Every data platform needs to gather data from external systems, represented as ‚Äúdata sources\" in the diagram. To access resources from a private environment, we need to use a gateway that allows us to reach external resources. This could be:</p><ul class=\"list\"><li><strong>Internet Gateway</strong> - For accessing public resources.</li><li><strong>NAT Gateway</strong> - Allow resources in private subnets to connect to services outside the private network.</li><li><strong>VPN Gateway</strong> - Establishes a secure tunnel with private resources within a different network of our organization or a partner.</li></ul><h4 id=\"load\"><a href=\"#load\" class=\"heading-anchor\"><strong>Load</strong></a></h4><p>Once extracted, data needs to be loaded into a central repository‚Äîtypically a <strong>Data Warehouse</strong>. This can be hosted within the same network as the workflow orchestration tool or exist as an external resource.</p><ul class=\"list\"><li><strong>Same Network:</strong> Configuration is simpler as the same team is likely responsible for setting up both components along with networking.</li><li><strong>External Resource</strong>: Requires additional networking considerations, but the same principles apply‚Äîensuring secure, reliable connectivity.</li></ul><h4 id=\"transform\"><a href=\"#transform\" class=\"heading-anchor\"><strong>Transform</strong></a></h4><p>The <strong>Transform phase</strong> follows a similar working pattern, as the workflow orchestration tool needs access to the Data Warehouse. The same resources need to communicate with each other, regardless of whether it‚Äôs the Load or Transform phase.</p><h4 id=\"data-consumption\"><a href=\"#data-consumption\" class=\"heading-anchor\"><strong>Data Consumption</strong></a></h4><p>The final stage is <strong>data consumption</strong>, where users and tools query the Data Warehouse. Given that sensitive information such as Client Identifying Data (CID) may be stored, secure connections are essential. BI tools, accessible by data platform consumers, need controlled access to the Data Warehouse. Such tools are often managed by a different team from those responsible for data gathering, loading, and transformation.</p><h2 id=\"application-layer-security\"><a href=\"#application-layer-security\" class=\"heading-anchor\">Application Layer Security</a></h2><p>When discussing the networking aspects of data platforms, it‚Äôs essential to understand the context within the <strong>OSI (Open Systems Interconnection)</strong> model. This article primarily focuses on the <strong>Network Layer (Layer 3) and Transport Layer (Layer 4)</strong>‚Äîthe backbone of data connectivity. These layers handle IP addressing, routing, and basic connection establishment, forming the foundation for gateways and other networking components.</p><p>However, security doesn‚Äôt stop at the network level. The <strong>Application Layer (Layer 7)</strong> plays a critical role in securing data and applications. While this article centers on network infrastructure, robust Layer 7 security is just as important. Common security measures include:</p><ul class=\"list\"><li><strong>OAuth</strong> for secure authorization</li><li><strong>Mutual TLS (mTLS)</strong> for encrypted, authenticated communication</li><li><strong>Basic authentication</strong> for simple access control</li><li><strong>API gateways</strong> for managing and securing API access</li><li><strong>Web Application Firewalls (WAF)</strong> for protecting against application-level attacks</li></ul><p>Regardless of network configuration, Application Layer security should always be implemented. A key principle to remember is that the weaker the Layer 7 security measures, the stronger the network-level controls must be to compensate. This inverse relationship between application-level and network-level security is key to maintaining overall system integrity.</p><p>That‚Äôs why, while our focus remains on network infrastructure, a holistic approach to data platform security should consider all relevant OSI layers, especially when dealing with sensitive data and critical business intelligence tools.</p><p>With this foundation in place, let‚Äôs dive into the specific networking options available for securing your data platform.</p><h2 id=\"connectivity-options\"><a href=\"#connectivity-options\" class=\"heading-anchor\">Connectivity Options</a></h2><p>While there are many possible approaches to networking configurations, we‚Äôll focus on the most common scenarios applicable to the majority of data platform use cases:</p><ul class=\"list\"><li><strong>Public access</strong></li><li><strong>Public access with Access Control List (ACL)</strong></li><li><strong>VPC peering</strong> (within a single project and multiple ones)</li><li><strong>Site-to-site VPN</strong></li></ul><p>We‚Äôll explore each in detail, followed by a brief discussion of additional networking possibilities.</p><h3 id=\"public-access\"><a href=\"#public-access\" class=\"heading-anchor\">Public Access</a></h3><p>Public Access is the <strong>least secure</strong> networking option, as it does not restrict access at the network level. Resources residing in a private network are configured to access the internet, while the target resource has no network security applied. This doesn‚Äôt necessarily mean the resource is available to anyone, as Application Layer security may still be in place. However, from a networking perspective, access is unrestricted.</p><p>This configuration exposes resources to potential attacks, as malicious actors can easily reach them and attempt to bypass application security. Whenever possible, such unrestricted access should be avoided.<is-land on:idle></is-land></p><dialog class=\"flow modal40\"><button autofocus class=\"button\">Close</button><picture><source type=\"image/webp\" srcset=\"/img/B4Kflo1I1s-960.webp 960w, /img/B4Kflo1I1s-1600.webp 1600w\" sizes=\"auto\"><img loading=\"lazy\" decoding=\"async\" src=\"/img/B4Kflo1I1s-960.jpeg\" alt=\"public access network\" width=\"1600\" height=\"646\" srcset=\"/img/B4Kflo1I1s-960.jpeg 960w, /img/B4Kflo1I1s-1600.jpeg 1600w\" sizes=\"auto\"></picture></dialog><button data-index=\"40\"><picture><source type=\"image/webp\" srcset=\"/img/B4Kflo1I1s-960.webp 960w, /img/B4Kflo1I1s-1600.webp 1600w\" sizes=\"auto\"><img loading=\"lazy\" decoding=\"async\" src=\"/img/B4Kflo1I1s-960.jpeg\" alt=\"public access network\" width=\"1600\" height=\"646\" srcset=\"/img/B4Kflo1I1s-960.jpeg 960w, /img/B4Kflo1I1s-1600.jpeg 1600w\" sizes=\"auto\"></picture></button><p></p><p>That said, Public Access remains the best option for <strong>specific, low-risk data sources</strong>, such as:</p><ul class=\"list\"><li>Exchange rates for currencies</li><li>Stock market prices</li><li>Other publicly accessible data needed in ELT pipelines</li></ul><p>To mitigate risks associated with Public Access, organizations can implement additional security measures within their private networks. For instance, they can configure a <strong>firewall</strong> to block access to all public resources except those explicitly whitelisted. This approach adheres to the principle of least privilege, ensuring only necessary connections are allowed.</p><p>By implementing such measures, organizations can balance the need for access to public data sources with maintaining a secure network environment.</p><h3 id=\"public-access-with-access-control-list-acl\"><a href=\"#public-access-with-access-control-list-acl\" class=\"heading-anchor\">Public Access with Access Control List (ACL)</a></h3><p>When a system is publicly available, in addition to securing access through Application Layer security controls, we can implement networking mechanisms to expose the system only to a limited group of servers or users. An <strong>Access Control List</strong>, often referred to as a whitelist, is a security mechanism implemented on the publicly available target system. While the simplest scenario involves allowing access for specific IP addresses, ACLs offer more sophisticated options, including:</p><ul class=\"list\"><li>Source and destination IP addresses</li><li>Port numbers</li><li>Network protocols (e.g., TCP, UDP, ICMP)</li><li>Time ranges for when the ACL is active</li></ul><p><is-land on:idle></is-land></p><dialog class=\"flow modal41\"><button autofocus class=\"button\">Close</button><picture><source type=\"image/webp\" srcset=\"/img/rmOW1HKnGZ-960.webp 960w, /img/rmOW1HKnGZ-1600.webp 1600w\" sizes=\"auto\"><img loading=\"lazy\" decoding=\"async\" src=\"/img/rmOW1HKnGZ-960.jpeg\" alt=\"Public Access with Access Control List network\" width=\"1600\" height=\"646\" srcset=\"/img/rmOW1HKnGZ-960.jpeg 960w, /img/rmOW1HKnGZ-1600.jpeg 1600w\" sizes=\"auto\"></picture></dialog><button data-index=\"41\"><picture><source type=\"image/webp\" srcset=\"/img/rmOW1HKnGZ-960.webp 960w, /img/rmOW1HKnGZ-1600.webp 1600w\" sizes=\"auto\"><img loading=\"lazy\" decoding=\"async\" src=\"/img/rmOW1HKnGZ-960.jpeg\" alt=\"Public Access with Access Control List network\" width=\"1600\" height=\"646\" srcset=\"/img/rmOW1HKnGZ-960.jpeg 960w, /img/rmOW1HKnGZ-1600.jpeg 1600w\" sizes=\"auto\"></picture></button><p></p><p>For ACLs to be effective, the system must have <strong>a fixed IP address</strong>. If this cannot be guaranteed, <strong>more secure alternatives like Site-to-Site VPN</strong> should be considered.</p><p>ACLs can be implemented at multiple levels, including routers, firewalls, or other network devices, providing a layered approach to security. Additionally, ACLs can be used for both inbound and outbound traffic, allowing for fine-grained control over data flow in both directions.</p><p>However, while they enhance security, they should not be relied upon as the sole protection mechanism‚Äîthey work best alongside other security measures, such as authentication, encryption, and regular security audits.</p><h3 id=\"vpc-peering\"><a href=\"#vpc-peering\" class=\"heading-anchor\">VPC Peering</a></h3><p>For cloud environments, <strong>VPC (Virtual Private Cloud) Peering</strong> allows direct, private network connections between different cloud resources without exposing traffic to the public internet</p><p>Since cloud providers use different naming conventions (AWS: accounts, Azure: subscriptions, GCP: projects), we‚Äôll use the term ‚Äúproject‚Äù to refer to these organizational units.</p><p><strong>VPC peering</strong> should be considered the default network configuration for resources within the same cloud provider, regardless of the specific implementation option.</p><p>The implementation process varies depending on whether the VPCs are located within the same project or separate ones. Therefore, we will discuss these scenarios separately to highlight their unique characteristics and requirements.</p><h4 id=\"vpc-peering-within-a-single-project\"><a href=\"#vpc-peering-within-a-single-project\" class=\"heading-anchor\">VPC Peering Within a Single Project</a></h4><p>Connecting two networks within the same project is a streamlined process. It requires no additional permissions and can be configured entirely from a single account. This peering effectively extends the network, making all resources in the peered network accessible from the first network.</p><p>As with other networking options, additional firewall rules or ACLs can be implemented to restrict access directionality or limit connectivity to specific services.</p><p><is-land on:idle></is-land></p><dialog class=\"flow modal42\"><button autofocus class=\"button\">Close</button><picture><source type=\"image/webp\" srcset=\"/img/eDMeGFZtME-960.webp 960w, /img/eDMeGFZtME-1600.webp 1600w\" sizes=\"auto\"><img loading=\"lazy\" decoding=\"async\" src=\"/img/eDMeGFZtME-960.jpeg\" alt=\"VPC Peering within a single project\" width=\"1600\" height=\"894\" srcset=\"/img/eDMeGFZtME-960.jpeg 960w, /img/eDMeGFZtME-1600.jpeg 1600w\" sizes=\"auto\"></picture></dialog><button data-index=\"42\"><picture><source type=\"image/webp\" srcset=\"/img/eDMeGFZtME-960.webp 960w, /img/eDMeGFZtME-1600.webp 1600w\" sizes=\"auto\"><img loading=\"lazy\" decoding=\"async\" src=\"/img/eDMeGFZtME-960.jpeg\" alt=\"VPC Peering within a single project\" width=\"1600\" height=\"894\" srcset=\"/img/eDMeGFZtME-960.jpeg 960w, /img/eDMeGFZtME-1600.jpeg 1600w\" sizes=\"auto\"></picture></button><p></p><h4 id=\"vpc-peering-across-separate-projects\"><a href=\"#vpc-peering-across-separate-projects\" class=\"heading-anchor\">VPC Peering Across Separate Projects</a></h4><p>When peering VPCs between different projects, additional security and administrative steps are required:</p><ul class=\"list\"><li>Cross-project peering permissions must be explicitly granted.</li><li>Approval is needed from administrators in both projects.</li><li>Firewall rules must be configured in each project to enable cross-project traffic.</li></ul><p>Despite these additional requirements, VPC Peering remains the most secure and efficient method for connecting resources within the same cloud provider, offering greater control and reduced exposure compared to internet-based connections.</p><p><is-land on:idle></is-land></p><dialog class=\"flow modal43\"><button autofocus class=\"button\">Close</button><picture><source type=\"image/webp\" srcset=\"/img/vQjw6TH34Y-960.webp 960w, /img/vQjw6TH34Y-1600.webp 1600w\" sizes=\"auto\"><img loading=\"lazy\" decoding=\"async\" src=\"/img/vQjw6TH34Y-960.jpeg\" alt=\"VPC Peering across separate projects\" width=\"1600\" height=\"894\" srcset=\"/img/vQjw6TH34Y-960.jpeg 960w, /img/vQjw6TH34Y-1600.jpeg 1600w\" sizes=\"auto\"></picture></dialog><button data-index=\"43\"><picture><source type=\"image/webp\" srcset=\"/img/vQjw6TH34Y-960.webp 960w, /img/vQjw6TH34Y-1600.webp 1600w\" sizes=\"auto\"><img loading=\"lazy\" decoding=\"async\" src=\"/img/vQjw6TH34Y-960.jpeg\" alt=\"VPC Peering across separate projects\" width=\"1600\" height=\"894\" srcset=\"/img/vQjw6TH34Y-960.jpeg 960w, /img/vQjw6TH34Y-1600.jpeg 1600w\" sizes=\"auto\"></picture></button><p></p><h3 id=\"site-to-site-s2s-vpn\"><a href=\"#site-to-site-s2s-vpn\" class=\"heading-anchor\">Site-to-site (S2S) VPN</a></h3><p>Site-to-Site VPN is a secure networking solution that connects two or more separate networks, typically in different physical locations, enabling them to communicate as if they were directly connected. This technology creates an encrypted tunnel over the public internet, allowing organizations to securely link their geographically dispersed offices, data centers, or cloud resources.</p><p><is-land on:idle></is-land></p><dialog class=\"flow modal44\"><button autofocus class=\"button\">Close</button><picture><source type=\"image/webp\" srcset=\"/img/eobX_P1cOC-960.webp 960w, /img/eobX_P1cOC-1600.webp 1600w\" sizes=\"auto\"><img loading=\"lazy\" decoding=\"async\" src=\"/img/eobX_P1cOC-960.jpeg\" alt=\"site to site vpn\" width=\"1600\" height=\"410\" srcset=\"/img/eobX_P1cOC-960.jpeg 960w, /img/eobX_P1cOC-1600.jpeg 1600w\" sizes=\"auto\"></picture></dialog><button data-index=\"44\"><picture><source type=\"image/webp\" srcset=\"/img/eobX_P1cOC-960.webp 960w, /img/eobX_P1cOC-1600.webp 1600w\" sizes=\"auto\"><img loading=\"lazy\" decoding=\"async\" src=\"/img/eobX_P1cOC-960.jpeg\" alt=\"site to site vpn\" width=\"1600\" height=\"410\" srcset=\"/img/eobX_P1cOC-960.jpeg 960w, /img/eobX_P1cOC-1600.jpeg 1600w\" sizes=\"auto\"></picture></button><p></p><p>Key aspects of Site-to-Site VPN:</p><ul class=\"list\"><li><strong>VPN Gateways:</strong> Specialized devices or software applications are deployed at each network endpoint to act as tunnel terminators.</li><li><strong>Encryption:</strong> Data is encrypted before entering the VPN tunnel and decrypted upon reaching its destination, ensuring confidentiality during transit.</li><li><strong>Tunneling Protocols:</strong> Protocols like IPsec establish the secure tunnel and manage encryption/decryption processes.</li><li><strong>Routing Configuration:</strong> Network administrators configure routing to direct traffic through the VPN tunnel instead of the public internet.</li></ul><p>A Site-to-Site VPN is one of the most secure ways to connect resources across different locations. While the tunnel relies on public internet infrastructure, all traffic is encrypted, ensuring that data cannot be decrypted without the secret key used to establish the connection.&nbsp; Because of this, securely sharing the secret key is crucial and should never be transmitted through unencrypted channels. With strong encryption and secure key management, Site-to-Site VPN provides an excellent solution for organizations requiring high levels of data protection and privacy across geographically dispersed networks.</p><h3 id=\"other-networking-possibilities\"><a href=\"#other-networking-possibilities\" class=\"heading-anchor\">Other Networking Possibilities</a></h3><p>There are more advanced options available for securing network traffic and isolating it from the public internet. Two notable solutions worth mentioning are:</p><h4 id=\"mpls-multiprotocol-label-switching\"><a href=\"#mpls-multiprotocol-label-switching\" class=\"heading-anchor\">MPLS (Multiprotocol Label Switching)</a></h4><p>MPLS is a packet forwarding technology that operates between Layer 2 and Layer 3 of the OSI model. It typically utilizes a dedicated network infrastructure, ensuring no public connection is involved. Implementing MPLS requires finding a vendor capable of leasing physical cables for exclusive use. While more expensive and complex to implement than previously mentioned options, MPLS offers enhanced security and guaranteed connection speeds.</p><h4 id=\"dedicated-link\"><a href=\"#dedicated-link\" class=\"heading-anchor\">Dedicated Link</a></h4><p>Cloud providers offer solutions like Google Cloud‚Äôs Dedicated Interconnect or AWS Direct Connect, which are faster to implement than MPLS, as the cloud provider handles much of the infrastructure. These options are ideal for establishing physical, private connections between on-premises networks and cloud provider networks. However, they may be excessive for connecting to a single data source on a data platform.</p><p>While these options provide additional layers of security and performance, they should be carefully considered based on specific organizational needs and resources.</p><h2 id=\"conclusion\"><a href=\"#conclusion\" class=\"heading-anchor\">Conclusion</a></h2><p>Selecting the right networking strategy for your data platform is critical to ensuring security, performance, and scalability. From public access to VPC peering and site-to-site VPNs, the choice of networking strategy significantly impacts your data platform‚Äôs security, performance, and flexibility. Each option comes with trade-offs that need to be considered.</p><ul class=\"list\"><li>Security should be a primary concern. Public access is the least secure option, while site-to-site VPN offers robust protection.</li><li>VPC peering provides an excellent balance of performance and security for resources within the same cloud provider.</li><li>Access Control Lists (ACLs) offer an additional layer of security for public access scenarios, allowing for fine-grained control.</li><li>Application layer security remains crucial regardless of the chosen networking option, complementing network-level protections.</li></ul><p>When designing your data platform‚Äôs networking architecture, consider your specific use case, security needs, and scalability requirements. Remember that a comprehensive approach, combining appropriate networking strategies with robust application-level security measures, will provide the most effective protection for your valuable data assets.</p><p>As technology evolves, staying updated on <strong>best practices and emerging solutions</strong> will help ensure your platform remains secure and efficient in the long run.</p></div>",
      "date_published": "2025-03-05T09:50:00Z"
    }
    ,{
      "id": "http://localhost:8080/blog/what-is-a-modular-data-platform/",
      "url": "http://localhost:8080/blog/what-is-a-modular-data-platform/",
      "title": "What is a Modular Data Platform?",
      "content_html": "<nav id=\"toc\" class=\"table-of-contents prose\"><ol><li class=\"flow\"><a href=\"#history-of-data-platforms-from-olap-cubes-to-hadoop-to-lakehouses\">History of Data Platforms: From OLAP Cubes to Hadoop to Lakehouses</a></li><li class=\"flow\"><a href=\"#reference-architecture-of-a-modern-and-modular-data-platform\">Reference Architecture of a Modern and Modular Data Platform</a><ol><li class=\"flow\"><a href=\"#data-sources\">Data Sources</a></li><li class=\"flow\"><a href=\"#ingestion\">Ingestion</a></li><li class=\"flow\"><a href=\"#landing\">Landing</a></li><li class=\"flow\"><a href=\"#preparation\">Preparation</a></li><li class=\"flow\"><a href=\"#modeling\">Modeling</a></li><li class=\"flow\"><a href=\"#consumption\">Consumption</a></li><li class=\"flow\"><a href=\"#data-cataloging\">Data Cataloging</a></li><li class=\"flow\"><a href=\"#data-orchestration\">Data Orchestration</a></li></ol></li><li class=\"flow\"><a href=\"#understanding-modularity-in-the-context-of-data-platforms\">Understanding Modularity in the Context of Data Platforms</a><ol><li class=\"flow\"><a href=\"#components-and-interfaces-before-tools\">Components and Interfaces Before Tools</a></li><li class=\"flow\"><a href=\"#workflows-and-developer-experience\">Workflows and Developer Experience</a></li><li class=\"flow\"><a href=\"#data-governance-and-security\">Data Governance and Security</a></li></ol></li><li class=\"flow\"><a href=\"#conclusion\">Conclusion</a></li></ol></nav><p><span id=\"toc-skipped\" class=\"visually-hidden\"></span></p><div class=\"flow prose\"><p></p><p>Modern data analytics can get complicated. With an abundance of tools, conflicting methodologies, and ever-evolving technologies, mistakes can be costly. However, at its core, data analytics remains grounded in a few fundamental principles. Understanding these fundamentals while leveraging modular and well-designed data platforms can significantly improve operational efficiency and decision-making.</p><h2 id=\"history-of-data-platforms-from-olap-cubes-to-hadoop-to-lakehouses\"><a href=\"#history-of-data-platforms-from-olap-cubes-to-hadoop-to-lakehouses\" class=\"heading-anchor\"><strong>History of Data Platforms: From OLAP Cubes to Hadoop to Lakehouses</strong></a></h2><p>The evolution of data platforms has been driven by two primary goals:</p><ul class=\"list\"><li><strong>Doing Analytics Better:</strong> improving analytics work with more efficient storage and retrieval of business and machine data; moving insights generation closer to the domain experts by improving self-service tools and processes</li><li><strong>Doing Better Analytics:</strong> increasing the value of analytics by having more and deeper insights; leverage statistical modelling, machine learning and AI to improve the quality of business decisions</li></ul><p>And so, while SQL, which was invented in 1975, remains at the core of analytics, there have been significant advances in technology.</p><p>OLAP cubes emerged in 1993, introducing multi-dimensional analysis. In the early 2000s, Hadoop revolutionized big data processing, allowing distributed storage and computing. More recently, the Lakehouse paradigm has sought to unify the best aspects of data warehouses and data lakes, improving performance, governance, and flexibility.</p><h2 id=\"reference-architecture-of-a-modern-and-modular-data-platform\"><a href=\"#reference-architecture-of-a-modern-and-modular-data-platform\" class=\"heading-anchor\"><strong>Reference Architecture of a Modern and Modular Data Platform</strong></a></h2><p>A modern data platform consists of several key components, each playing a crucial role in the data lifecycle. These components enable efficient data movement, transformation, and consumption while ensuring modularity and scalability.</p><p><is-land on:idle></is-land></p><dialog class=\"flow modal49\"><button autofocus class=\"button\">Close</button><picture><source type=\"image/webp\" srcset=\"/img/s6LJnDtkZT-960.webp 960w, /img/s6LJnDtkZT-1600.webp 1600w\" sizes=\"auto\"><img loading=\"lazy\" decoding=\"async\" src=\"/img/s6LJnDtkZT-960.jpeg\" alt width=\"1600\" height=\"824\" srcset=\"/img/s6LJnDtkZT-960.jpeg 960w, /img/s6LJnDtkZT-1600.jpeg 1600w\" sizes=\"auto\"></picture></dialog><button data-index=\"49\"><picture><source type=\"image/webp\" srcset=\"/img/s6LJnDtkZT-960.webp 960w, /img/s6LJnDtkZT-1600.webp 1600w\" sizes=\"auto\"><img loading=\"lazy\" decoding=\"async\" src=\"/img/s6LJnDtkZT-960.jpeg\" alt width=\"1600\" height=\"824\" srcset=\"/img/s6LJnDtkZT-960.jpeg 960w, /img/s6LJnDtkZT-1600.jpeg 1600w\" sizes=\"auto\"></picture></button><p></p><h3 id=\"data-sources\"><a href=\"#data-sources\" class=\"heading-anchor\"><strong>Data Sources</strong></a></h3><p>Data sources are the origin of information within an organization. These range from structured databases, APIs, and SaaS applications to unstructured sources such as logs, IoT streams, and social media feeds. Some sources offer modern APIs for easy integration, while others, particularly legacy systems, require extensive workarounds.</p><h3 id=\"ingestion\"><a href=\"#ingestion\" class=\"heading-anchor\"><strong>Ingestion</strong></a></h3><p>Ingestion refers to the process of transferring data from sources into the platform reliably. This is typically done via scheduled batch jobs, though some architectures incorporate real-time ingestion using event brokers like Kafka.</p><p>The ingestion landscape is fragmented, with numerous tools available, such as Azure Data Factory and Fivetran. However, no tool provides connectors for every possible source. Consequently, organizations often need to develop custom connectors, leading to maintenance challenges and dependencies on vendors.</p><h3 id=\"landing\"><a href=\"#landing\" class=\"heading-anchor\"><strong>Landing</strong></a></h3><p>Landing zones serve as the initial storage layer where raw, unprocessed data is deposited after ingestion. This stage ensures that data is captured in its original form, preserving fidelity and enabling downstream transformation.</p><p>Storage for this type of data (including lakehouse data) has been standardized around the AWS S3 object storage API. Consequently, most cloud providers now offer their own variations of object storage with APIs closely mirroring AWS S3.</p><h3 id=\"preparation\"><a href=\"#preparation\" class=\"heading-anchor\"><strong>Preparation</strong></a></h3><p>Since raw data can be messy and inconsistent, preparation is necessary to clean, standardize, and format it for further processing. This stage includes:</p><ul class=\"list\"><li>Data masking</li><li>Data anonymization</li><li>Structuring into standardized formats such as Delta Tables or Apache Iceberg Parquet files</li></ul><p>Note that both data masking and anonymization could be done also during landing on data ‚Äúin-transit‚Äù to avoid storing sensitive information on the platform.</p><p>Data engineers typically handle this step using workflow tools like Alteryx, Azure Data Factory, or programming languages such as Python.</p><h3 id=\"modeling\"><a href=\"#modeling\" class=\"heading-anchor\"><strong>Modeling</strong></a></h3><p>Modeling transforms prepared data into well-structured datasets optimized for analytical use. Historically, this was the ‚ÄúT‚Äù in ETL (Extract, Transform, Load). Today, tools like dbt have popularized the concept of modular and scalable transformation workflows.</p><h3 id=\"consumption\"><a href=\"#consumption\" class=\"heading-anchor\"><strong>Consumption</strong></a></h3><p>Once modeled, data is consumed in various ways, including:</p><ul class=\"list\"><li>Traditional dashboards and Excel reports</li><li>Embedded analytics within applications</li><li>AI-powered data exploration (e.g., generative AI and natural language querying)</li></ul><h3 id=\"data-cataloging\"><a href=\"#data-cataloging\" class=\"heading-anchor\"><strong>Data Cataloging</strong></a></h3><p>A data catalog is a comprehensive inventory of an organization‚Äôs data assets, documenting their structure, relationships, and usage. It extends beyond datasets to include analytical assets such as dashboards, reports, and Jupyter notebooks, ensuring a unified and well-organized view of available information.</p><p>Despite its critical role in data management, data cataloging is often overlooked or deprioritized in analytics projects. However, a well-maintained data catalog is fundamental to effective data governance and security. By systematically identifying all data assets, their ownership, and their respective domains, organizations can enhance discoverability, streamline compliance efforts, and facilitate data democratization.</p><h3 id=\"data-orchestration\"><a href=\"#data-orchestration\" class=\"heading-anchor\"><strong>Data Orchestration</strong></a></h3><p>Data orchestration refers to the automated coordination of ETL processes, from data ingestion and preparation to final modeling for consumption. It ensures that data flows seamlessly across different stages, reducing manual intervention and improving efficiency.</p><p>This industry is highly fragmented, with traditional IT approaches relying on UI-based tools such as Talend and Azure Data Factory. More modern methodologies, however, focus on code-driven orchestration using tools like Apache Airflow and Prefect. These newer solutions provide greater flexibility, scalability, and integration capabilities, making them preferred choices for organizations aiming to build robust and automated data pipelines.</p><h2 id=\"understanding-modularity-in-the-context-of-data-platforms\"><a href=\"#understanding-modularity-in-the-context-of-data-platforms\" class=\"heading-anchor\"><strong>Understanding Modularity in the Context of Data Platforms</strong></a></h2><p>Unlike ERP systems, which are often monolithic, data platforms are inherently modular. The diversity of data workflows and use cases makes it impractical to consolidate everything into a single tool.</p><p>Some vendors, such as Databricks and Microsoft Fabric, attempt to provide an all-in-one solution. However, even these platforms require integration with external components to cover all aspects of data management.</p><h3 id=\"components-and-interfaces-before-tools\"><a href=\"#components-and-interfaces-before-tools\" class=\"heading-anchor\"><strong>Components and Interfaces Before Tools</strong></a></h3><p>The success of a data platform hinges on well-defined interfaces between its components. A common pitfall is over-reliance on a single vendor, leading to inflexible architectures that struggle to adapt to evolving business needs. Organizations should prioritize:</p><ul class=\"list\"><li>Well-defined interfaces between tools</li><li>Single point for managing accesses (i.e. using A/D groups)</li><li>Loose coupling between components to enable flexibility</li></ul><h3 id=\"workflows-and-developer-experience\"><a href=\"#workflows-and-developer-experience\" class=\"heading-anchor\"><strong>Workflows and Developer Experience</strong></a></h3><p>A streamlined developer experience is crucial for maintaining data platform efficiency. Poorly designed workflows can introduce bottlenecks, reduce productivity, and increase technical debt. Best practices include:</p><ul class=\"list\"><li>Automating repetitive tasks (e.g., CI/CD for data pipelines)</li><li>Enforcing coding standards and documentation</li><li>Providing self-service capabilities for data consumers</li></ul><h3 id=\"data-governance-and-security\"><a href=\"#data-governance-and-security\" class=\"heading-anchor\"><strong>Data Governance and Security</strong></a></h3><p>With numerous tools and evolving datasets, data governance and security must be proactive rather than reactive. <strong>Traditional IT governance models, which assume static datasets, are insufficient for modern data platforms</strong>. Without a structured approach to creating and managing new data assets, governance becomes <strong>impossible</strong>.</p><p>Effective data governance requires:</p><ul class=\"list\"><li>Clear company policies on data access, privacy, and security.</li><li>A solid understanding of analytics workflows to incorporate governance steps and audit reviews seamlessly.</li><li>Automated data cataloging to maintain visibility into data assets and their ownership.</li><li>A comprehensive inventory of all analytics tools, ensuring each one is correctly configured and continuously monitored for compliance.</li></ul><p>While data governance is straightforward in principle, it requires a structured, realistic approach with well-defined steps to ensure its successful implementation and long-term effectiveness.</p><h2 id=\"conclusion\"><a href=\"#conclusion\" class=\"heading-anchor\"><strong>Conclusion</strong></a></h2><p>Modern data platforms are modular ecosystems that require careful design and governance to be effective. By understanding the historical evolution of data architectures, organizations can make informed decisions about structuring their platforms. Prioritizing interoperability, developer experience, and security ensures a scalable and efficient data operations strategy.</p><p>Organizations that embrace modularity and best practices in data management will not only improve operational efficiency but also gain a competitive advantage in an increasingly data-driven world.</p></div>",
      "date_published": "2025-02-10T09:30:00Z"
    }
    ,{
      "id": "http://localhost:8080/blog/breaking-down-prefect-deployments-to-improve-the-data-ops-efficiency/",
      "url": "http://localhost:8080/blog/breaking-down-prefect-deployments-to-improve-the-data-ops-efficiency/",
      "title": "Breaking Down Prefect Deployments To Improve The Data Ops Efficiency",
      "content_html": "<nav id=\"toc\" class=\"table-of-contents prose\"><ol><li class=\"flow\"><a href=\"#why-observability-matters-in-etl-processes\">Why Observability Matters in ETL Processes</a></li><li class=\"flow\"><a href=\"#the-pitfalls-of-a-single-monolithic-flow\">The Pitfalls of a Single Monolithic Flow</a></li><li class=\"flow\"><a href=\"#the-case-for-granulated-focused-flows\">The Case for Granulated, Focused Flows</a></li><li class=\"flow\"><a href=\"#conclusion\">Conclusion</a></li></ol></nav><p><span id=\"toc-skipped\" class=\"visually-hidden\"></span></p><div class=\"flow prose\"><p></p><p>When building data platforms, it‚Äôs tempting to focus entirely on the technology stack‚Äîchoosing shiny tools, debating between bulk loads or streaming, and designing storage and infrastructure to meet current needs. Yet, the rush to get data flowing often overshadows a crucial question: <strong>How will we monitor and operate all of this effectively?</strong></p><p>In the early stages, data projects typically start small: an MVP, one or two data sources, and a couple of flow runs per day. At this scale, operations often feel secondary‚Äî issues can be solved on the spot, and data engineering teams are under pressure to deliver data to the end users. But as the platform scales, this oversight catches up. Within months, many teams find themselves struggling to manage DataOps, with operational gaps threatening their progress.</p><p>Observability and day-to-day functionality are the bedrock of robust, scalable, and maintainable data pipelines. Modern orchestration tools like Prefect excel at breaking down pipelines into smaller, more manageable pieces, making it easier to monitor, troubleshoot, and deploy smoothly. By designing pipelines with intention and visibility in mind, teams can ensure their data platform remains reliable‚Äîeven as it evolves.</p><h2 id=\"why-observability-matters-in-etl-processes\"><a href=\"#why-observability-matters-in-etl-processes\" class=\"heading-anchor\">Why Observability Matters in ETL Processes</a></h2><p>Observability is a cornerstone of modern data engineering and operations. As ETL pipelines become critical for decision-making, data teams need deep visibility into pipeline performance and meaningful, actionable logs. The stakes are high‚Äîwhen something goes wrong, time is lost (and as we all know, time is money, or at least that is what they say), and teams are left scrambling to identify issues. At best, this means tedious log analysis and guesswork; at worst‚Äîhandling complaints from frustrated end-users.</p><p>To avoid these pitfalls, observability is a must. It not only ensures transparency with stakeholders but also equips teams to diagnose and address problems efficiently. Effective observability hinges on four dimensions:</p><ol class=\"list\"><li><strong>Transparency:</strong> Understand what each step in the pipeline does, including inputs and outputs.</li><li><strong>Traceability:</strong> Track data as it flows through the pipeline, making it possible to pinpoint where issues arise.</li><li><strong>Granularity:</strong> Drill down to isolate performance bottlenecks, failed tasks, or long-running tasks.</li><li><strong>Scalability:</strong> Expand monitoring and alerting systems to keep pace as the ETL process grows in complexity.</li></ol><h2 id=\"the-pitfalls-of-a-single-monolithic-flow\"><a href=\"#the-pitfalls-of-a-single-monolithic-flow\" class=\"heading-anchor\">The Pitfalls of a Single Monolithic Flow</a></h2><p>When starting an ELT project, it‚Äôs common to build one or two monolithic flows. These flows often contain dozens of tasks, which can inevitably grow as the solution scales.</p><p>The code usually looks then more or less like this:</p><p><strong>1. Task to fetch a list of tables from MS SQL</strong></p><pre class=\"language-sql\"><code class=\"language-sql\"><span class=\"token variable\">@task</span>\ndef get_table_names<span class=\"token punctuation\">(</span>conn_str: str<span class=\"token punctuation\">)</span> <span class=\"token operator\">-</span><span class=\"token operator\">&gt;</span> List<span class=\"token punctuation\">[</span>str<span class=\"token punctuation\">]</span>:\n    <span class=\"token string\">\"\"\"\n    Connect to an MS SQL database and return a list of tables.\n    \"\"\"</span>\n    query <span class=\"token operator\">=</span> <span class=\"token string\">\"\"\"\n    SELECT TABLE_NAME\n    FROM INFORMATION_SCHEMA.TABLES\n    WHERE TABLE_TYPE = 'BASE TABLE'\n      AND TABLE_CATALOG = DB_NAME()\n    \"\"\"</span>\n    <span class=\"token keyword\">with</span> pyodbc<span class=\"token punctuation\">.</span><span class=\"token keyword\">connect</span><span class=\"token punctuation\">(</span>conn_str<span class=\"token punctuation\">)</span> <span class=\"token keyword\">as</span> conn:\n        <span class=\"token keyword\">cursor</span> <span class=\"token operator\">=</span> conn<span class=\"token punctuation\">.</span><span class=\"token keyword\">cursor</span><span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n        <span class=\"token keyword\">cursor</span><span class=\"token punctuation\">.</span><span class=\"token keyword\">execute</span><span class=\"token punctuation\">(</span>query<span class=\"token punctuation\">)</span>\n        results <span class=\"token operator\">=</span> <span class=\"token keyword\">cursor</span><span class=\"token punctuation\">.</span>fetchall<span class=\"token punctuation\">(</span><span class=\"token punctuation\">)</span>\n\n    table_names <span class=\"token operator\">=</span> <span class=\"token punctuation\">[</span><span class=\"token keyword\">row</span><span class=\"token punctuation\">[</span><span class=\"token number\">0</span><span class=\"token punctuation\">]</span> <span class=\"token keyword\">for</span> <span class=\"token keyword\">row</span> <span class=\"token operator\">in</span> results<span class=\"token punctuation\">]</span>\n    <span class=\"token keyword\">return</span> table_names</code></pre><p><strong>2. Task to extract data from a specific table into a DataFrame</strong></p><pre class=\"language-sql\"><code class=\"language-sql\"><span class=\"token variable\">@task</span>\ndef extract_table_to_df<span class=\"token punctuation\">(</span>conn_str: str<span class=\"token punctuation\">,</span> table_name: str<span class=\"token punctuation\">)</span> <span class=\"token operator\">-</span><span class=\"token operator\">&gt;</span> pd<span class=\"token punctuation\">.</span>DataFrame:\n    <span class=\"token string\">\"\"\"\n    Run SELECT * on the given table and return a Pandas DataFrame.\n    \"\"\"</span>\n    query <span class=\"token operator\">=</span> f<span class=\"token string\">\"SELECT * FROM {table_name}\"</span>\n    <span class=\"token keyword\">with</span> pyodbc<span class=\"token punctuation\">.</span><span class=\"token keyword\">connect</span><span class=\"token punctuation\">(</span>conn_str<span class=\"token punctuation\">)</span> <span class=\"token keyword\">as</span> conn:\n        df <span class=\"token operator\">=</span> pd<span class=\"token punctuation\">.</span>read_sql<span class=\"token punctuation\">(</span>query<span class=\"token punctuation\">,</span> conn<span class=\"token punctuation\">)</span>\n    <span class=\"token keyword\">return</span> df</code></pre><p><strong>3. Task to write a DataFrame to S3 as a Parquet file</strong></p><pre class=\"language-sql\"><code class=\"language-sql\"><span class=\"token variable\">@task</span>\ndef write_parquet_to_s3<span class=\"token punctuation\">(</span>df: pd<span class=\"token punctuation\">.</span>DataFrame<span class=\"token punctuation\">,</span> bucket: str<span class=\"token punctuation\">,</span> table_name: str<span class=\"token punctuation\">)</span>:\n    <span class=\"token string\">\"\"\"\n    Write the given DataFrame as a Parquet file to the specified S3 bucket.\n    \"\"\"</span>\n\n    s3_path <span class=\"token operator\">=</span> f<span class=\"token string\">\"s3://{bucket}/{table_name}.parquet\"</span>\n\n    df<span class=\"token punctuation\">.</span>to_parquet<span class=\"token punctuation\">(</span>\n        path<span class=\"token operator\">=</span>s3_path<span class=\"token punctuation\">,</span>\n        <span class=\"token keyword\">engine</span><span class=\"token operator\">=</span><span class=\"token string\">\"pyarrow\"</span><span class=\"token punctuation\">,</span>\n        <span class=\"token keyword\">index</span><span class=\"token operator\">=</span><span class=\"token boolean\">False</span><span class=\"token punctuation\">,</span>\n        storage_options<span class=\"token operator\">=</span>{\n            <span class=\"token string\">\"key\"</span>: get_secret_from_gcsm<span class=\"token punctuation\">(</span><span class=\"token string\">\"AWS_ACCESS_KEY_ID\"</span><span class=\"token punctuation\">)</span><span class=\"token punctuation\">,</span>     \n            <span class=\"token string\">\"secret\"</span>: get_secret_from_gcsm<span class=\"token punctuation\">(</span><span class=\"token string\">\"AWS_SECRET_ACCESS_KEY\"</span><span class=\"token punctuation\">)</span>}<span class=\"token punctuation\">,</span>\n    <span class=\"token punctuation\">)</span>\n\n    <span class=\"token keyword\">return</span> s3_path</code></pre><p><strong>4. Main Flow orchestrating the above tasks</strong></p><pre class=\"language-sql\"><code class=\"language-sql\"><span class=\"token variable\">@flow</span>\ndef ms_sql_to_s3_flow<span class=\"token punctuation\">(</span>\n    conn_str: str<span class=\"token punctuation\">,</span>\n    bucket: str<span class=\"token punctuation\">,</span>\n<span class=\"token punctuation\">)</span>:\n    <span class=\"token string\">\"\"\"\n    A Prefect flow that loads all tables from MS SQL into S3 as Parquet files.\n    \"\"\"</span>\n    <span class=\"token comment\"># Fetch all table names</span>\n    <span class=\"token keyword\">tables</span> <span class=\"token operator\">=</span> get_table_names<span class=\"token punctuation\">(</span>conn_str<span class=\"token punctuation\">)</span>\n\n    <span class=\"token comment\"># For each table, extract and load</span>\n    <span class=\"token keyword\">for</span> table_name <span class=\"token operator\">in</span> <span class=\"token keyword\">tables</span>:\n        df <span class=\"token operator\">=</span> extract_table_to_df<span class=\"token punctuation\">(</span>conn_str<span class=\"token punctuation\">,</span> table_name<span class=\"token punctuation\">)</span>\n        write_parquet_to_s3<span class=\"token punctuation\">(</span>df<span class=\"token punctuation\">,</span> bucket<span class=\"token punctuation\">,</span> table_name<span class=\"token punctuation\">)</span></code></pre><p>At first, this approach might seem efficient. A single flow can ingest all objects from a database in one run‚Äîstraightforward and convenient, right?</p><p>Initially, with just 10 objects in the database, it works well enough. But as the source database grows to 100 or more items, the cracks begin to show. Usually, this approach introduces several significant challenges:</p><ol class=\"list\"><li><strong>Difficult Monitoring:</strong> A single failure makes the entire flow as failed, forcing data engineers to dig through logs to identify the problematic element.</li></ol><p><is-land on:idle></is-land></p><dialog class=\"flow modal4\"><button autofocus class=\"button\">Close</button><picture><source type=\"image/webp\" srcset=\"/img/U7yo_GzWvo-960.webp 960w, /img/U7yo_GzWvo-1600.webp 1600w\" sizes=\"auto\"><img loading=\"lazy\" decoding=\"async\" src=\"/img/U7yo_GzWvo-960.jpeg\" alt=\"Single Monolithic Flow difficult monitorig\" width=\"1600\" height=\"109\" srcset=\"/img/U7yo_GzWvo-960.jpeg 960w, /img/U7yo_GzWvo-1600.jpeg 1600w\" sizes=\"auto\"></picture></dialog><button data-index=\"4\"><picture><source type=\"image/webp\" srcset=\"/img/U7yo_GzWvo-960.webp 960w, /img/U7yo_GzWvo-1600.webp 1600w\" sizes=\"auto\"><img loading=\"lazy\" decoding=\"async\" src=\"/img/U7yo_GzWvo-960.jpeg\" alt=\"Single Monolithic Flow difficult monitorig\" width=\"1600\" height=\"109\" srcset=\"/img/U7yo_GzWvo-960.jpeg 960w, /img/U7yo_GzWvo-1600.jpeg 1600w\" sizes=\"auto\"></picture></button><p></p><ol start=\"2\" class=\"list\"><li><strong>Limited Reusability:</strong> It‚Äôs hard to run deployments for one table or only failed objects without re-running the entire flow.</li><li><strong>Reduced Scheduling Flexibility:</strong> Monoflow might require running all tasks together, even when only a subset of tasks needs frequent execution.</li><li><strong>SLA Reporting:</strong> Measuring success rates becomes much harder. Reporting on flow run states is unreliable since the failure on one table out of 1,000 causes the whole flow to be marked as failed. Again, this requires digging into logs to measure performance accurately.</li><li><strong>Execution Time</strong>: Monolith flows are time-consuming and don‚Äôt allow parallel execution, hindering efficiency.</li></ol><p>In essence, a monolithic approach limits observability, reduces performance, and complicates operations.</p><h2 id=\"the-case-for-granulated-focused-flows\"><a href=\"#the-case-for-granulated-focused-flows\" class=\"heading-anchor\">The Case for Granulated, Focused Flows</a></h2><p>When it comes to sizing your ELT flows, trust me‚Äîyou‚Äôd rather fight 100 duck-sized horses than one horse-sized duck. In other words, breaking down monolithic flows into smaller, focused units is the key to scaling effectively.</p><p>The first step is modularizing the monolithic flow. Ideally, each deployment flow should represent a single data object. For example, if you‚Äôre ingesting data from an SQL database, think about organizing your process to allow for per-table scalability‚Äîit might require more time investment but will divide the complexity.</p><p>With the right tools, this approach is not as complex as it sounds. Prefect allows defining deployments with YAML, leveraging project-level default configurations stored under the definitions: key in the prefect.yml file. There are two main ways of using them:</p><ul class=\"list\"><li>using the entire value as-is,</li><li>using part of the pre-defined values (eg. overriding only a single parameter).</li></ul><p><is-land on:idle></is-land></p><dialog class=\"flow modal5\"><button autofocus class=\"button\">Close</button><picture><source type=\"image/webp\" srcset=\"/img/8LA2FSTlgr-556.webp 556w\" sizes=\"auto\"><img loading=\"lazy\" decoding=\"async\" src=\"/img/8LA2FSTlgr-556.jpeg\" alt=\"predefined values for granulated, focused flows\" width=\"556\" height=\"377\"></picture></dialog><button data-index=\"5\"><picture><source type=\"image/webp\" srcset=\"/img/8LA2FSTlgr-556.webp 556w\" sizes=\"auto\"><img loading=\"lazy\" decoding=\"async\" src=\"/img/8LA2FSTlgr-556.jpeg\" alt=\"predefined values for granulated, focused flows\" width=\"556\" height=\"377\"></picture></button><p></p><p>This way, you can stick with the pre-defined daily schedule as it is, which makes the deployment creation way easier than it initially seemed.</p><p>Here‚Äôs why granular flow deployments are worth the effort:</p><ol class=\"list\"><li><strong>Parallelism:</strong> Each table flow can run independently in parallel with others. If one table experiences performance degradation, it does not immediately affect the rest. And yes, it can be included in the monoflow, but why spend time reinventing the wheel? Orchestrator can take care of that.</li><li><strong>Monitoring and Error Handling</strong>: If a single table fails, its flow run alone fails. This allows one to quickly identify the failed table, debug it, and restart only that deployment. Also, it helps with monitoring the execution time of a particular table or with tracking data quality issues.</li><li><strong>Improved Data Quality Testing:</strong> It‚Äôs much easier to enable data quality tests per data object instead of having universal rules. Is it better to have customized tests per column in the data set or check if the set is not null only?</li><li><strong>Incremental Maintenance and Scalability:</strong> Modular flows create clear boundaries. Adding or updating flows for new or modified tables doesn‚Äôt necessarily affect existing deployments. Each table‚Äôs logic is easier to maintain and evolve in isolation.</li><li><strong>Version Control:</strong> Each deployment can be versioned independently. This makes testing changes for one table more straightforward and also makes the CI/CD implementation easier.</li><li><strong>Team Collaboration:</strong> Different engineers can own specific deployments, making it easier to distribute responsibility and keep changes localized. It‚Äôs good to use tags to identify project-related deployments‚Äîe.g., it‚Äôs possible to have a sales tag in Prefect for sales data-related processes.</li><li><strong>Granular Scheduling</strong>: Some tables need to be refreshed three times daily, but some should be reloaded monthly only. The granular approach allows for more playing with the schedule.</li><li><strong>SLA Reporting:</strong> It‚Äôs simpler, as the real situation is shown on the run level, and failure means real failure.</li></ol><p><is-land on:idle></is-land></p><dialog class=\"flow modal6\"><button autofocus class=\"button\">Close</button><picture><source type=\"image/webp\" srcset=\"/img/a2G_ARBp8W-887.webp 887w\" sizes=\"auto\"><img loading=\"lazy\" decoding=\"async\" src=\"/img/a2G_ARBp8W-887.jpeg\" alt=\"granular flow deployments SLA reporting\" width=\"887\" height=\"228\"></picture></dialog><button data-index=\"6\"><picture><source type=\"image/webp\" srcset=\"/img/a2G_ARBp8W-887.webp 887w\" sizes=\"auto\"><img loading=\"lazy\" decoding=\"async\" src=\"/img/a2G_ARBp8W-887.jpeg\" alt=\"granular flow deployments SLA reporting\" width=\"887\" height=\"228\"></picture></button><p></p><h2 id=\"conclusion\"><a href=\"#conclusion\" class=\"heading-anchor\">Conclusion</a></h2><p>In conclusion, a granular approach to orchestrated deployments is more than just a technical choice‚Äîit‚Äôs a strategic advantage. By breaking large, monolithic pipelines into focused, modular flows, data teams gain clearer observability, easier troubleshooting, and the flexibility to handle diverse scheduling needs</p><p>Focusing on key concerns‚Äîperformance, reliability, and maintainability‚Äîcan help you build a better data solution using a granular approach. Over time, this approach will lead to more predictable, scalable, and maintainable ETL processes.</p></div>",
      "date_published": "2025-01-28T09:45:00Z"
    }
    ,{
      "id": "http://localhost:8080/blog/dlt-and-prefect-a-great-combo-for-streamlined-data-ingestion-pipelines/",
      "url": "http://localhost:8080/blog/dlt-and-prefect-a-great-combo-for-streamlined-data-ingestion-pipelines/",
      "title": "dlt and Prefect, a Great Combo for Streamlined Data Ingestion Pipelines",
      "content_html": "<nav id=\"toc\" class=\"table-of-contents prose\"><ol><li class=\"flow\"><a href=\"#a-short-introduction-to-dlt-and-prefect\">A Short Introduction to dlt and Prefect</a><ol><li class=\"flow\"><a href=\"#dlt\">dlt</a></li><li class=\"flow\"><a href=\"#prefect\">Prefect</a></li></ol></li><li class=\"flow\"><a href=\"#creating-data-connectors-and-pipelines-with-dlt\">Creating Data Connectors and Pipelines with dlt</a><ol><li class=\"flow\"><a href=\"#data-pipeline-features\">Data Pipeline Features</a><ol><li class=\"flow\"><a href=\"#modularity\">Modularity</a></li><li class=\"flow\"><a href=\"#extensibility\">Extensibility</a></li><li class=\"flow\"><a href=\"#reliability\">Reliability</a></li><li class=\"flow\"><a href=\"#security\">Security</a></li><li class=\"flow\"><a href=\"#privacy\">Privacy</a></li><li class=\"flow\"><a href=\"#efficiency\">Efficiency</a></li></ol></li></ol></li><li class=\"flow\"><a href=\"#orchestrating-data-pipelines-with-prefect\">Orchestrating Data Pipelines with Prefect</a><ol><li class=\"flow\"><a href=\"#orchestration-job-features\">Orchestration Job Features</a><ol><li class=\"flow\"><a href=\"#alerting\">Alerting</a></li><li class=\"flow\"><a href=\"#reliability-1\">Reliability</a></li><li class=\"flow\"><a href=\"#secret-management\">Secret Management</a></li><li class=\"flow\"><a href=\"#distributed-processing\">Distributed Processing</a></li></ol></li></ol></li><li class=\"flow\"><a href=\"#production-workflow\">Production Workflow</a><ol><li class=\"flow\"><a href=\"#overview\">Overview</a></li><li class=\"flow\"><a href=\"#configuring-dlt\">Configuring dlt</a></li><li class=\"flow\"><a href=\"#creating-a-dlt-pipeline\">Creating a dlt Pipeline</a><ol><li class=\"flow\"><a href=\"#pipeline-design\">Pipeline Design</a></li><li class=\"flow\"><a href=\"#inspecting-the-data-manually\">Inspecting the Data Manually</a></li><li class=\"flow\"><a href=\"#testing-the-pipeline\">Testing the Pipeline</a></li></ol></li><li class=\"flow\"><a href=\"#creating-a-prefect-flow-and-deployment\">Creating a Prefect Flow and Deployment</a><ol><li class=\"flow\"><a href=\"#flow-design\">Flow Design</a></li><li class=\"flow\"><a href=\"#handling-pipeline-secrets\">Handling Pipeline Secrets</a></li></ol></li><li class=\"flow\"><a href=\"#deploying-to-production\">Deploying to Production</a></li></ol></li><li class=\"flow\"><a href=\"#summary\">Summary</a></li><li class=\"flow\"><a href=\"#next-steps\">Next steps</a><ol><li class=\"flow\"><a href=\"#data-transformation\">Data Transformation</a></li><li class=\"flow\"><a href=\"#ready-to-dive-deeper\">Ready to Dive Deeper?</a></li></ol></li><li class=\"flow\"><a href=\"#footnotes\">Footnotes</a></li></ol></nav><p><span id=\"toc-skipped\" class=\"visually-hidden\"></span></p><div class=\"flow prose\"><p></p><p><strong>Doing data ingestion right is hard‚Ä¶</strong></p><p>Despite advances in data engineering, data ingestion, which includes the Extract and Load (EL) steps of the ELT process, remains a persistent challenge for many data teams.</p><p>This complexity is often due to the real-world limitations of open-source tools, leading teams to opt for UI-based solutions. While these tools are great for getting started quickly, they often lack the flexibility and scalability required for production-grade data platforms.<br>In the era of AI, UI-based tools face one more limitation: they miss out on most of the benefits of the advanced code generation capacity of modern LLMs (Large Language Models)<a href=\"#footnotes\">[1]</a>.</p><p>Even if teams do decide to use open-source solutions, they often end up creating volumes of low-quality glue code. This in-house software, typically written in a rush by non-professional engineers, often fails to meet essential requirements for modern data platforms, such as EaC (Everything as Code), security, monitoring &amp; alerting, reliability, or extensibility. Moreover, since it‚Äôs written by non-professional engineers, such code is far more brittle and much harder to maintain and modify. Consequently, all modifications to the code (such as adding new features or fixing bugs) take much more time and are far riskier than they should be.</p><p><strong>‚Ä¶but there is light at the end of the tunnel</strong></p><p>Luckily, in recent years, with the growing adoption of software engineering practices, we‚Äôve seen a professionalization of the data engineering field. This has resulted in the creation of a number of high-quality, open-source tools that simplify and improve the quality of data engineering work, such as <a href=\"https://dlthub.com/\" rel=\"noopener\">dlt</a> and <a href=\"https://www.prefect.io/\" rel=\"noopener\">Prefect</a>.</p><p>In this article, we explore how dlt and Prefect can be seamlessly integrated to implement a best-practice data ingestion component of a modern data platform. Our insights are grounded in real-world experience designing and implementing scalable, code-based data platforms with these open-source tools.</p><h2 id=\"a-short-introduction-to-dlt-and-prefect\"><a href=\"#a-short-introduction-to-dlt-and-prefect\" class=\"heading-anchor\">A Short Introduction to dlt and Prefect</a></h2><h3 id=\"dlt\"><a href=\"#dlt\" class=\"heading-anchor\">dlt</a></h3><p><a href=\"https://dlthub.com/\" rel=\"noopener\">dlt</a> is a Python data ingestion framework enabling data engineers to define connectors and pipelines as code. It offers a rich set of features for building best-practice pipelines and supports both built-in and custom connectors built with regular Python code.</p><p>dlt ingests data in <a href=\"https://dlthub.com/docs/reference/explainers/how-dlt-works\" rel=\"noopener\">three stages</a>: extract, normalize, and load. The <strong>extract</strong> stage downloads source data to disk. The <strong>normalize</strong> stage applies light transformations to the data, such as column renaming or datetime parsing. The <strong>load</strong> stage loads the data into the destination system.</p><p>Here‚Äôs a compact guide to key dlt concepts:</p><ul class=\"list\"><li><strong>dlt config</strong>: dlt can be configured in three ways: through files (<code>config.toml and secrets.toml</code>), environment variables, and Python code.<br>Using <code>config.toml</code> for default settings is recommended, as it‚Äôs easy to store the file together with pipeline code on git. While it can contain some pipeline-level settings as well, its main purpose is to configure global behavior such as logging, parallelization, execution settings, and source or destination configuration common to all pipelines.</li><li><strong>Resource</strong> and <strong>Source</strong>:<br>A resource is a representation of a single item in a dataset. It can be a file, a database table, a REST API endpoint, etc.<br>A source is a collection of resources, such as a filesystem (eg. s3), a database, or a REST API.<br>By applying hints to the resource with <code>resource.apply_hints()</code>, we can configure extraction settings specific to the resource, a pipeline, or a pipeline run, such as primary key, cursor column, column typing, partitioning, etc. We can also apply some light transformations to the data (eg. data masking) before it‚Äôs loaded to the destination with the <code>resource.add_map()</code> method.<br>dlt is flexible when it comes to working with sources and resources, and it‚Äôs easy to use either, depending on the need.</li><li><strong>Pipeline:</strong> In dlt, pipeline describes the flow of data from a source (or resource) to a destination. Each pipeline handles a single source&lt;-&gt;destination pair and takes a source or resource as input.<br>Pipelines can be reused to ingest different resources each run. For example, we can have one ‚ÄúPostgres to S3‚Äù pipeline, but ingest each Postgres table separately due to different scheduling or configuration needs.<br>A pipeline definition contains pipeline- or pipeline run-specific destination configuration, as well as settings for the load phase of the ingestion. Under the hood, a pipeline run (<code>pipeline.run()</code>) executes each pipeline step: extract (<code>pipeline.extract()</code>), normalize (<code>pipeline.normalize()</code>), and load (<code>pipeline.load()</code>).</li></ul><h3 id=\"prefect\"><a href=\"#prefect\" class=\"heading-anchor\">Prefect</a></h3><p><a href=\"https://www.prefect.io/\" rel=\"noopener\">Prefect</a> is a Python data orchestration library that allows data and machine learning engineers to define data workflows (data ingestion, transformation, model training, etc.) as code. It provides a rich set of features to help engineers implement best-practice data orchestration workflows.</p><p>Its cloud offering eliminates the historically stressful and labor-intensive maintenance of data orchestration systems.</p><p>Let‚Äôs unpack the core concepts of Prefect:</p><ul class=\"list\"><li><strong>Task:</strong> A task is a single unit of work in a Prefect flow. It describes a single step to be executed in the workflow.<br>While it‚Äôs possible to implement the logic of the step directly in the task, in most cases, we recommend keeping tasks as thin wrappers around regular Python functions.</li><li><strong>Flow:</strong> A flow is a collection of tasks that define a data workflow. You can think of it as a graph of tasks, describing their relationship (eg. this task should always run after this one, and this other task should run after that one, but only if it fails).<br>Similar to a dlt pipeline, the same flow can be reused with different sets of parameters. An instance of a flow with specific parameter values is called a <a href=\"https://docs.prefect.io/v3/deploy/index\" rel=\"noopener\">deployment</a>.<br>In this article, we utilize this fact by utilizing a single <code>extract_and_load()</code> flow capable of executing any dlt pipeline, depending on the parameters passed to it. As a result, each ingestion becomes a new Prefect deployment rather than a new flow, which has a major consequence: deployments can be defined with YAML, which means that they don‚Äôt require any Python code to be written, which means users don‚Äôt need to set up a local Python development environment just to eg. ingest a new table with an existing pipeline. Instead, we can, for example, expose a simple application that allows non-technical users to create new deployments with a few clicks.</li><li><strong>Deployment:</strong> A deployment is a way to run a flow with a specific set of parameters and environment configuration. While most environment configurations in Prefect would typically be defined at the workspace level, deployments allow for overriding some of these settings, including on a per-run basis, which simplifies testing and debugging.</li></ul><h2 id=\"creating-data-connectors-and-pipelines-with-dlt\"><a href=\"#creating-data-connectors-and-pipelines-with-dlt\" class=\"heading-anchor\">Creating Data Connectors and Pipelines with dlt</a></h2><p>Now that we‚Äôve covered the theoretical underpinnings of dlt and Prefect, it‚Äôs time to see these concepts in action. We‚Äôll explore how to implement best-practice dlt pipelines and bring these tools to life.</p><h3 id=\"data-pipeline-features\"><a href=\"#data-pipeline-features\" class=\"heading-anchor\">Data Pipeline Features</a></h3><p>Alright, before we dive into the technical part, let‚Äôs start with the basics. A production-grade data pipeline needs to have several key features:</p><ol class=\"list\"><li>Modularity: The pipeline should be designed to allow the reuse of components across multiple pipelines.</li><li>Extensibility: The pipeline must be upgradeable without disrupting ongoing production jobs.</li><li>Reliability: The ability to inspect pipeline execution and quickly identify and resolve issues is crucial.</li><li>Security: Proper mechanisms must be in place to securely store and access secrets.</li><li>Privacy: Data storage should adhere to privacy regulations, ensuring compliance.</li><li>Efficiency: Pipelines must be optimized for cost-effective execution.</li></ol><p>Data pipelines aren‚Äôt one-size-fits-all, and achieving a production-grade pipeline involves ensuring those key features. But how to get there?</p><h4 id=\"modularity\"><a href=\"#modularity\" class=\"heading-anchor\">Modularity</a></h4><p>To achieve modularity, it‚Äôs best to split the dlt pipeline code into the following structure:</p><pre class=\"language-bash\"><code class=\"language-bash\">‚îú‚îÄ‚îÄ pipelines\n‚îÇ   ‚îú‚îÄ‚îÄ a_to_c.py\n‚îÇ   ‚îú‚îÄ‚îÄ b_to_c.py\n‚îÇ   ‚îî‚îÄ‚îÄ utils.py</code></pre><p>In this structure, <code>a_to_c.py</code> and <code>b_to_c.py</code> represent two example pipelines, each handling data from a source system (a and b) to a destination system ¬©.</p><p>The <code>utils.py</code> file contains common utilities such as data masking implementation, default configuration for source and destination systems, or default pipeline configuration (except configuration specified in dlt‚Äôs <code>config.toml</code>; for more information, see the dlt config paragraph in <a href=\"#dlt\">the dlt section</a>).</p><h4 id=\"extensibility\"><a href=\"#extensibility\" class=\"heading-anchor\">Extensibility</a></h4><p>Implementing extensibility goes beyond modularity. The code should also be testable, and ideally, automated testing should be integrated into the CI/CD process.</p><p>Since dlt pipelines are implemented using Python, they can be tested with common tools like <code>pytest</code>. Unit tests should focus on custom utility functions, while integration tests verify the entire pipeline‚Äôs behavior.</p><p>For integration testing, use a local database or disk drive instead of the target database. <a href=\"https://duckdb.org/\" rel=\"noopener\">DuckDB</a> is a great choice for this purpose, as it‚Äôs a lightweight, in-memory database that can be used to inspect the loaded data quickly.</p><h4 id=\"reliability\"><a href=\"#reliability\" class=\"heading-anchor\">Reliability</a></h4><p>To maintain trust with data platform users, make sure that when production pipelines fail, you are informed immediately and can recover quickly. While we recommend <a href=\"#alerting\">implementing alerting in the orchestration layer</a>, pipeline recoverability depends on having access to detailed logs.</p><p>Luckily, dlt provides rich built-in logging and error-handling mechanisms. It‚Äôs a good idea to also enable <a href=\"https://dlthub.com/docs/general-usage/pipeline#display-the-loading-progress\" rel=\"noopener\">progress monitoring</a> for additional useful information, such as CPU and memory usage.</p><h4 id=\"security\"><a href=\"#security\" class=\"heading-anchor\">Security</a></h4><p>dlt supports various ways of storing credentials. For local use, secrets can be stored in a .<code>dlt/secrets.toml</code> file, while production environments may benefit from an external credential store, such as <a href=\"https://cloud.google.com/security/products/secret-manager?hl=en\" rel=\"noopener\">Google Cloud Secret Manager</a>. To accomplish this, you can store the <a href=\"https://dlthub.com/docs/walkthroughs/add_credentials#retrieving-credentials-from-google-cloud-secret-manager\" rel=\"noopener\">secret retrieval utility function</a> in <code>utils.py</code> and reuse it within your pipelines.</p><p>However, since we‚Äôre using Prefect for orchestration, it‚Äôs also possible to follow a different path and <a href=\"#secret-management\">use Prefect Secrets to store the credentials</a>.</p><h4 id=\"privacy\"><a href=\"#privacy\" class=\"heading-anchor\">Privacy</a></h4><p>Data anonymization and/or pseudonymization are crucial to ensure compliance with privacy regulations. Data can be erased/anonymized either:</p><ol class=\"list\"><li>During the ingestion phase (in which case the original data never enters the destination system)</li></ol><p><is-land on:idle></is-land></p><dialog class=\"flow modal11\"><button autofocus class=\"button\">Close</button><picture><source type=\"image/webp\" srcset=\"/img/USynO6REx1-960.webp 960w, /img/USynO6REx1-1600.webp 1600w\" sizes=\"auto\"><img loading=\"lazy\" decoding=\"async\" src=\"/img/USynO6REx1-960.jpeg\" alt=\"data masking in data ingestion\" width=\"1600\" height=\"335\" srcset=\"/img/USynO6REx1-960.jpeg 960w, /img/USynO6REx1-1600.jpeg 1600w\" sizes=\"auto\"></picture></dialog><button data-index=\"11\"><picture><source type=\"image/webp\" srcset=\"/img/USynO6REx1-960.webp 960w, /img/USynO6REx1-1600.webp 1600w\" sizes=\"auto\"><img loading=\"lazy\" decoding=\"async\" src=\"/img/USynO6REx1-960.jpeg\" alt=\"data masking in data ingestion\" width=\"1600\" height=\"335\" srcset=\"/img/USynO6REx1-960.jpeg 960w, /img/USynO6REx1-1600.jpeg 1600w\" sizes=\"auto\"></picture></button><p></p><ol start=\"2\" class=\"list\"><li>During the transformation phase (in which case private data is stored in one or more layers in the destination system but hidden from the eyes of end users)</li></ol><p><is-land on:idle></is-land></p><dialog class=\"flow modal12\"><button autofocus class=\"button\">Close</button><picture><source type=\"image/webp\" srcset=\"/img/TpvfAarwiy-960.webp 960w, /img/TpvfAarwiy-1600.webp 1600w\" sizes=\"auto\"><img loading=\"lazy\" decoding=\"async\" src=\"/img/TpvfAarwiy-960.jpeg\" alt=\"data masking in transformation\" width=\"1600\" height=\"335\" srcset=\"/img/TpvfAarwiy-960.jpeg 960w, /img/TpvfAarwiy-1600.jpeg 1600w\" sizes=\"auto\"></picture></dialog><button data-index=\"12\"><picture><source type=\"image/webp\" srcset=\"/img/TpvfAarwiy-960.webp 960w, /img/TpvfAarwiy-1600.webp 1600w\" sizes=\"auto\"><img loading=\"lazy\" decoding=\"async\" src=\"/img/TpvfAarwiy-960.jpeg\" alt=\"data masking in transformation\" width=\"1600\" height=\"335\" srcset=\"/img/TpvfAarwiy-960.jpeg 960w, /img/TpvfAarwiy-1600.jpeg 1600w\" sizes=\"auto\"></picture></button><p></p><p>While dlt doesn‚Äôt provide built-in anonymization features, it does provide the necessary tools to implement the first option effectively.</p><p>For more information, see the <a href=\"https://dlthub.com/docs/general-usage/customising-pipelines/pseudonymizing_columns\" rel=\"noopener\">example</a> in the official documentation.</p><h4 id=\"efficiency\"><a href=\"#efficiency\" class=\"heading-anchor\">Efficiency</a></h4><p>To ensure pipelines are both cost-effective and high-performing, several optimization techniques can be applied:</p><ul class=\"list\"><li><strong>Incremental extraction</strong><br>Loading data incrementally allows for reducing the amount of data that needs to be <strong>extracted</strong>. Currently, dlt supports incremental extraction for its <a href=\"https://dlthub.com/docs/dlt-ecosystem/verified-sources/#core-sources\" rel=\"noopener\">core sources</a>: <a href=\"https://dlthub.com/docs/general-usage/incremental-loading#incremental-loading-with-a-cursor-field\" rel=\"noopener\">REST API</a>, <a href=\"https://dlthub.com/docs/walkthroughs/sql-incremental-configuration\" rel=\"noopener\">SQL database</a>, and <a href=\"https://dlthub.com/docs/dlt-ecosystem/verified-sources/filesystem/basic#5-incremental-loading\" rel=\"noopener\">filesystem</a>.<br>Incremental extraction allows us to download only new or modified data.</li><li><strong>Write dispositions</strong><br><a href=\"https://dlthub.com/docs/general-usage/incremental-loading#choosing-a-write-disposition\" rel=\"noopener\">Write dispositions</a> work in tandem with the two extraction methods to reduce the amount of data that needs to be <strong>loaded</strong>. For example, if you only extracted new and modified data, you don‚Äôt want to overwrite existing data, as that would result in data loss. In such a case, insert the new records and update the existing ones instead.</li><li><a href=\"https://dlthub.com/docs/reference/performance#parallelism\" rel=\"noopener\">Parallelization<br></a>dlt allows parallelizing each stage of the pipeline utilizing multithreading and multiprocessing (depending on the stage).<br>In cases where further parallelization is needed (i.e., the workload exceeds the capacity of a single machine), utilizing orchestrator-layer parallelization may be required. However, this scenario is now rare, as large virtual machines capable of processing petabytes of data are widely available, and dlt can leverage the machine‚Äôs resources more efficiently than older tools or typical in-house Python code.</li><li><a href=\"https://dlthub.com/docs/reference/performance\" rel=\"noopener\"><strong>Various other optimizations</strong></a></li></ul><p>As the topic of incremental loading can be complex even for seasoned data engineers, we‚Äôve prepared a diagram of all the viable ELT patterns:</p><p><is-land on:idle></is-land></p><dialog class=\"flow modal13\"><button autofocus class=\"button\">Close</button><picture><source type=\"image/webp\" srcset=\"/img/uT145YgjSn-960.webp 960w, /img/uT145YgjSn-1600.webp 1600w\" sizes=\"auto\"><img loading=\"lazy\" decoding=\"async\" src=\"/img/uT145YgjSn-960.jpeg\" alt=\"Extract load transform patterns\" width=\"1600\" height=\"2176\" srcset=\"/img/uT145YgjSn-960.jpeg 960w, /img/uT145YgjSn-1600.jpeg 1600w\" sizes=\"auto\"></picture></dialog><button data-index=\"13\"><picture><source type=\"image/webp\" srcset=\"/img/uT145YgjSn-960.webp 960w, /img/uT145YgjSn-1600.webp 1600w\" sizes=\"auto\"><img loading=\"lazy\" decoding=\"async\" src=\"/img/uT145YgjSn-960.jpeg\" alt=\"Extract load transform patterns\" width=\"1600\" height=\"2176\" srcset=\"/img/uT145YgjSn-960.jpeg 960w, /img/uT145YgjSn-1600.jpeg 1600w\" sizes=\"auto\"></picture></button><p></p><p><strong>NOTE:</strong> dlt also provides sub-types of the ‚Äúmerge‚Äù disposition, including&nbsp;<a href=\"https://dlthub.com/blog/scd2-and-incremental-loading\" rel=\"noopener\">SCD type 2</a>; however, for clarity, we did not include these in the diagram. For more information on these subtypes, see&nbsp;<a href=\"https://dlthub.com/docs/general-usage/incremental-loading#merge-incremental-loading\" rel=\"noopener\">relevant documentation</a>.</p><p>The choice of a specific implementation depends on what is supported by the source and destination systems as well as on how the source data is generated. Ideally, incremental extract should be used whenever possible. Then, whether you choose the ‚Äúappend‚Äù or ‚Äúmerge‚Äù write disposition depends on how the data is generated: if you can guarantee that only new records are produced and no existing data is ever modified, you can safely use the ‚Äúappend‚Äù disposition. Next, you need to check if the destination system handles the disposition you intend to use (eg. some systems don‚Äôt support the ‚Äúmerge‚Äù disposition).</p><p>The following diagram from&nbsp;<a href=\"https://dlthub.com/docs/general-usage/incremental-loading#two-simple-questions-determine-the-write-disposition-you-use\" rel=\"noopener\">dlt‚Äôs official documentation</a>&nbsp;also provides a good overview of when to choose which write disposition:</p><p><is-land on:idle></is-land></p><dialog class=\"flow modal14\"><button autofocus class=\"button\">Close</button><picture><source type=\"image/webp\" srcset=\"/img/TixcvWiB7K-960.webp 960w, /img/TixcvWiB7K-1600.webp 1600w\" sizes=\"auto\"><img loading=\"lazy\" decoding=\"async\" src=\"/img/TixcvWiB7K-960.jpeg\" alt=\"how to choose write dispositionin in dlt\" width=\"1600\" height=\"977\" srcset=\"/img/TixcvWiB7K-960.jpeg 960w, /img/TixcvWiB7K-1600.jpeg 1600w\" sizes=\"auto\"></picture></dialog><button data-index=\"14\"><picture><source type=\"image/webp\" srcset=\"/img/TixcvWiB7K-960.webp 960w, /img/TixcvWiB7K-1600.webp 1600w\" sizes=\"auto\"><img loading=\"lazy\" decoding=\"async\" src=\"/img/TixcvWiB7K-960.jpeg\" alt=\"how to choose write dispositionin in dlt\" width=\"1600\" height=\"977\" srcset=\"/img/TixcvWiB7K-960.jpeg 960w, /img/TixcvWiB7K-1600.jpeg 1600w\" sizes=\"auto\"></picture></button><p></p><h2 id=\"orchestrating-data-pipelines-with-prefect\"><a href=\"#orchestrating-data-pipelines-with-prefect\" class=\"heading-anchor\">Orchestrating Data Pipelines with Prefect</a></h2><p>Orchestrating data pipelines with Prefect can streamline your workflow and significantly improve efficiency. Let‚Äôs dive into the best practices for implementing Prefect flows and how they integrate smoothly with your data pipelines.</p><h3 id=\"orchestration-job-features\"><a href=\"#orchestration-job-features\" class=\"heading-anchor\">Orchestration Job Features</a></h3><p>Ideally, the orchestration layer is a thin wrapper over the underlying data pipeline logic. Whenever a feature can be implemented at the pipeline level, it should be implemented there in order to prevent excessive coupling with the orchestration layer and minimize complexity, which simplifies self-service data ingestion.</p><p>Here are a few key features that are best handled at the orchestration layer:</p><ul class=\"list\"><li>alerting</li><li>additional reliability measures</li><li>security (specifically, secret management)</li><li>distributed processing</li></ul><h4 id=\"alerting\"><a href=\"#alerting\" class=\"heading-anchor\">Alerting</a></h4><p>With Prefect, you can set up <a href=\"https://docs.prefect.io/v3/automate/events/automations-triggers#manage-automations\" rel=\"noopener\">alerts</a>, ensuring you‚Äôre notified via Slack, Teams, or email whenever jobs or infrastructure components enter an unexpected state.</p><h4 id=\"reliability-1\"><a href=\"#reliability-1\" class=\"heading-anchor\">Reliability</a></h4><p>While we can (and, where possible, should) implement retries and <a href=\"https://dlthub.com/docs/general-usage/http/requests#customizing-retry-settings\" rel=\"noopener\">timeouts</a> at the pipeline level, Prefect provides these features at the task and flow level. Think of this as a last-resort, catch-all mechanism that allows data engineers to ensure timeouts and retries are enforced regardless of how well the dlt pipeline or helper code is written, again lowering the bar for self-service data ingestion.</p><h4 id=\"secret-management\"><a href=\"#secret-management\" class=\"heading-anchor\">Secret Management</a></h4><p>Security is always a top concern, and Prefect‚Äôs secret management integrations make it easier than ever to store and handle secrets. Whether it‚Äôs Google Cloud Secret Manager or AWS Secret Manager, Prefect allows you to securely retrieve credentials and pass them to the dlt pipeline. This approach ensures that no credentials are stored locally, and administrators have fine-grained control over access by utilizing Prefect‚Äôs Role-Based Access Control (RBAC).</p><h4 id=\"distributed-processing\"><a href=\"#distributed-processing\" class=\"heading-anchor\">Distributed Processing</a></h4><p>While any code-based orchestration tool allows for distributed processing, this feature is rarely required at the pipeline level in recent times. Firstly, data ingestion tools such as dlt are capable of efficiently utilizing machine resources, including parallelization and efficient and safe use of memory. Secondly, virtual machines have grown bigger‚Äîwe can now easily rent VMs with hundreds of cores and hundreds of gigabytes of RAM. Therefore, typically, distributed processing is only required in case we need to run multiple resource-hungry pipelines in parallel.</p><h2 id=\"production-workflow\"><a href=\"#production-workflow\" class=\"heading-anchor\">Production Workflow</a></h2><p>Now that we‚Äôve outlined the essential features of a production-grade dlt pipeline and Prefect flow, let‚Äôs break down the steps of creating and orchestrating data ingestion pipelines in production.</p><h3 id=\"overview\"><a href=\"#overview\" class=\"heading-anchor\">Overview</a></h3><p>The diagram below illustrates the key steps in this production workflow.</p><p><is-land on:idle></is-land></p><dialog class=\"flow modal15\"><button autofocus class=\"button\">Close</button><picture><source type=\"image/webp\" srcset=\"/img/m5ty1gBN3J-960.webp 960w, /img/m5ty1gBN3J-1600.webp 1600w\" sizes=\"auto\"><img loading=\"lazy\" decoding=\"async\" src=\"/img/m5ty1gBN3J-960.jpeg\" alt=\"data pipeline workflow\" width=\"1600\" height=\"591\" srcset=\"/img/m5ty1gBN3J-960.jpeg 960w, /img/m5ty1gBN3J-1600.jpeg 1600w\" sizes=\"auto\"></picture></dialog><button data-index=\"15\"><picture><source type=\"image/webp\" srcset=\"/img/m5ty1gBN3J-960.webp 960w, /img/m5ty1gBN3J-1600.webp 1600w\" sizes=\"auto\"><img loading=\"lazy\" decoding=\"async\" src=\"/img/m5ty1gBN3J-960.jpeg\" alt=\"data pipeline workflow\" width=\"1600\" height=\"591\" srcset=\"/img/m5ty1gBN3J-960.jpeg 960w, /img/m5ty1gBN3J-1600.jpeg 1600w\" sizes=\"auto\"></picture></button><p></p><ol class=\"list\"><li><strong>Create a dlt pipeline:</strong> We start by creating a dlt pipeline (if the one we need doesn‚Äôt exist yet). Once the pipeline is finished and tests pass, we can move on to the next step.</li><li><strong>Create Prefect deployment</strong>: We create a Prefect deployment for the pipeline. Notice we utilize Prefect‚Äôs <code>prefect.yaml</code> file together with a single <code>extract_and_load()</code> flow capable of executing any dlt pipeline to drastically simplify this process.</li><li><strong>Create a Pull Request:</strong> We create a pull request with the new deployment. This triggers the CI/CD process.</li><li><strong>DEV environment:</strong> The deployment is created in the DEV Prefect workspace, and a DEV Docker image is built. We can now manually run the deployment in Prefect UI, which will execute our pipeline in the DEV environment.</li><li><strong>PROD environment:</strong> Once we‚Äôre happy with the results, we merge the pull request. This triggers a CI/CD job, which creates the deployment in the PROD Prefect workspace and builds a PROD Docker image. The deployment schedule is also only enabled at this stage.</li></ol><p>If the pipeline already exists and only a new table is being ingested, the user needs only add a few lines of <code>YAML toprefect.yaml</code> and create a PR.</p><h3 id=\"configuring-dlt\"><a href=\"#configuring-dlt\" class=\"heading-anchor\">Configuring dlt</a></h3><p>While dlt is highly configurable and allows for a lot of customization and optimization, we recommend starting with three highly useful configurations:</p><ul class=\"list\"><li><code>runtime.log_level</code> to enable more logging</li><li><code>normalize.parquet_normalizer.add_dlt_load_id</code> to add a dlt load ID to the loaded data</li><li><code>normalize.parquet_normalizer.add_dlt_id</code> to add a unique id to each row.</li></ul><p>The ID settings will make our data easier to work with for downstream users, as well as make our loads (especially incremental ones) easier to debug.</p><h3 id=\"creating-a-dlt-pipeline\"><a href=\"#creating-a-dlt-pipeline\" class=\"heading-anchor\">Creating a dlt Pipeline</a></h3><h4 id=\"pipeline-design\"><a href=\"#pipeline-design\" class=\"heading-anchor\">Pipeline Design</a></h4><p>We start by creating a dlt pipeline, following the best practices detailed in the <a href=\"#creating-data-connectors-and-pipelines-with-dlt\">Creating data connectors and pipelines with dlt</a> section above.</p><p>For testability and modularity, we recommend splitting the pipeline into a resource (source data) and pipeline (journey and destination) parts. This way, you can easily test each part separately.</p><h4 id=\"inspecting-the-data-manually\"><a href=\"#inspecting-the-data-manually\" class=\"heading-anchor\">Inspecting the Data Manually</a></h4><p>At any stage of pipeline development, you can manually inspect the loaded data, e.g., by printing it to the console or by checking the database directly.</p><h4 id=\"testing-the-pipeline\"><a href=\"#testing-the-pipeline\" class=\"heading-anchor\">Testing the Pipeline</a></h4><p>For integration testing, you can use DuckDB as a destination system. It‚Äôs lightweight and allows you to quickly check ingested data, so you can iterate faster.</p><h3 id=\"creating-a-prefect-flow-and-deployment\"><a href=\"#creating-a-prefect-flow-and-deployment\" class=\"heading-anchor\">Creating a Prefect Flow and Deployment</a></h3><h4 id=\"flow-design\"><a href=\"#flow-design\" class=\"heading-anchor\">Flow Design</a></h4><p>After the dlt pipeline is working, it‚Äôs time to wrap it in a Prefect task and flow. Keep the orchestration layer simple‚Äîuse a single <code>extract_and_load()</code> flow for all data ingestion tasks. With Prefect deployments handling the pipeline name and arguments, you can set everything up with just a few lines of YAML.</p><h4 id=\"handling-pipeline-secrets\"><a href=\"#handling-pipeline-secrets\" class=\"heading-anchor\">Handling Pipeline Secrets</a></h4><p>Secrets should be passed through a special dictionary parameter, such as secrets. These secrets should then extracted from Prefect blocks and forwarded to the dlt pipeline, ensuring they are securely handled.</p><h3 id=\"deploying-to-production\"><a href=\"#deploying-to-production\" class=\"heading-anchor\">Deploying to Production</a></h3><p>A pull request with the new deployment should automatically trigger the CI/CD process in our project repository‚Äôs CI/CD pipelines. We will soon dive deeper into how to implement this process using GitHub Actions in a separate article, so stay tuned!</p><h2 id=\"summary\"><a href=\"#summary\" class=\"heading-anchor\">Summary</a></h2><p>Building a modern, scalable data platform starts with mastering data ingestion, which requires tools that are as powerful as they are flexible. By combining dlt for efficient, open-source data pipelines with Prefect for orchestration, you can create workflows that are not only production-ready but also streamlined for both developers and data teams.</p><p>This approach ensures flexibility, scalability, and cost-effectiveness, making it ideal for modern data platforms while also strategically positioning your platform to excel in the upcoming AI age.</p><h2 id=\"next-steps\"><a href=\"#next-steps\" class=\"heading-anchor\">Next steps</a></h2><h3 id=\"data-transformation\"><a href=\"#data-transformation\" class=\"heading-anchor\">Data Transformation</a></h3><p>dlt and Prefect (with the help of <a href=\"https://www.getdbt.com/\" rel=\"noopener\">dbt</a>) are just as good at data transformation as they are at data ingestion. Stay tuned as we explore how to integrate these tools for data transformation in a future article!</p><h3 id=\"ready-to-dive-deeper\"><a href=\"#ready-to-dive-deeper\" class=\"heading-anchor\">Ready to Dive Deeper?</a></h3><p>If you‚Äôre ready to build a cutting-edge data platform with dlt and Prefect, <a href=\"https://meetings-eu1.hubspot.com/alessio-civitillo/the-scalable-way?uuid=871f7790-0d02-4f3a-9287-3c0f24b53ba2\" rel=\"noopener\">get in touch</a>. We offer expert guidance to help you set up every component and provide a fully equipped template Git repository with production-grade code. No fluff‚Äîjust practical, scalable solutions designed to handle real-world challenges and set your data workflows up for long-term success.</p><h2 id=\"footnotes\"><a href=\"#footnotes\" class=\"heading-anchor\">Footnotes</a></h2><p>[1] While more and more UI-based tools add copilot capabilities, they face several fundamental limitations:</p><ul class=\"list\"><li>Copilots, while text-based, are limited by the UI tools they are built upon.</li></ul><p>Imagine instructing someone to build a complex LEGO castle with only a basic set of blocks. No matter how clearly you explain, the result will always be limited, forcing you to find workarounds.</p><ul class=\"list\"><li>These UI tools often use a custom language to define data pipelines, which adds another layer of complexity.</li></ul><p>As the quality of LLMs is highly reliant on the size and quality of the dataset they‚Äôre learning from, it means these assistants cannot reach the same level of fluency as LLMs trained on much more popular languages, such as Python.</p><p>Imagine the person you‚Äôre instructing to build your LEGO castle has very little experience with LEGO or construction in general. They would struggle to understand basic jargon and construction trade practices, and they would often make mistakes requiring your intervention.</p></div>",
      "date_published": "2025-01-27T12:54:00Z"
    }
    ,{
      "id": "http://localhost:8080/blog/deploying-prefect-on-any-cloud-using-a-single-virtual-machine/",
      "url": "http://localhost:8080/blog/deploying-prefect-on-any-cloud-using-a-single-virtual-machine/",
      "title": "Deploying Prefect on any Cloud Using a Single Virtual Machine",
      "content_html": "<nav id=\"toc\" class=\"table-of-contents prose\"><ol><li class=\"flow\"><a href=\"#challenges-with-picking-a-data-platform-architecture\">Challenges With Picking a Data Platform Architecture</a></li><li class=\"flow\"><a href=\"#data-platform-orchestration-the-key-to-seamless-integration\">Data Platform Orchestration: The Key to Seamless Integration</a><ol><li class=\"flow\"><a href=\"#data-orchestration-tools\">Data Orchestration Tools</a></li></ol></li><li class=\"flow\"><a href=\"#what-is-prefect-cloud\">What is Prefect Cloud?</a><ol><li class=\"flow\"><a href=\"#common-struggle-for-prefect-users-deployment\">Common Struggle for Prefect Users: Deployment</a></li><li class=\"flow\"><a href=\"#eternal-dilemma-server-based-or-serverless\">Eternal Dilemma: Server-based or Serverless</a></li><li class=\"flow\"><a href=\"#deployment-options-for-a-server-based-data-platform\">Deployment Options for a Server-based Data Platform</a></li></ol></li><li class=\"flow\"><a href=\"#recommended-setup-for-getting-started-lightweight-kubernetes-on-a-single-virtual-machine\">Recommended Setup for Getting Started: Lightweight Kubernetes on a Single Virtual Machine</a></li><li class=\"flow\"><a href=\"#conclusion\">Conclusion</a></li></ol></nav><p><span id=\"toc-skipped\" class=\"visually-hidden\"></span></p><div class=\"flow prose\"><p></p><p>Choosing the right data platform architecture is quite a challenge for any organization. It‚Äôs a balancing act: you need something that delivers immediate value while staying flexible enough for future growth, all without sacrificing scalability, simplicity, or efficiency.</p><p>This article offers a thoughtful guide to the decision-making process behind choosing Prefect with lightweight Kubernetes (K3S) on a single Virtual Machine (VM) with any cloud provider. You‚Äôll explore:</p><ul class=\"list\"><li>Why simplicity and flexibility are essential for modern data platforms.</li><li>Key considerations for selecting the right data orchestration tool.</li><li>Insights into serverless vs server-based execution of Prefect flows.</li><li>Approaches to running a server-based Prefect worker</li></ul><p>Rather than a step-by-step tutorial, this guide is designed to help you make solid platform architecture decisions and design a solution tailored to your organization‚Äôs unique needs. Let‚Äôs dive in.</p><h2 id=\"challenges-with-picking-a-data-platform-architecture\"><a href=\"#challenges-with-picking-a-data-platform-architecture\" class=\"heading-anchor\">Challenges With Picking a Data Platform Architecture</a></h2><p>The options for building a data platform are endless, but many fall short. With the rise of affordable cloud storage, expectations have changed, leaving many once-revolutionary legacy systems struggling to keep up. At the same time, new solutions making big claims often fail, either missing critical features or bogging organizations down with unnecessary complexity. For smaller companies, the challenge is even greater‚Äîa data platform should drive business value, not require a dedicated team just to maintain it.</p><p>Starting small may seem practical, but early shortcuts can turn into major obstacles as the platform grows. Undoing poor architectural choices later is often costly and disruptive. That‚Äôs why <strong>choosing a solution that is both simple and scalable from the outset is essential</strong>.</p><p>For decision-makers, this journey begins by stepping back and evaluating both the current state of their team and the platform they rely on. The <strong>Data Platform Maturity Curve</strong> is a helpful framework for this:</p><p><is-land on:idle></is-land></p><dialog class=\"flow modal35\"><button autofocus class=\"button\">Close</button><picture><source type=\"image/webp\" srcset=\"/img/GGk32ZeRJa-960.webp 960w, /img/GGk32ZeRJa-1600.webp 1600w\" sizes=\"auto\"><img loading=\"lazy\" decoding=\"async\" src=\"/img/GGk32ZeRJa-960.jpeg\" alt=\"data platform maturity\" width=\"1600\" height=\"860\" srcset=\"/img/GGk32ZeRJa-960.jpeg 960w, /img/GGk32ZeRJa-1600.jpeg 1600w\" sizes=\"auto\"></picture></dialog><button data-index=\"35\"><picture><source type=\"image/webp\" srcset=\"/img/GGk32ZeRJa-960.webp 960w, /img/GGk32ZeRJa-1600.webp 1600w\" sizes=\"auto\"><img loading=\"lazy\" decoding=\"async\" src=\"/img/GGk32ZeRJa-960.jpeg\" alt=\"data platform maturity\" width=\"1600\" height=\"860\" srcset=\"/img/GGk32ZeRJa-960.jpeg 960w, /img/GGk32ZeRJa-1600.jpeg 1600w\" sizes=\"auto\"></picture></button><p></p><p>Depending on the organization‚Äôs data technology maturity level, your platform must adapt. This article focuses on those in the middle of the curve, where simple scripts and ad-hoc solutions are no longer enough, but advanced features like autoscaling aren‚Äôt yet necessary. At this stage, the platform delivers tangible business value and is steadily becoming integral to operations. Downtime‚Äîwhether it lasts hours, a day, or even a week‚Äîis growing increasingly expensive.</p><p>The goal? A platform that‚Äôs lightweight, scalable, and future-ready without overcomplicating things.</p><h2 id=\"data-platform-orchestration-the-key-to-seamless-integration\"><a href=\"#data-platform-orchestration-the-key-to-seamless-integration\" class=\"heading-anchor\">Data Platform Orchestration: The Key to Seamless Integration</a></h2><p>Even the best-designed data platform is useless if it‚Äôs not integrated. No matter how carefully you choose your architecture, your platform‚Äôs success hinges on how well its core components‚Äîingestion, transformation, and serving‚Äîwork together. These phases can only operate efficiently when they are tightly aligned.</p><p><is-land on:idle></is-land></p><dialog class=\"flow modal36\"><button autofocus class=\"button\">Close</button><picture><source type=\"image/webp\" srcset=\"/img/KgwfpAY-HF-960.webp 960w, /img/KgwfpAY-HF-1600.webp 1600w\" sizes=\"auto\"><img loading=\"lazy\" decoding=\"async\" src=\"/img/KgwfpAY-HF-960.jpeg\" alt=\"data enginnering stages lifecycl\" width=\"1600\" height=\"519\" srcset=\"/img/KgwfpAY-HF-960.jpeg 960w, /img/KgwfpAY-HF-1600.jpeg 1600w\" sizes=\"auto\"></picture></dialog><button data-index=\"36\"><picture><source type=\"image/webp\" srcset=\"/img/KgwfpAY-HF-960.webp 960w, /img/KgwfpAY-HF-1600.webp 1600w\" sizes=\"auto\"><img loading=\"lazy\" decoding=\"async\" src=\"/img/KgwfpAY-HF-960.jpeg\" alt=\"data enginnering stages lifecycl\" width=\"1600\" height=\"519\" srcset=\"/img/KgwfpAY-HF-960.jpeg 960w, /img/KgwfpAY-HF-1600.jpeg 1600w\" sizes=\"auto\"></picture></button><p></p><p><em>Adapted from ‚ÄúFundamentals of Data Engineering: Plan and Build Robust Data Systems‚Äù by Joe Reis &amp; Matt Housley</em></p><p>Early-stage platforms often rely on manual orchestration, which works at first but quickly becomes a bottleneck as data grows and workflows become more complex. Managing, ensuring accuracy, and reducing downtime requires a more structured approach.</p><p>A few basic improvements can help push the boundaries further. For instance:</p><ul class=\"list\"><li>Instead of running all scripts locally, they can be executed on a virtual machine.</li><li>Setting up a database helps centralize data</li><li>Basic automation of workflows can be managed with cron jobs in Linux.</li></ul><p>While these incremental improvements help in the short term, significant challenges remain:</p><ul class=\"list\"><li><strong>Manual code execution</strong> becomes increasingly error-prone as the scale of operations grows.</li><li><strong>Cron jobs</strong> become difficult to manage as workflows become more complex and interdependent. Debugging failures can quickly turn into a nightmare, especially with cascading issues across multiple flows.</li></ul><p>This is where automated data orchestration becomes the key to streamlining workflows across the entire lifecycle. It allows teams to automate, monitor, and scale operations by transforming disconnected processes into a cohesive system, minimizing manual intervention and reducing errors.</p><p>Let‚Äôs review the most popular options available in the market.</p><h3 id=\"data-orchestration-tools\"><a href=\"#data-orchestration-tools\" class=\"heading-anchor\">Data Orchestration Tools</a></h3><p>The three leading orchestration tools in the market are:</p><ul class=\"list\"><li><strong>Apache Airflow:</strong> An open-source and community-driven tool with robust features but a steep learning curve. Managed versions like Google Cloud Composer and Amazon MWAA simplify deployment but tie users to specific cloud providers.</li><li><strong>Prefect:</strong> A modern, cloud-agnostic, and easy-to-configure solution emphasizing scalability, portability, and developer-friendly features that allow for flexible orchestration. Prefect‚Äôs architecture also supports running workflows in hybrid environments, seamlessly bridging on-premises and cloud solutions.</li><li><strong>Dagster:</strong> Designed for data-aware orchestration, Dagster prioritizes validation, lineage, and developer productivity, making it ideal for teams handling complex pipelines.</li></ul><p>At The Scalable Way, we have worked with both Airflow and Prefect in a few projects. We advise Prefect for a lightweight setup with fewer deployment things to worry about.</p><h2 id=\"what-is-prefect-cloud\"><a href=\"#what-is-prefect-cloud\" class=\"heading-anchor\">What is Prefect Cloud?</a></h2><p>Prefect Cloud is a fully managed orchestration platform that simplifies running and monitoring Python-based workflows without the overhead of managing infrastructure. It‚Äôs well-suited for teams looking to automate data workflows, from ingestion and transformation to serving.</p><p>Its strengths include:</p><ul class=\"list\"><li><strong>Scalability</strong>: Handles thousands of workflows with ease.</li><li><strong>Monitoring and alerting</strong>: Built-in features simplify issue detection and resolution.</li><li><strong>Cloud-agnostic architecture</strong>: Runs seamlessly across environments, avoiding vendor lock-in.</li></ul><p>By automating the orchestration layer, Prefect Cloud allows teams to focus on building robust pipelines without the overhead of managing infrastructure.</p><h3 id=\"common-struggle-for-prefect-users-deployment\"><a href=\"#common-struggle-for-prefect-users-deployment\" class=\"heading-anchor\">Common Struggle for Prefect Users: Deployment</a></h3><p>Adopting Prefect as an orchestrator unlocks many possibilities, but like any powerful tool, it comes with a learning curve. Prefect flexibility and a developer-first approach can initially feel daunting for teams unfamiliar with building solid deployment solutions.</p><p>Prefect‚Äôs philosophy emphasizes providing tools rather than prescribing solutions, allowing users to adapt its features to their specific needs. While this approach offers flexibility and scalability, it can leave data engineers uncertain about where to start with scalable deployment practices like CI/CD pipelines and autoscaling.</p><h3 id=\"eternal-dilemma-server-based-or-serverless\"><a href=\"#eternal-dilemma-server-based-or-serverless\" class=\"heading-anchor\">Eternal Dilemma: Server-based or Serverless</a></h3><p>Another consideration is choosing the right setup for running Prefect flows. There are two primary approaches, each designed to cater to different needs:</p><ul class=\"list\"><li><strong>Server-based</strong>: This requires setting up infrastructure such as virtual machines, lightweight Kubernetes (e.g., K3S), or managed Kubernetes clusters. While these setups provide maximum control, scalability, and adaptability, they demand a higher level of expertise and upfront effort.</li><li><strong>Serverless</strong>: Managed solutions like Prefect Cloud‚Äôs service or serverless compute options from cloud providers (AWS Fargate, Google Cloud Run, Azure Container Instances) eliminate the need for infrastructure management, making them appealing for simpler workflows.</li></ul><p>Serverless solutions, though convenient, are best suited for simpler workflows, as they come with five notable challenges:</p><ol class=\"list\"><li><strong>Startup Overhead</strong>: Prefect Worker images often have heavy dependencies, increasing flow initialization time. This leads to latency, as serverless platforms can introduce delays between task executions due to event-driven triggers. A long-running server with a persistent Prefect Worker is usually much quicker.</li><li><strong>Vendor Lock-In</strong>: Serverless solutions are often tightly integrated with specific cloud providers, making it difficult to migrate workflows across platforms. Even Prefect Work Pools, though useful, have limited functionality at the Pro tier.</li><li><strong>Cost Management</strong>: Serverless can be cost-effective for intermittent workloads, but can become expensive with unpredictable usage patterns. Managing costs is trickier compared to traditional server-based setups.</li><li><strong>Limited Control and Security Concerns</strong>: Serverless architectures limit control over the execution environment, as all logic runs on cloud provider-managed machines. This raises security risks, especially for companies dealing with sensitive data or operating in highly regulated industries, due to reduced visibility and potential vulnerabilities in shared infrastructure.</li><li><strong>Token Management and Data Access Risks</strong>: Serverless setups require Prefect to hold a token for accessing cloud resources, creating security risks if mismanaged. Server-based setups mitigate this by reversing the data flow, allowing the server to pull from Prefect, and reducing the risk of data breaches or unintended data exposure.</li></ol><p>Ultimately, the choice between server-based and serverless depends on the teams‚Äô needs and stage of data maturity. However, for most organizations aiming to scale, a Prefect Work Pool running on a long-running server is a more optimal and reliable solution.</p><h3 id=\"deployment-options-for-a-server-based-data-platform\"><a href=\"#deployment-options-for-a-server-based-data-platform\" class=\"heading-anchor\">Deployment Options for a Server-based Data Platform</a></h3><ul class=\"list\"><li><strong>Local Prefect Worker Process</strong></li></ul><p>Connects directly to Prefect Cloud and serves as an introductory setup to understand Prefect Cloud‚Äôs functionality. However, this is not suitable for production scenarios due to limited scalability and resilience.</p><ul class=\"list\"><li><strong>Systemd Process on Single or Multiple VMs</strong></li></ul><p>Runs Prefect flows in Docker containers, providing a lightweight setup that is relatively easy to configure. This approach is well-suited to small projects and teams, as Docker limits unnecessary complexity.</p><ul class=\"list\"><li><strong>Single VM with Lightweight Kubernetes (K3S)</strong></li></ul><p>It‚Äôs not as simple as a Systemd setup because of the introduction of Kubernetes and Helm. Thanks to these tools, it‚Äôs more scalable and adaptable for future growth. This setup offers flexibility for migration to more robust configurations as project demands increase.</p><ul class=\"list\"><li><strong>Managed Kubernetes Cluster</strong></li></ul><p>The most feature-rich solution-managed Kubernetes supports autoscaling, spot instances, and integrations with tools like Active Directory. It is ideal for comprehensive data platforms. However, this approach adds operational complexity and may be excessive for smaller projects.</p><h2 id=\"recommended-setup-for-getting-started-lightweight-kubernetes-on-a-single-virtual-machine\"><a href=\"#recommended-setup-for-getting-started-lightweight-kubernetes-on-a-single-virtual-machine\" class=\"heading-anchor\">Recommended Setup for Getting Started: Lightweight Kubernetes on a Single Virtual Machine</a></h2><p>The lightweight Kubernetes on a single Virtual Machine (VM) setup strikes an ideal balance between cost efficiency and operational flexibility. By leveraging lightweight Kubernetes (K3S), you gain the core benefits of Kubernetes with significantly reduced overhead, making it perfect for smaller environments or projects with constrained resources. Its streamlined architecture ensures smooth operations without the complexity of managing a full Kubernetes cluster. The diagram illustrates a basic architecture that effectively meets most requirements for running Prefect flows in a scalable manner.</p><p><is-land on:idle></is-land></p><dialog class=\"flow modal37\"><button autofocus class=\"button\">Close</button><picture><source type=\"image/webp\" srcset=\"/img/Q4en1un5bs-960.webp 960w, /img/Q4en1un5bs-1600.webp 1600w\" sizes=\"auto\"><img loading=\"lazy\" decoding=\"async\" src=\"/img/Q4en1un5bs-960.jpeg\" alt=\"lightweight data platform setup\" width=\"1600\" height=\"1033\" srcset=\"/img/Q4en1un5bs-960.jpeg 960w, /img/Q4en1un5bs-1600.jpeg 1600w\" sizes=\"auto\"></picture></dialog><button data-index=\"37\"><picture><source type=\"image/webp\" srcset=\"/img/Q4en1un5bs-960.webp 960w, /img/Q4en1un5bs-1600.webp 1600w\" sizes=\"auto\"><img loading=\"lazy\" decoding=\"async\" src=\"/img/Q4en1un5bs-960.jpeg\" alt=\"lightweight data platform setup\" width=\"1600\" height=\"1033\" srcset=\"/img/Q4en1un5bs-960.jpeg 960w, /img/Q4en1un5bs-1600.jpeg 1600w\" sizes=\"auto\"></picture></button><p></p><p>Using Helm charts to deploy the Prefect Worker simplifies orchestration, ensuring seamless integration with existing systems while minimizing manual configurations. Helm also makes updates easier, promotes standardization, and reduces deployment errors.</p><p>Running everything on a single virtual machine keeps the infrastructure simple yet scalable. If project demands grow, you can easily upgrade the VM or expand to a multi-node cluster without major changes to your architecture. Additionally, this setup simplifies maintenance, provides clear monitoring and debugging paths, and avoids vendor lock-in, preserving flexibility for future enhancements.</p><h2 id=\"conclusion\"><a href=\"#conclusion\" class=\"heading-anchor\">Conclusion</a></h2><p>Building a modern data platform is no easy task. Success lies in keeping it simple while ensuring flexibility and scalability. With the right tools and setup, like Prefect and lightweight Kubernetes on a single virtual machine, you can create a platform that delivers immediate value and adapts as your needs grow.</p><p>By focusing on scalable, modular solutions, you‚Äôre not just solving today‚Äôs problems‚Äîyou‚Äôre building a platform ready for whatever comes next.</p></div>",
      "date_published": "2025-01-15T09:29:00Z"
    }
    
  ]
}